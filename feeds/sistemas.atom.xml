<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Linux Sysadmin - Sistemas</title><link href="https://www.linuxsysadmin.ml/" rel="alternate"></link><link href="https://www.linuxsysadmin.ml/feeds/sistemas.atom.xml" rel="self"></link><id>https://www.linuxsysadmin.ml/</id><updated>2019-01-07T10:00:00+01:00</updated><entry><title>Usando el servidor integrado de PHP en un contenedor Docker</title><link href="https://www.linuxsysadmin.ml/2019/01/usando-el-servidor-integrado-de-php-en-un-contenedor-docker.html" rel="alternate"></link><published>2019-01-07T10:00:00+01:00</published><updated>2019-01-07T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2019-01-07:/2019/01/usando-el-servidor-integrado-de-php-en-un-contenedor-docker.html</id><summary type="html">&lt;p&gt;En mi trabajo estamos renovando el proveedor de infraestructura de nuestros servicios. Al migrar las máquina nos estamos encontrando servicios desorganizados, en varios lenguajes y en versiones antiguas. Uno de estos servicios es un frontal PHP mugriento, y lo migramos rápidamente en un contenedor usando el servidor incorporado de PHP …&lt;/p&gt;</summary><content type="html">&lt;p&gt;En mi trabajo estamos renovando el proveedor de infraestructura de nuestros servicios. Al migrar las máquina nos estamos encontrando servicios desorganizados, en varios lenguajes y en versiones antiguas. Uno de estos servicios es un frontal PHP mugriento, y lo migramos rápidamente en un contenedor usando el servidor incorporado de PHP.&lt;/p&gt;
&lt;p&gt;Normalmente, estoy en contra de usar servidores de desarrollo para servir la aplicación final, pero dada la prisa de la migración y el escaso número de usuarios que usan este frontal (¡tres!), fue una medida más que aceptable, en vistas a una futura mejora.&lt;/p&gt;
&lt;p&gt;El resultado no fue tan malo como esperaba, y como me gusta como servidor de desarrollo, me lo he apuntado. Como no soy de programar PHP, lo mantendré en mi máquina hasta que me harte de verlo, momento en que su eliminación va a dejar mi máquina 100% libre de PHP, gracias a &lt;strong&gt;Docker&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Para mantener limpio mi entorno, lo voy a poner todo en una carpeta local, con el &lt;em&gt;docker-compose.yml&lt;/em&gt; de runtime, el &lt;em&gt;Dockerfile&lt;/em&gt; para construir el contenedor, y una carpeta de código que voy a montar como volumen para no tener que reconstruir la imagen a cada cambio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/phpserver$ tree
.
├── app
│   └── info.php
├── docker-compose.yml
└── Dockerfile

&lt;span class="m"&gt;1&lt;/span&gt; directory, &lt;span class="m"&gt;3&lt;/span&gt; files
gerard@atlantis:~/workspace/phpserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El &lt;em&gt;Dockerfile&lt;/em&gt; es bastante simple, y se limita a instalar &lt;strong&gt;PHP&lt;/strong&gt;, y las extensiones que nuestro código pueda necesitar. El uso de &lt;strong&gt;tini&lt;/strong&gt; es simplemente para que el contenedor pueda parar de forma correcta; más información en &lt;a href="https://www.linuxsysadmin.ml/2017/09/un-proceso-inicial-para-docker-tini-y-dumb-init.html"&gt;este otro artículo&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/phpserver$ cat Dockerfile
FROM alpine:3.8
RUN apk add --no-cache tini php7 php7-session php7-pdo_mysql
ENTRYPOINT &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/sbin/tini&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/bin/php7&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-S&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;0.0.0.0:8080&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-t&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;/srv/app&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@atlantis:~/workspace/phpserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: Es posible que necesitéis ajustar las dependencias &lt;code&gt;php7-*&lt;/code&gt; en base a vuestro código. Las extensiones &lt;code&gt;php7-session&lt;/code&gt; y &lt;code&gt;php7-pdo_mysql&lt;/code&gt; las puse porque son lo que se necesita para ejecutar &lt;a href="https://www.adminer.org/"&gt;Adminer&lt;/a&gt;; al acabar la prueba de concepto me puse a probar de hacer una API REST y tuve que poner &lt;code&gt;php7-json&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Tanto para el &lt;em&gt;runtime&lt;/em&gt; como para el &lt;em&gt;build time&lt;/em&gt;, voy a utilizar &lt;strong&gt;docker-compose&lt;/strong&gt;, que me permite versionar ambos procesos y su manejo de forma fácil:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/phpserver$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  phpserver:
    image: phpserver
    build: .
    container_name: phpserver
    hostname: phpserver
    volumes:
      - ./app:/srv/app:ro
    ports:
      - &lt;span class="s2"&gt;&amp;quot;8080:8080&amp;quot;&lt;/span&gt;
gerard@atlantis:~/workspace/phpserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finalmente, y para ver que todo funciona necesitamos una aplicación, que en este caso es un &lt;code&gt;phpinfo()&lt;/code&gt; estándar. Como he mencionado también puse &lt;strong&gt;adminer&lt;/strong&gt;, pero esto es ahora irrelevante. Se pone algo solo para que el artículo quede completo y funcional, pero cambiaremos el código según lo vayamos desarrollando.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/phpserver$ cat app/info.php
&amp;lt;?php phpinfo&lt;span class="o"&gt;()&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; ?&amp;gt;
gerard@atlantis:~/workspace/phpserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La construcción no tiene secreto alguno; se construye con el contexto de la carpeta local (solo se usa el &lt;code&gt;Dockerfile&lt;/code&gt;) y se le asigna el &lt;em&gt;tag&lt;/em&gt; de imagen del campo &lt;code&gt;image&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/phpserver$ docker-compose build
Building phpserver
Step &lt;span class="m"&gt;1&lt;/span&gt;/4 : FROM alpine:3.8
 ---&amp;gt; 196d12cf6ab1
Step &lt;span class="m"&gt;2&lt;/span&gt;/4 : RUN apk add --no-cache tini php7 php7-session php7-pdo_mysql
 ---&amp;gt; Running in 565e2b5f767b
...
Removing intermediate container 565e2b5f767b
 ---&amp;gt; 2df4637bc4bc
Step &lt;span class="m"&gt;3&lt;/span&gt;/4 : ENTRYPOINT &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/sbin/tini&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 ---&amp;gt; Running in 3069cd1a57d4
Removing intermediate container 3069cd1a57d4
 ---&amp;gt; 5c34542dde4f
Step &lt;span class="m"&gt;4&lt;/span&gt;/4 : CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/bin/php7&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-S&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;0.0.0.0:8080&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-t&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;/srv/app&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
 ---&amp;gt; Running in 00027b5023e2
Removing intermediate container 00027b5023e2
 ---&amp;gt; de340d9fb385
Successfully built de340d9fb385
Successfully tagged phpserver:latest
gerard@atlantis:~/workspace/phpserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La verdad es que no paro de maravillarme de lo poco que ocupan las imagenes con base de &lt;strong&gt;Alpine Linux&lt;/strong&gt;... ¡Solo 15mb!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/phpserver$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
phpserver           latest              de340d9fb385        &lt;span class="m"&gt;19&lt;/span&gt; seconds ago      &lt;span class="m"&gt;14&lt;/span&gt;.3MB
alpine              &lt;span class="m"&gt;3&lt;/span&gt;.8                 196d12cf6ab1        &lt;span class="m"&gt;2&lt;/span&gt; months ago        &lt;span class="m"&gt;4&lt;/span&gt;.41MB
gerard@atlantis:~/workspace/phpserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Levantamos el servidor con el comando habitual, si más complicaciones:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/phpserver$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;phpserver_default&amp;quot;&lt;/span&gt; with the default driver
Creating phpserver ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@atlantis:~/workspace/phpserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos ver el resultado en &lt;a href="http://atlantis:8080/info.php"&gt;http://atlantis:8080/info.php&lt;/a&gt;. Dada la naturaleza de &lt;strong&gt;PHP&lt;/strong&gt;, solo os queda abrir la carpeta &lt;code&gt;app&lt;/code&gt; y meter vuestro código, bien sea copiándolo, o bien sea desarrollándolo ahí directamente. No os olvidéis de ir revisando la salida usando &lt;code&gt;docker-compose logs -f&lt;/code&gt; por si os faltaran extensiones, o simplemente para ver los errores que vuestro código pueda generar.&lt;/p&gt;</content><category term="docker"></category><category term="php"></category></entry><entry><title>Montando una wiki interna con MediaWiki y Docker</title><link href="https://www.linuxsysadmin.ml/2018/12/montando-una-wiki-interna-con-mediawiki-y-docker.html" rel="alternate"></link><published>2018-12-03T10:00:00+01:00</published><updated>2018-12-03T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-12-03:/2018/12/montando-una-wiki-interna-con-mediawiki-y-docker.html</id><summary type="html">&lt;p&gt;Tras cambiar de equipo de trabajo, me encuentro con un repositorio de información procedimental consistente en una carpeta compartida con varias versiones de documentos que hacen referencia al mismo procedimiento. Esto convierte la tarea de buscar un procedimiento en un infierno, por no mencionar el gran esfuerzo de mantenerlos actualizados …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Tras cambiar de equipo de trabajo, me encuentro con un repositorio de información procedimental consistente en una carpeta compartida con varias versiones de documentos que hacen referencia al mismo procedimiento. Esto convierte la tarea de buscar un procedimiento en un infierno, por no mencionar el gran esfuerzo de mantenerlos actualizados.&lt;/p&gt;
&lt;p&gt;Lo normal es que las empresas te pongan pegas porque propones una herramienta no autorizada, o "porque siempre se ha utilizado esto"; no me son excusas nuevas. Lo que me tocó las narices en esta situación es que la excusa era más que cutre: "montar esto va a llevar mucho tiempo".&lt;/p&gt;
&lt;p&gt;Así pues, y en virtud de tan necias palabras he decidido hacer este artículo: montar una aplicación PHP clásica con &lt;strong&gt;Docker&lt;/strong&gt;, con una inversión temporal negligible porque todo está en la librería estándar o creado por un tercero.&lt;/p&gt;
&lt;h2&gt;Una wiki de estar por casa&lt;/h2&gt;
&lt;p&gt;En el equipo somos un número tirando a pequeño de chicos de operaciones, de variado conocimiento y experiencia. Pero desengañémonos: me quiero ir de vacaciones y no tener que hacerlo con un móvil y un portátil. Muchas cosas deben estar en manos de colaboradores más o menos capaces.&lt;/p&gt;
&lt;p&gt;No es tan importante el motor de &lt;em&gt;wiki&lt;/em&gt; como lo es el procedimiento y la facilidad de acceso y modificación. Como amantes del software libre y gracias a su presencia en &lt;a href="https://hub.docker.com/_/mediawiki/"&gt;DockerHub&lt;/a&gt;, la solución casi obligada es &lt;strong&gt;MediaWiki&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Aunque esta &lt;em&gt;wiki&lt;/em&gt; puede funcionar con una base de datos &lt;strong&gt;SQLite&lt;/strong&gt;, parece más natural utilizar la compañera clásica de un sistema &lt;strong&gt;LAMP&lt;/strong&gt;, aunque como los tiempos varian, he elegido &lt;strong&gt;MariaDB&lt;/strong&gt;, también en la librería oficial de &lt;strong&gt;DockerHub&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;El &lt;em&gt;setup&lt;/em&gt; no puede ser más simple:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Una base de datos tipo &lt;strong&gt;MySQL&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Un contendor &lt;strong&gt;MediaWiki&lt;/strong&gt;, configurado para apuntar a la base de datos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Por simplicidad, vamos a utilizar &lt;strong&gt;docker-compose&lt;/strong&gt; para levantar ambos servicios (o el &lt;em&gt;stack&lt;/em&gt; si usáis &lt;strong&gt;Docker Swarm&lt;/strong&gt;). Este es el fichero &lt;em&gt;docker-compose.yml&lt;/em&gt; que he utilizado:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/tools/wiki$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  wiki_ops:
    image: mediawiki
    container_name: wiki_ops
    hostname: wiki_ops
    ports:
      - &lt;span class="m"&gt;8080&lt;/span&gt;:80
  mariadb:
    image: mariadb
    container_name: mariadb
    hostname: mariadb
    environment:
      MYSQL_DATABASE: wiki_ops
      MYSQL_USER: wiki_ops
      MYSQL_PASSWORD: changeme
      MYSQL_RANDOM_ROOT_PASSWORD: &lt;span class="s2"&gt;&amp;quot;yes&amp;quot;&lt;/span&gt;
gerard@atlantis:~/tools/wiki$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/tools/wiki$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;wiki_default&amp;quot;&lt;/span&gt; with the default driver
Creating wiki_ops ... &lt;span class="k"&gt;done&lt;/span&gt;
Creating mariadb  ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@atlantis:~/tools/wiki$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El resultado lo podemos ver en &lt;a href="http://atlantis:8080/"&gt;http://atlantis:8080/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: No me voy a meter en manejar volúmenes, reinicios y otras cuestiones como múltiples instancias o balancedores; esto solo va a alargar el artículo innecesariamente.&lt;/p&gt;
&lt;h2&gt;Configurando nuestra instancia&lt;/h2&gt;
&lt;p&gt;Cuando vamos a la página web con un navegador, el &lt;em&gt;software&lt;/em&gt; se da cuenta que no existe una configuración local, es decir, que no tenemos nuestra wiki configurada. La reacción programada es la de levantar el asistente de configuración.&lt;/p&gt;
&lt;p&gt;La configuración es trivial, de acuerdo a los parámetros de nuestro &lt;em&gt;docker-compose.yml&lt;/em&gt;, siendo los de la base de datos los más importantes. Si completamos el asistente, descargaremos un fichero &lt;code&gt;LocalSettings.php&lt;/code&gt; que deberemos colocar en &lt;code&gt;/var/www/html/&lt;/code&gt;; yo lo he hecho mediante un &lt;em&gt;file volume&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/tools/wiki$ tree
.
├── docker-compose.yml
└── LocalSettings.php

&lt;span class="m"&gt;0&lt;/span&gt; directories, &lt;span class="m"&gt;2&lt;/span&gt; files
gerard@atlantis:~/tools/wiki$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/tools/wiki$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  wiki_ops:
    image: mediawiki
    container_name: wiki_ops
    hostname: wiki_ops
    volumes:
      - ./LocalSettings.php:/var/www/html/LocalSettings.php
    ports:
      - &lt;span class="m"&gt;8080&lt;/span&gt;:80
  mariadb:
    image: mariadb
    container_name: mariadb
    hostname: mariadb
    environment:
      MYSQL_DATABASE: wiki_ops
      MYSQL_USER: wiki_ops
      MYSQL_PASSWORD: changeme
      MYSQL_RANDOM_ROOT_PASSWORD: &lt;span class="s2"&gt;&amp;quot;yes&amp;quot;&lt;/span&gt;
gerard@atlantis:~/tools/wiki$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/tools/wiki$ docker-compose up -d
mariadb is up-to-date
Recreating wiki_ops ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@atlantis:~/tools/wiki$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto tenemos nuestra &lt;em&gt;wiki&lt;/em&gt;, gracias a &lt;strong&gt;Docker&lt;/strong&gt;. ¿Os ha parecido mucho tiempo?&lt;/p&gt;</content><category term="wiki"></category><category term="docker"></category><category term="mediawiki"></category><category term="mariadb"></category><category term="mysql"></category></entry><entry><title>Un registro docker privado por HTTPS con autenticación básica</title><link href="https://www.linuxsysadmin.ml/2018/11/un-registro-docker-privado-por-https-con-autenticacion-basica.html" rel="alternate"></link><published>2018-11-19T10:00:00+01:00</published><updated>2018-11-19T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-11-19:/2018/11/un-registro-docker-privado-por-https-con-autenticacion-basica.html</id><summary type="html">&lt;p&gt;Cuando usamos integración continua o despliegues en varios servidores y usamos &lt;strong&gt;docker&lt;/strong&gt;, se hace importante tener una fuente de imágenes de donde descargar las nuestras propias. Aquí entra en juego la confidencialidad, y es necesario pagar la capa privada de un registro, o podemos simplemente crear un registro nuestro propio …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Cuando usamos integración continua o despliegues en varios servidores y usamos &lt;strong&gt;docker&lt;/strong&gt;, se hace importante tener una fuente de imágenes de donde descargar las nuestras propias. Aquí entra en juego la confidencialidad, y es necesario pagar la capa privada de un registro, o podemos simplemente crear un registro nuestro propio.&lt;/p&gt;
&lt;p&gt;Si el registro está abierto a nuestra red corporativa, somos vulnerables a ataques maliciosos por parte de empleados descontentos o traviesos. En estos casos se recomienda utilizar TLS para encriptar las comunicaciones y activar autenticación para que no nos puedan reescribir las imágenes.&lt;/p&gt;
&lt;p&gt;Hacerlo no es muy complicado y solo vamos a tener que hacerlo una vez; vale la pena y así dejamos un punto de preocupación menos en nuestra infraestructura. Vamos a partir del siguiente escenario:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Un servidor con &lt;strong&gt;docker&lt;/strong&gt; dedicado al registro, con nombre &lt;code&gt;registry.test&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Un servidor cliente, también con &lt;strong&gt;docker&lt;/strong&gt; para simular los futuros clientes de nuestro registro, con nombre &lt;code&gt;node01.test&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: Es importante poner un dominio en nuestro servidor de registro, porque sino &lt;strong&gt;docker&lt;/strong&gt; puede pensarse que se trata de un usuario de &lt;strong&gt;DockerHub&lt;/strong&gt;; por ejemplo &lt;code&gt;registry/image&lt;/code&gt; se puede referir a la URL "registry" o ir a &lt;strong&gt;DockerHub&lt;/strong&gt; y hacer el &lt;em&gt;push&lt;/em&gt; o &lt;em&gt;pull&lt;/em&gt; con el usuario "registry".&lt;/p&gt;
&lt;h2&gt;Activando TLS&lt;/h2&gt;
&lt;p&gt;Para activar TLS, solamente se necesita indicar el &lt;em&gt;path&lt;/em&gt; en el contenedor en donde está el certificado, con las configuraciones o variables de entorno &lt;code&gt;REGISTRY_HTTP_TLS_KEY&lt;/code&gt; y &lt;code&gt;REGISTRY_HTTP_TLS_CERTIFICATE&lt;/code&gt;. Como la imagen no lleva certificados, y para facilitar su cambio, vamos a montar los certificados como volúmenes locales.&lt;/p&gt;
&lt;p&gt;Como no tenemos dichos certificados, vamos a crearlos. Por economía voy a utilizar un certificado autofirmado, pero tal vez os interese utilizar uno firmado por una autoridad certificadora, como &lt;strong&gt;VeriSign&lt;/strong&gt; u otras. En el caso del certificado autofirmado, necesitaremos un paso adicional en cada cliente, que ya veremos.&lt;/p&gt;
&lt;p&gt;Creamos la carpeta de certificados (que luego montaremos), y generamos la clave y el certificado con los comandos habituales:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@registry:~/registry$ mkdir certs
gerard@registry:~/registry$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@registry:~/registry$ openssl req -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key -x509 -days &lt;span class="m"&gt;365&lt;/span&gt; -out certs/domain.crt
...
Common Name &lt;span class="o"&gt;(&lt;/span&gt;e.g. server FQDN or YOUR name&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;[]&lt;/span&gt;:registry.test
...
gerard@registry:~/registry$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: El campo CN es importante; debe coincidir con el dominio HTTPS que se solicite o el certificado será rechazado.&lt;/p&gt;
&lt;p&gt;Levantaremos con &lt;strong&gt;docker-compose&lt;/strong&gt; por comodidad y para facilitar el levantamiento futuro del mismo; solo hemos cambiado el puerto de servicio al 443 y hemos indicado la clave y el certificado, en la ruta que montamos como volúmen. El resto es a gusto del consumidor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@registry:~/registry$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  registry:
    image: registry:2
    container_name: registry
    hostname: registry
    environment:
      REGISTRY_HTTP_ADDR: &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:443
      REGISTRY_HTTP_TLS_KEY: /certs/domain.key
      REGISTRY_HTTP_TLS_CERTIFICATE: /certs/domain.crt
    volumes:
      - data:/var/lib/registry
      - ./certs:/certs:ro
    ports:
      - &lt;span class="s2"&gt;&amp;quot;443:443&amp;quot;&lt;/span&gt;
    restart: always
volumes:
  data:
gerard@registry:~/registry$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En este punto, nuestra carpeta de &lt;em&gt;runtime&lt;/em&gt; solo tiene 3 ficheros:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@registry:~/registry$ tree
.
├── certs
│   ├── domain.crt
│   └── domain.key
└── docker-compose.yml

&lt;span class="m"&gt;1&lt;/span&gt; directory, &lt;span class="m"&gt;3&lt;/span&gt; files
gerard@registry:~/registry$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Levantamos el servicio, y con esto tenemos el registro en funcionamiento, aunque sin autenticación por el momento.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@registry:~/registry$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;registry_default&amp;quot;&lt;/span&gt; with the default driver
Creating volume &lt;span class="s2"&gt;&amp;quot;registry_data&amp;quot;&lt;/span&gt; with default driver
Creating registry ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@registry:~/registry$
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Trabajando con el cliente&lt;/h3&gt;
&lt;p&gt;El funcionamiento en el cliente es el mismo de siempre; solo tenemos que preceder el nombre de la imagen por la URL del registro a utilizar. Para no crear mi propia imagen y emborronar el artículo, voy a descargar una cualquiera y a adueñármela:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ docker pull alpine
Using default tag: latest
latest: Pulling from library/alpine
4fe2ade4980c: Pull &lt;span class="nb"&gt;complete&lt;/span&gt;
Digest: sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528
Status: Downloaded newer image &lt;span class="k"&gt;for&lt;/span&gt; alpine:latest
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ docker tag alpine registry.test/alpine
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En este momento tenemos las dos imágenes, aunque se puede ver por el &lt;em&gt;image id&lt;/em&gt; que son las mismas.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ docker images
REPOSITORY             TAG                 IMAGE ID            CREATED             SIZE
alpine                 latest              196d12cf6ab1        &lt;span class="m"&gt;5&lt;/span&gt; weeks ago         &lt;span class="m"&gt;4&lt;/span&gt;.41MB
registry.test/alpine   latest              196d12cf6ab1        &lt;span class="m"&gt;5&lt;/span&gt; weeks ago         &lt;span class="m"&gt;4&lt;/span&gt;.41MB
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La subimos al registro con el correspondiente &lt;code&gt;docker pull&lt;/code&gt; y listo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ docker push registry.test/alpine
The push refers to repository &lt;span class="o"&gt;[&lt;/span&gt;registry.test/alpine&lt;span class="o"&gt;]&lt;/span&gt;
Get https://registry.test/v2/: x509: certificate signed by unknown authority
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: El &lt;em&gt;push&lt;/em&gt; ha fallado, porque el certificado no es confiable, al no estar firmado por ninguna autoridad certificadora. Si queremos que se acepte este certificado, necesitamos un paso adicional, que es el que sigue:&lt;/p&gt;
&lt;p&gt;Para que &lt;strong&gt;docker&lt;/strong&gt; confíe en un certificado no confiable, debemos añadir dicho certificado a la ruta &lt;code&gt;/etc/docker/certs.d/&amp;lt;dominio&amp;gt;/ca.crt&lt;/code&gt;. Este &lt;code&gt;ca.crt&lt;/code&gt; no es otro que el certificado del registro (no la clave), que hemos llamado &lt;code&gt;domain.crt&lt;/code&gt; en el servidor del registro.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ sudo mkdir -p /etc/docker/certs.d/registry.test
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ sudo cat /etc/docker/certs.d/registry.test/ca.crt
-----BEGIN CERTIFICATE-----
...
-----END CERTIFICATE-----
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;No es necesario reiniciar nada. Relanzamos el &lt;code&gt;docker push&lt;/code&gt; y ya debería funcionar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ docker push registry.test/alpine
The push refers to repository &lt;span class="o"&gt;[&lt;/span&gt;registry.test/alpine&lt;span class="o"&gt;]&lt;/span&gt;
df64d3292fd6: Pushed
latest: digest: sha256:02892826401a9d18f0ea01f8a2f35d328ef039db4e1edcc45c630314a0457d5b size: &lt;span class="m"&gt;528&lt;/span&gt;
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos verificar que el registro contiene la imagen consultando su propia API:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ curl -k https://registry.test/v2/_catalog
{&amp;quot;repositories&amp;quot;:[&amp;quot;alpine&amp;quot;]}
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Esto nos demuestra que el registro privado funciona según lo esperado.&lt;/p&gt;
&lt;h2&gt;Habilitando la autenticación&lt;/h2&gt;
&lt;p&gt;Vamos a utilizar autenticación básica por su simplicidad, pero hay varios métodos posibles. Para ello vamos a utilizar la misma técnica: indicar autenticación básica mediante variables de entorno, indicando el &lt;em&gt;path&lt;/em&gt; a un fichero de autenticación que vamos a montar como volúmen.&lt;/p&gt;
&lt;p&gt;Generamos un fichero &lt;code&gt;htpasswd&lt;/code&gt; estándar, que se puede crear mediante la misma imagen del registro:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@registry:~/registry$ mkdir auth
gerard@registry:~/registry$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@registry:~/registry$ docker run --entrypoint htpasswd --rm registry:2 -Bbn user p4ssw0rd &amp;gt; auth/htpasswd
gerard@registry:~/registry$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Es posible crear varios usuarios, pero no es muy útil; todos ellos van a poder ver las mismas imágenes y modificarlas a placer.&lt;/p&gt;
&lt;p&gt;En este punto tenemos un fichero nuevo con los usuarios aceptados; si alguna vez tenemos que cambiarlos, como no forman parte de la imagen, basta con "dar el cambiazo".&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@registry:~/registry$ cat auth/htpasswd
user:&lt;span class="nv"&gt;$2&lt;/span&gt;y&lt;span class="nv"&gt;$05$M&lt;/span&gt;/IbI44MSrDFj9bcuFRPt.6tiit1r0V1.KCy2tf4hAzNuznqR9cXG
gerard@registry:~/registry$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El número de ficheros de &lt;em&gt;runtime&lt;/em&gt; no ha incrementado casi nada:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@registry:~/registry$ tree
.
├── auth
│   └── htpasswd
├── certs
│   ├── domain.crt
│   └── domain.key
└── docker-compose.yml

&lt;span class="m"&gt;2&lt;/span&gt; directories, &lt;span class="m"&gt;4&lt;/span&gt; files
gerard@registry:~/registry$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo vamos a necesitar algunas modificaciones en el &lt;em&gt;docker-compose.yml&lt;/em&gt; para añadir el volumen de autenticación y las variables de entorno que la activan.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@registry:~/registry$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  registry:
    image: registry:2
    container_name: registry
    hostname: registry
    environment:
      REGISTRY_HTTP_ADDR: &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:443
      REGISTRY_HTTP_TLS_KEY: /certs/domain.key
      REGISTRY_HTTP_TLS_CERTIFICATE: /certs/domain.crt
      REGISTRY_AUTH: htpasswd
      REGISTRY_AUTH_HTPASSWD_REALM: LinuxSysadmin registry
      REGISTRY_AUTH_HTPASSWD_PATH: /auth/htpasswd
    volumes:
      - data:/var/lib/registry
      - ./certs:/certs:ro
      - ./auth:/auth:ro
    ports:
      - &lt;span class="s2"&gt;&amp;quot;443:443&amp;quot;&lt;/span&gt;
    restart: always
volumes:
  data:
gerard@registry:~/registry$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a levantar de nuevo el servicio para que apliquen los cambios:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@registry:~/registry$ docker-compose up -d
Recreating registry ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@registry:~/registry$
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Verificando la autenticación&lt;/h3&gt;
&lt;p&gt;El primer indicio de que algo falla es que no podemos consultar la API, ni descargar la imagen:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ curl -k https://registry.test/v2/_catalog
&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;errors&amp;quot;&lt;/span&gt;:&lt;span class="o"&gt;[{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;code&amp;quot;&lt;/span&gt;:&lt;span class="s2"&gt;&amp;quot;UNAUTHORIZED&amp;quot;&lt;/span&gt;,&lt;span class="s2"&gt;&amp;quot;message&amp;quot;&lt;/span&gt;:&lt;span class="s2"&gt;&amp;quot;authentication required&amp;quot;&lt;/span&gt;,&lt;span class="s2"&gt;&amp;quot;detail&amp;quot;&lt;/span&gt;:&lt;span class="o"&gt;[{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt;:&lt;span class="s2"&gt;&amp;quot;registry&amp;quot;&lt;/span&gt;,&lt;span class="s2"&gt;&amp;quot;Class&amp;quot;&lt;/span&gt;:&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;,&lt;span class="s2"&gt;&amp;quot;Name&amp;quot;&lt;/span&gt;:&lt;span class="s2"&gt;&amp;quot;catalog&amp;quot;&lt;/span&gt;,&lt;span class="s2"&gt;&amp;quot;Action&amp;quot;&lt;/span&gt;:&lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}]}]}&lt;/span&gt;
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ docker pull registry.test/alpine
Using default tag: latest
Error response from daemon: Get https://registry.test/v2/alpine/manifests/latest: no basic auth credentials
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En el caso de la API, podemos usar el &lt;em&gt;flag&lt;/em&gt; que &lt;strong&gt;curl&lt;/strong&gt; nos ofrece, que ya gestiona la parte de la autenicación básica:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ curl -k -u user:p4ssw0rd https://registry.test/v2/_catalog
&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;repositories&amp;quot;&lt;/span&gt;:&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;alpine&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]}&lt;/span&gt;
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para poder utilizar &lt;strong&gt;docker&lt;/strong&gt;, vamos a necesitar hacer &lt;em&gt;login&lt;/em&gt;. &lt;strong&gt;Docker&lt;/strong&gt; ya nos ofrece esta facilidad.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ docker login registry.test
Username: user
Password:
WARNING! Your password will be stored unencrypted in /home/gerard/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Una vez hecho el &lt;em&gt;login&lt;/em&gt;, ya podemos hacer las operaciones de &lt;em&gt;push&lt;/em&gt; y de &lt;em&gt;pull&lt;/em&gt; al registro.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ docker pull registry.test/alpine
Using default tag: latest
latest: Pulling from alpine
4fe2ade4980c: Pull &lt;span class="nb"&gt;complete&lt;/span&gt;
Digest: sha256:02892826401a9d18f0ea01f8a2f35d328ef039db4e1edcc45c630314a0457d5b
Status: Downloaded newer image &lt;span class="k"&gt;for&lt;/span&gt; registry.test/alpine:latest
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: El ejemplo parte de un servidor sin imágenes.&lt;/p&gt;
&lt;p&gt;Solo nos falta ver que la imagen está disponible en el servidor local:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ docker images
REPOSITORY             TAG                 IMAGE ID            CREATED             SIZE
registry.test/alpine   latest              196d12cf6ab1        &lt;span class="m"&gt;5&lt;/span&gt; weeks ago         &lt;span class="m"&gt;4&lt;/span&gt;.41MB
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Las credenciales se guardan en &lt;code&gt;~/.docker/config.json&lt;/code&gt;, de una forma poco segura; es mejor ir haciendo &lt;em&gt;login&lt;/em&gt; y &lt;em&gt;logout&lt;/em&gt; entre operaciones. Alternativamente a las operationes de &lt;em&gt;login&lt;/em&gt; y &lt;em&gt;logout&lt;/em&gt;, podemos ir creando y eliminando este fichero según convenga.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ cat .docker/config.json
&lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;auths&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;registry.test&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
                        &lt;span class="s2"&gt;&amp;quot;auth&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;dXNlcjpwNHNzdzByZA==&amp;quot;&lt;/span&gt;
                &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;HttpHeaders&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;User-Agent&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;Docker-Client/18.06.1-ce (linux)&amp;quot;&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Como ejemplo de la seguridad del fichero, solo hace falta ver que huele a una cadena en &lt;strong&gt;base64&lt;/strong&gt;; descodificarla es trivial:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node01:~$ &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;dXNlcjpwNHNzdzByZA&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; base64 -d
user:p4ssw0rd
gerard@node01:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Conclusiones&lt;/h2&gt;
&lt;p&gt;Tener un registro privado de &lt;strong&gt;docker&lt;/strong&gt;, seguro y restringido, es relativamente fácil. Vale la pena dedicar un poco de esfuerzo al principio para que dicho servidor no sea la fuente de nuestras preocupaciones por su falta de seguridad o intrusiones futuras.&lt;/p&gt;
&lt;p&gt;Estos pasos se hacen una sola vez por cada registro y no se tocan casi nunca, salvo renovación de certificados o de credenciales. Gracias a &lt;strong&gt;docker-compose&lt;/strong&gt;, esto también es trivial...&lt;/p&gt;</content><category term="docker"></category><category term="registro"></category><category term="ssl"></category><category term="tls"></category><category term="autenticación"></category><category term="autenticacion basica"></category></entry><entry><title>Usando Traefik en un cluster de Docker Swarm</title><link href="https://www.linuxsysadmin.ml/2018/10/usando-traefik-en-un-cluster-de-docker-swarm.html" rel="alternate"></link><published>2018-10-29T10:00:00+01:00</published><updated>2018-10-29T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-10-29:/2018/10/usando-traefik-en-un-cluster-de-docker-swarm.html</id><summary type="html">&lt;p&gt;Hace unas semanas, hablamos de un balanceador que trabaja muy bien con &lt;strong&gt;docker&lt;/strong&gt;. Se trataba de &lt;strong&gt;traefik&lt;/strong&gt; y nos permitía olvidarnos de su configuración, que él mismo podía extraer de los metadatos de los contenedores y reconfigurarse dinámicamente. Hoy vamos a explicar como funciona con un &lt;em&gt;cluster&lt;/em&gt; de &lt;strong&gt;docker swarm …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Hace unas semanas, hablamos de un balanceador que trabaja muy bien con &lt;strong&gt;docker&lt;/strong&gt;. Se trataba de &lt;strong&gt;traefik&lt;/strong&gt; y nos permitía olvidarnos de su configuración, que él mismo podía extraer de los metadatos de los contenedores y reconfigurarse dinámicamente. Hoy vamos a explicar como funciona con un &lt;em&gt;cluster&lt;/em&gt; de &lt;strong&gt;docker swarm&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Para ello vamos a partir de un &lt;em&gt;swarm&lt;/em&gt; bastante simple de dos nodos (un &lt;em&gt;manager&lt;/em&gt; y un &lt;em&gt;worker&lt;/em&gt;). No es el ideal, pero es lo mínimo que puedo virtualizar sin acabar con los recursos de mi máquina y sin complicar demasiado las cosas.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@manager:~$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
d9uluevfbh7vftbnhf2upmdnw *   manager             Ready               Active              Leader              &lt;span class="m"&gt;18&lt;/span&gt;.06.1-ce
83tb1sa8l1z06h7vl6c4f4ucd     worker              Ready               Active                                  &lt;span class="m"&gt;18&lt;/span&gt;.06.1-ce
gerard@manager:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para asegurar que el balanceador está en la misma red que los contenedores (y por lo tanto, les pueda pasar peticiones), vamos a crear una red &lt;em&gt;overlay&lt;/em&gt; que permita comunicarse a todos los contenedores de forma independiente del &lt;em&gt;host&lt;/em&gt; en el que se encuentren.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@manager:~$ docker network create --driver&lt;span class="o"&gt;=&lt;/span&gt;overlay traefik-net
pe1s0yl4r402jagfgapmo10oc
gerard@manager:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;AVISO&lt;/strong&gt;: Por algún motivo, este comando creó una red que en el mismo rango que la red de los servidores del &lt;em&gt;swarm&lt;/em&gt;. Esto da muchos problemas de comunicación en el futuro. Simplemente cread otra, para que la dirección de red cambie.&lt;/p&gt;
&lt;h2&gt;El balanceador&lt;/h2&gt;
&lt;p&gt;Levantar el balanceador es tan fácil como poner un contenedor que ejecute la imagen oficial &lt;strong&gt;traefik&lt;/strong&gt;; partimos del artículo anterior sobre &lt;a href="https://www.linuxsysadmin.ml/2018/09/un-balanceador-dinamico-para-docker-traefik.html"&gt;este servicio&lt;/a&gt;. El único &lt;em&gt;flag&lt;/em&gt; añadido es &lt;code&gt;--docker.swarmMode&lt;/code&gt;, que es el que indica que el balanceador tiene que sacar los metadatos del &lt;em&gt;cluster swarm&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Para poder leer la información del &lt;em&gt;cluster&lt;/em&gt;, es condición necesaria que se ejecute en un &lt;em&gt;manager&lt;/em&gt;. Ello lo podemos conseguir mediante las &lt;em&gt;constraints&lt;/em&gt; de &lt;em&gt;placement&lt;/em&gt;. Otra decisión de diseño es que voy a ejecutar un &lt;strong&gt;traefik&lt;/strong&gt; en cada &lt;em&gt;manager&lt;/em&gt; con &lt;code&gt;mode: global&lt;/code&gt; y con las restricciones anteriores (aunque en este caso solo hay uno).&lt;/p&gt;
&lt;p&gt;También quiero quiero que el puerto 8080 de cada &lt;em&gt;manager&lt;/em&gt; sea ese &lt;strong&gt;traefik&lt;/strong&gt; concreto (&lt;code&gt;mode: host&lt;/code&gt;), y no el balanceador &lt;em&gt;ingress&lt;/em&gt; que viene por defecto. Por supuesto, el balanceador va a estar en la red de servicio antes creada, en donde también pondremos los contenedores de servicio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@manager:~/traefik$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3.2&amp;#39;&lt;/span&gt;
services:
  traefik:
    image: traefik
    command: --api --docker --docker.swarmMode --docker.exposedbydefault&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt;
    ports:
      - target: &lt;span class="m"&gt;80&lt;/span&gt;
        published: &lt;span class="m"&gt;80&lt;/span&gt;
        protocol: tcp
        mode: host
      - target: &lt;span class="m"&gt;8080&lt;/span&gt;
        published: &lt;span class="m"&gt;8080&lt;/span&gt;
        protocol: tcp
        mode: host
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - traefik-net
    deploy:
      mode: global
      placement:
        constraints:
          - node.role &lt;span class="o"&gt;==&lt;/span&gt; manager
networks:
  traefik-net:
    external: &lt;span class="nb"&gt;true&lt;/span&gt;
gerard@manager:~/traefik$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Desplegamos el &lt;em&gt;stack&lt;/em&gt; de un solo servicio que hemos creado y verificamos que está corriendo en todos los &lt;em&gt;managers&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@manager:~/traefik$ docker stack deploy -c docker-compose.yml traefik
Creating service traefik_traefik
gerard@manager:~/traefik$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@manager:~/traefik$ docker stack ps traefik
ID                  NAME                                        IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
k83hxjvqmp6n        traefik_traefik.d9uluevfbh7vftbnhf2upmdnw   traefik:latest      manager             Running             Running &lt;span class="m"&gt;12&lt;/span&gt; minutes ago                     
gerard@manager:~/traefik$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos ver que &lt;strong&gt;traefik&lt;/strong&gt; responde solo en la máquina en la que está ejecutando, y no en el resto. Así nos ahorramos balancear el balanceador.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@gateway:~$ curl http://manager:8080/
&amp;lt;a &lt;span class="nv"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/dashboard/&amp;quot;&lt;/span&gt;&amp;gt;Found&amp;lt;/a&amp;gt;.
gerard@gateway:~$ curl http://worker:8080/
curl: &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; Failed to connect to worker port &lt;span class="m"&gt;8080&lt;/span&gt;: Conexión rehusada
gerard@gateway:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto el balanceador está listo.&lt;/p&gt;
&lt;h2&gt;Un servicio de ejemplo&lt;/h2&gt;
&lt;p&gt;Desde el punto de vista de los servicios, no cambia nada; cada &lt;strong&gt;traefik&lt;/strong&gt; se actualizará con lo que lea de los metadatos del &lt;em&gt;cluster&lt;/em&gt;. Solo hay que recordar que debe estar en la misma red que los balanceadores...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@manager:~/whoami$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  whoami:
    image: emilevauge/whoami
    networks:
      - traefik-net
    deploy:
      replicas: &lt;span class="m"&gt;4&lt;/span&gt;
      labels:
        traefik.frontend.rule: Host:whoami.docker.localhost
        traefik.port: &lt;span class="m"&gt;80&lt;/span&gt;
        traefik.enable: &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;
networks:
  traefik-net:
    external: &lt;span class="nb"&gt;true&lt;/span&gt;
gerard@manager:~/whoami$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Tras desplegar el servicio y ver que todas las instáncias están funcionando, podremos empezar las pruebas.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@manager:~/whoami$ docker stack deploy -c docker-compose.yml whoami
Creating service whoami_whoami
gerard@manager:~/whoami$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@manager:~/whoami$ docker stack ps whoami
ID                  NAME                  IMAGE                      NODE                DESIRED STATE       CURRENT STATE             ERROR               PORTS
hgsz3vgfyorj        whoami_whoami.1       emilevauge/whoami:latest   worker              Running             Running &lt;span class="m"&gt;30&lt;/span&gt; minutes ago
ita8jth3nxvn        whoami_whoami.2       emilevauge/whoami:latest   manager             Running             Running &lt;span class="m"&gt;22&lt;/span&gt; minutes ago
pib7gf0dixjl        whoami_whoami.3       emilevauge/whoami:latest   worker              Running             Running &lt;span class="m"&gt;2&lt;/span&gt; minutes ago
s9c965gqq2gy        whoami_whoami.4       emilevauge/whoami:latest   manager             Running             Running &lt;span class="m"&gt;2&lt;/span&gt; minutes ago
gerard@manager:~/whoami$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Las pruebas son tan simples como verificar que realmente se está balanceando entre todos los contenedores que, debido a la imagen usada, es trivial.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@gateway:~$ curl -sH &lt;span class="s2"&gt;&amp;quot;Host: whoami.docker.localhost&amp;quot;&lt;/span&gt; http://manager/ &lt;span class="p"&gt;|&lt;/span&gt; grep Hostname
Hostname: 8c98d5545ce6
gerard@gateway:~$ curl -sH &lt;span class="s2"&gt;&amp;quot;Host: whoami.docker.localhost&amp;quot;&lt;/span&gt; http://manager/ &lt;span class="p"&gt;|&lt;/span&gt; grep Hostname
Hostname: a9b2b58e98bb
gerard@gateway:~$ curl -sH &lt;span class="s2"&gt;&amp;quot;Host: whoami.docker.localhost&amp;quot;&lt;/span&gt; http://manager/ &lt;span class="p"&gt;|&lt;/span&gt; grep Hostname
Hostname: 2eb66b929b01
gerard@gateway:~$ curl -sH &lt;span class="s2"&gt;&amp;quot;Host: whoami.docker.localhost&amp;quot;&lt;/span&gt; http://manager/ &lt;span class="p"&gt;|&lt;/span&gt; grep Hostname
Hostname: 9040b20f948e
gerard@gateway:~$ curl -sH &lt;span class="s2"&gt;&amp;quot;Host: whoami.docker.localhost&amp;quot;&lt;/span&gt; http://manager/ &lt;span class="p"&gt;|&lt;/span&gt; grep Hostname
Hostname: 8c98d5545ce6
gerard@gateway:~$ curl -sH &lt;span class="s2"&gt;&amp;quot;Host: whoami.docker.localhost&amp;quot;&lt;/span&gt; http://manager/ &lt;span class="p"&gt;|&lt;/span&gt; grep Hostname
Hostname: a9b2b58e98bb
gerard@gateway:~$ curl -sH &lt;span class="s2"&gt;&amp;quot;Host: whoami.docker.localhost&amp;quot;&lt;/span&gt; http://manager/ &lt;span class="p"&gt;|&lt;/span&gt; grep Hostname
Hostname: 2eb66b929b01
gerard@gateway:~$ curl -sH &lt;span class="s2"&gt;&amp;quot;Host: whoami.docker.localhost&amp;quot;&lt;/span&gt; http://manager/ &lt;span class="p"&gt;|&lt;/span&gt; grep Hostname
Hostname: 9040b20f948e
gerard@gateway:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para desplegar el servicio no se necesita tocar el balanceador; aquí reside la fuerza de &lt;strong&gt;traefik&lt;/strong&gt;. De hecho, podemos desplegar un &lt;em&gt;stack&lt;/em&gt; nuevo con una segunda versión, y al rato eliminar el viejo; con eso tendríamos un despliegue sin cortes y, con un poco de juego de etiquetas, un &lt;a href="https://www.linuxsysadmin.ml/2018/05/despliegues-sin-corte-de-servicio-blue-green-deployments.html"&gt;blue-green deployment&lt;/a&gt; sin complicaciones.&lt;/p&gt;
&lt;h2&gt;Otras posibles mejoras&lt;/h2&gt;
&lt;p&gt;Esta es una lista con las ideas que todavía quedan en el tintero, y que pueden ayudarnos a crear el &lt;em&gt;cluster&lt;/em&gt; perfecto, aunque no las he implementado:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Incrementar los &lt;em&gt;managers&lt;/em&gt; para tener alta disponibilidad, tanto del balanceador, como del &lt;em&gt;swarm&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Se van a necesitar más nodos &lt;em&gt;worker&lt;/em&gt; para repartir la carga de contenedores y servicios&lt;/li&gt;
&lt;li&gt;Se recomienda limitar el despliegue de servicios solamente en nodos &lt;em&gt;workers&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Podemos tener un IP flotante entre los &lt;em&gt;managers&lt;/em&gt; usando algo como &lt;strong&gt;keepalived&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="traefik"></category><category term="docker"></category><category term="swarm"></category></entry><entry><title>DNSmasq como una utilidad de cache DNS local usando Docker</title><link href="https://www.linuxsysadmin.ml/2018/10/dnsmasq-como-una-utilidad-de-cache-dns-local-usando-docker.html" rel="alternate"></link><published>2018-10-01T10:00:00+02:00</published><updated>2018-10-01T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-10-01:/2018/10/dnsmasq-como-una-utilidad-de-cache-dns-local-usando-docker.html</id><summary type="html">&lt;p&gt;Ya vimos en otros artículos lo fácilmente que podemos utilizar &lt;strong&gt;dnsmasq&lt;/strong&gt; en un &lt;em&gt;gateway&lt;/em&gt; para ayudar a los servidores a que se conozcan entre sí por nombre y como una forma de ocultar el DNS real de la red interna. Sin embargo es una caché excelente para un sistema aislado …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ya vimos en otros artículos lo fácilmente que podemos utilizar &lt;strong&gt;dnsmasq&lt;/strong&gt; en un &lt;em&gt;gateway&lt;/em&gt; para ayudar a los servidores a que se conozcan entre sí por nombre y como una forma de ocultar el DNS real de la red interna. Sin embargo es una caché excelente para un sistema aislado.&lt;/p&gt;
&lt;p&gt;Cualquier sistema, sea un servidor o no, puede utilizar un proceso &lt;strong&gt;dnsmasq&lt;/strong&gt; local como caché y como forma de resolver por nombre entornos que todavia no tienen un DNS disponible en internet. Realmente es un proceso que no consume casi ninguna memoria y nos puede ayudar mucho en caso de una caída del DNS global, o en caso de desconectarnos de la red por cualquier motivo.&lt;/p&gt;
&lt;p&gt;Lo que suele ser problemático es instalarlo en nuestro sistema, por un tema de permisos o de conflictos; en estos casos podemos confiar en convertirlo en un contenedor, que no pasa de los 5mb de disco.&lt;/p&gt;
&lt;h2&gt;La imagen con DNSmasq&lt;/h2&gt;
&lt;p&gt;La imagen en sí misma no oculta ninguna complicación; se trata de instalar &lt;strong&gt;dnsmasq&lt;/strong&gt; en nuestro sistema base y asegurarnos que ejectua en &lt;em&gt;foreground&lt;/em&gt;. Para evitar una imagen muy grande, podemos utilizar &lt;strong&gt;Alpine Linux&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/dnsmasq$ cat build/Dockerfile
FROM alpine:3.8
RUN apk add --no-cache dnsmasq
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/sbin/dnsmasq&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-k&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@atlantis:~/workspace/dnsmasq$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para construir y levantar el servicio, podemos utilizar &lt;strong&gt;docker-compose&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/dnsmasq$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  dnsmasq:
    image: dnsmasq
    build: build
    container_name: dnsmasq
    hostname: dnsmasq
    cap_add:
      - NET_ADMIN
    network_mode: host
    restart: always
    volumes:
      - ./hosts:/etc/hosts:ro
gerard@atlantis:~/workspace/dnsmasq$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/dnsmasq$ docker-compose build
Building dnsmasq
...
Successfully tagged dnsmasq:latest
gerard@atlantis:~/workspace/dnsmasq$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
...
dnsmasq             latest              b43fd1b79394        &lt;span class="m"&gt;7&lt;/span&gt; minutes ago       &lt;span class="m"&gt;4&lt;/span&gt;.76MB
...
gerard@atlantis:~/workspace/dnsmasq$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto tenemos nuestra imagen.&lt;/p&gt;
&lt;h2&gt;Ejecutando el contenedor&lt;/h2&gt;
&lt;p&gt;Como decisión de diseño, he preferido montar el fichero &lt;code&gt;/etc/hosts&lt;/code&gt; desde una carpeta local, para poder modificarlo a placer y sin tener permisos de superusuario.&lt;/p&gt;
&lt;p&gt;Levantamos el servicio encima de la red local; de esta forma, crearemos la ilusión de que &lt;strong&gt;dnsmasq&lt;/strong&gt; corre en la misma máquina que la va a utilizar directamente en &lt;em&gt;localhost&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: &lt;strong&gt;dnsmasq&lt;/strong&gt; utiliza algunas operaciones especiales de redes. Podéis ejecutar en modo privilegiado (&lt;code&gt;--privileged&lt;/code&gt;) o podéis darle la &lt;em&gt;capability&lt;/em&gt; &lt;code&gt;NET_ADMIN&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/dnsmasq$ docker-compose up -d
Creating dnsmasq ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@atlantis:~/workspace/dnsmasq$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto tenemos el servicio &lt;strong&gt;dnsmasq&lt;/strong&gt; accesible desde nuestro servidor, en el puerto estándar:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/dnsmasq$ ss -lnt &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="m"&gt;53&lt;/span&gt;
LISTEN     &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;32&lt;/span&gt;           *:53                       *:*
LISTEN     &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;32&lt;/span&gt;          :::53                      :::*
gerard@atlantis:~/workspace/dnsmasq$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Utilizando el nuevo servidor DNS&lt;/h2&gt;
&lt;p&gt;Solo falta indicar que el servidor DNS que nuestra máquina debe utilizar es &lt;em&gt;localhost&lt;/em&gt; en &lt;code&gt;/etc/resolv.conf&lt;/code&gt;, posiblemente seguido de otros. De esta forma, la primera petición DNS fallará, se irá a buscar en el siguiente servidor, y se guardará en &lt;em&gt;caché&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/dnsmasq$ cat /etc/resolv.conf
domain ...
search ...
nameserver &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1
nameserver ...
nameserver ...
gerard@atlantis:~/workspace/dnsmasq$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En el caso de utilizar DHCP para obtener dirección IP automáticamente, el fichero &lt;code&gt;/etc/resolv.conf&lt;/code&gt; se sobreescribe con lo que nos pase el &lt;em&gt;router&lt;/em&gt;. Podemos instruir al cliente de DHCP para que siempre nos añada &lt;em&gt;localhost&lt;/em&gt; en el fichero (es lo que tuve que hacer yo).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/dnsmasq$ cat /etc/dhcp/dhclient.conf 
...
prepend domain-name-servers &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1&lt;span class="p"&gt;;&lt;/span&gt;
gerard@atlantis:~/workspace/dnsmasq$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Probando el servidor DNS&lt;/h2&gt;
&lt;p&gt;El paso más interesante de este &lt;em&gt;setup&lt;/em&gt; es la capacidad de &lt;em&gt;cachear&lt;/em&gt; las peticiones DNS, y esto lo podemos probar haciendo simplemente varias peticiones:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@atlantis:~# dig www.linuxsysadmin.ml
...
&lt;span class="p"&gt;;;&lt;/span&gt; Query time: &lt;span class="m"&gt;30&lt;/span&gt; msec
&lt;span class="p"&gt;;;&lt;/span&gt; SERVER: &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1#53&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;127&lt;/span&gt;.0.0.1&lt;span class="o"&gt;)&lt;/span&gt;
...
root@atlantis:~# dig www.linuxsysadmin.ml
...
&lt;span class="p"&gt;;;&lt;/span&gt; Query time: &lt;span class="m"&gt;0&lt;/span&gt; msec
&lt;span class="p"&gt;;;&lt;/span&gt; SERVER: &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1#53&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;127&lt;/span&gt;.0.0.1&lt;span class="o"&gt;)&lt;/span&gt;
...
root@atlantis:~# dig www.linuxsysadmin.ml
...
&lt;span class="p"&gt;;;&lt;/span&gt; Query time: &lt;span class="m"&gt;0&lt;/span&gt; msec
&lt;span class="p"&gt;;;&lt;/span&gt; SERVER: &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1#53&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;127&lt;/span&gt;.0.0.1&lt;span class="o"&gt;)&lt;/span&gt;
...
root@atlantis:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemo ver claramente dos cosas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Estamos utilizando nuestro servidor local DNS&lt;/li&gt;
&lt;li&gt;La primera petición no está en &lt;em&gt;caché&lt;/em&gt; y tarda un poco, pero el resto son inmediatas&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La otra funcionalidad que ganamos con &lt;strong&gt;dnsmasq&lt;/strong&gt; es la de resolver nombres desde el fichero &lt;code&gt;/etc/hosts&lt;/code&gt; del contenedor. Para evitarme modificar sistemas de ficheros privilegiados, el &lt;em&gt;docker-compose.yml&lt;/em&gt; añade este fichero desde un fichero local, que podemos modificar a placer segun nuestros gustos (no os olvidéis de reiniciar el contenedor para que pille los cambios).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/dnsmasq$ cat hosts
&lt;span class="m"&gt;127&lt;/span&gt;.0.0.1 test.api.local
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3 auth.api.private
gerard@atlantis:~/workspace/dnsmasq$ docker-compose restart
Restarting dnsmasq ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@atlantis:~/workspace/dnsmasq$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto podemos resolver dominios que no existen fuera de nuestra red, con la comodidad de hacerlo por nombre, y sin modificar direcciones IP si los recolocamos en otros servidores.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@atlantis:~# dig test.api.local +short
&lt;span class="m"&gt;127&lt;/span&gt;.0.0.1
root@atlantis:~# dig auth.api.private +short
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3
root@atlantis:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto ya podemos decir que lo tenemos todo funcionando.&lt;/p&gt;</content><category term="dnsmasq"></category><category term="docker"></category></entry><entry><title>Un contenedor multiservicio con docker y s6</title><link href="https://www.linuxsysadmin.ml/2018/09/un-contenedor-multiservicio-con-docker-y-s6.html" rel="alternate"></link><published>2018-09-24T10:00:00+02:00</published><updated>2018-09-24T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-09-24:/2018/09/un-contenedor-multiservicio-con-docker-y-s6.html</id><summary type="html">&lt;p&gt;Lo he vuelto a hacer: a pesar de que es una antipráctica de &lt;strong&gt;docker&lt;/strong&gt;, me veo tentado a ejecutar varios servicios en mis contenedores. Solo lo hago cuando estos servicios tienen un objetivo común, como servir PHP (nginx/php-fpm); para ello necesitamos un gestor de procesos. Hoy hablaremos de &lt;strong&gt;s6 …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Lo he vuelto a hacer: a pesar de que es una antipráctica de &lt;strong&gt;docker&lt;/strong&gt;, me veo tentado a ejecutar varios servicios en mis contenedores. Solo lo hago cuando estos servicios tienen un objetivo común, como servir PHP (nginx/php-fpm); para ello necesitamos un gestor de procesos. Hoy hablaremos de &lt;strong&gt;s6&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Hasta ahora, el servicio que utilizaba era &lt;a href="https://www.linuxsysadmin.ml/2017/03/multiples-servicios-en-un-mismo-contenedor-docker.html"&gt;runit&lt;/a&gt;, pero existe un servicio similar llamado &lt;strong&gt;s6&lt;/strong&gt; que además sirve como &lt;a href="https://www.linuxsysadmin.ml/2017/09/un-proceso-inicial-para-docker-tini-y-dumb-init.html"&gt;&lt;em&gt;init&lt;/em&gt; correcto&lt;/a&gt; para &lt;strong&gt;docker&lt;/strong&gt;. Por supuesto, no he podido resistir la tentación de darle un intento, y el resultado me ha gustado.&lt;/p&gt;
&lt;h2&gt;Funcionamiento de s6&lt;/h2&gt;
&lt;p&gt;Cuando ejecutamos &lt;strong&gt;s6&lt;/strong&gt;, ejecutamos el binario &lt;code&gt;s6-svscan&lt;/code&gt; que monitoriza una carpeta concreta. En esta carpeta tenemos una subcarpeta por servicio; cada subcarpeta es la definición de un servicio y &lt;code&gt;s6-svscan&lt;/code&gt; va a lanzar sobre la misma el proceso &lt;code&gt;s6-supervise&lt;/code&gt;, que se encarga de mantener ese proceso concreto.&lt;/p&gt;
&lt;p&gt;Esta subcarpeta tiene 3 tipos de ficheros y carpetas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Obligatoriamente un binario, enlace o &lt;em&gt;script&lt;/em&gt; llamado &lt;code&gt;run&lt;/code&gt;, que es lo que se ejecutará para levantar el servicio&lt;/li&gt;
&lt;li&gt;Opcionalmente un binario, enlace o &lt;em&gt;script&lt;/em&gt; llamado &lt;code&gt;finish&lt;/code&gt;, que se llamará cuando el servicio acabe por cualquier motivo&lt;/li&gt;
&lt;li&gt;Carpetas &lt;code&gt;event&lt;/code&gt; y &lt;code&gt;status&lt;/code&gt; que son efímeras y las usa &lt;strong&gt;s6&lt;/strong&gt; para mantener el estado del proceso&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Adicionalmente, la carpeta de servicios tiene un "servicio" especial: &lt;code&gt;.s6-svscan&lt;/code&gt; que dispone de dos binarios, enalces o &lt;em&gt;scripts&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;finish&lt;/code&gt; &amp;rarr; se va a ejecutar cuando &lt;code&gt;s6-svscan&lt;/code&gt; acaba por la causa que sea (es opcional, pero salta un &lt;em&gt;warning&lt;/em&gt; si no lo encuentra)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;crash&lt;/code&gt; &amp;rarr; se ejecuta si el proceso &lt;code&gt;s6-svscan&lt;/code&gt; acaba anormalmente, y es opcional&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Por lo tanto, un &lt;em&gt;setup&lt;/em&gt; típico incluye crear &lt;code&gt;.s6-svscan/finish&lt;/code&gt; y varios &lt;code&gt;&amp;lt;servicio&amp;gt;/run&lt;/code&gt;. Con esto es suficiente.&lt;/p&gt;
&lt;h2&gt;Un ejemplo: contenedor con servidor web y SFTP&lt;/h2&gt;
&lt;p&gt;El primer paso es definir la carpeta de servicios, de donde va a leer &lt;code&gt;s6-svscan&lt;/code&gt;. En este ejemplo vamos a utilizar &lt;code&gt;/etc/s6&lt;/code&gt; y en ella vamos a definir:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nginx/run&lt;/code&gt; &amp;rarr; para mantener levantado un servidor web &lt;strong&gt;nginx&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ssh/run&lt;/code&gt; &amp;rarr; para mantener el servidor SSH/SFTP levantado&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.s6-svscan/finish&lt;/code&gt; &amp;rarr; no hace nada, pero es para suprimir el &lt;em&gt;warning&lt;/em&gt; al acabar el contenedor&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/miniserver$ tree -a
.
├── s6
│   ├── nginx
│   │   └── run
│   ├── .s6-svscan
│   │   └── finish
│   └── ssh
│       └── run
├── Dockerfile
└── nginx.conf

&lt;span class="m"&gt;4&lt;/span&gt; directories, &lt;span class="m"&gt;5&lt;/span&gt; files
gerard@atlantis:~/workspace/miniserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: No ponemos &lt;em&gt;scripts&lt;/em&gt; de &lt;code&gt;finish&lt;/code&gt; en los servicios porque no queremos hacer nada cuando acaben, más allá de su reinicio por parte de &lt;strong&gt;s6&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Si la caída de un servicio es suficientemente grave como para querer parar el contenedor, podéis poner &lt;code&gt;s6-svscanctl -t /etc/s6&lt;/code&gt; en el script &lt;code&gt;finish&lt;/code&gt; de ese servicio para parar &lt;strong&gt;s6&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Lo importante de los &lt;em&gt;scripts&lt;/em&gt; de &lt;code&gt;run&lt;/code&gt; es que no acaben, lo que se interpreta como servicio acabado (y candidato a levantar de nuevo). Este paradigma no nos es nuevo en &lt;strong&gt;docker&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;En cuanto a los &lt;em&gt;scripts&lt;/em&gt; en sí mismos, no hacen nada especialmente complicado:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/miniserver$ cat s6/nginx/run
&lt;span class="c1"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="nb"&gt;exec&lt;/span&gt; /usr/sbin/nginx -g &lt;span class="s2"&gt;&amp;quot;daemon off;&amp;quot;&lt;/span&gt;
gerard@atlantis:~/workspace/miniserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/miniserver$ cat s6/ssh/run
&lt;span class="c1"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; key in rsa ecdsa ed25519&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="nb"&gt;test&lt;/span&gt; -e /etc/ssh/ssh_host_&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;key&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;_key &lt;span class="o"&gt;||&lt;/span&gt; ssh-keygen -t &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;key&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; -N &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; -f /etc/ssh/ssh_host_&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;key&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;_key -q
&lt;span class="k"&gt;done&lt;/span&gt;

&lt;span class="nb"&gt;exec&lt;/span&gt; /usr/sbin/sshd -D -e
gerard@atlantis:~/workspace/miniserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/miniserver$ cat s6/.s6-svscan/finish
&lt;span class="c1"&gt;#!/bin/sh&lt;/span&gt;
gerard@atlantis:~/workspace/miniserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El único &lt;em&gt;script&lt;/em&gt; que hace algo un poco más complejo es el de SSH, que se encarga de crear las &lt;em&gt;host keys&lt;/em&gt; si no hubiera ninguna, para que cada contenedor genere las suyas propias y no vengan en la imagen.&lt;/p&gt;
&lt;p&gt;En cuanto a la imagen, se necesita que todos los comandos usados funcionen en el contenedor. Esto nos obliga a instalar los paquetes y configuraciones como es habitual. La parte propia de &lt;strong&gt;s6&lt;/strong&gt; se limita a tres cosas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instalar el paquete &lt;strong&gt;s6&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Crear su carpeta de servicios en &lt;code&gt;/etc/s6&lt;/code&gt;, en este caso mediante copia de los scripts&lt;/li&gt;
&lt;li&gt;Indicar que el comando a ejectuar será &lt;code&gt;s6-svscan /etc/s6&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Así quedaría el &lt;code&gt;Dockerfile&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/miniserver$ cat Dockerfile
FROM alpine:3.8

&lt;span class="c1"&gt;# ssh daemon&lt;/span&gt;
RUN apk add --no-cache openssh &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    adduser -D gerard &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gerard:s3cr3t&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; chpasswd

&lt;span class="c1"&gt;# nginx server&lt;/span&gt;
RUN apk add --no-cache nginx &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    ln -s /dev/stdout /var/log/nginx/access.log &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    ln -s /dev/stderr /var/log/nginx/error.log &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    mkdir /run/nginx &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /etc/nginx/conf.d/default.conf &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    install -d -o gerard -g gerard -m &lt;span class="m"&gt;755&lt;/span&gt; /srv/www
COPY nginx.conf /etc/nginx/

&lt;span class="c1"&gt;# s6 supervision tools&lt;/span&gt;
RUN apk add --no-cache s6
COPY s6 /etc/s6
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/bin/s6-svscan&amp;quot;&lt;/span&gt;,&lt;span class="s2"&gt;&amp;quot;/etc/s6&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@atlantis:~/workspace/miniserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Por tener el ejemplo completo, y aunque no tiene nada que ver con &lt;strong&gt;s6&lt;/strong&gt;, incluyo también la configuración de &lt;strong&gt;nginx&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/miniserver$ cat nginx.conf
worker_processes &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

events &lt;span class="o"&gt;{&lt;/span&gt;
        worker_connections &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

http &lt;span class="o"&gt;{&lt;/span&gt;
        include mime.types&lt;span class="p"&gt;;&lt;/span&gt;
        default_type application/octet-stream&lt;span class="p"&gt;;&lt;/span&gt;
        sendfile on&lt;span class="p"&gt;;&lt;/span&gt;
        keepalive_timeout &lt;span class="m"&gt;65&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

        server &lt;span class="o"&gt;{&lt;/span&gt;
                listen &lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
                server_name _&lt;span class="p"&gt;;&lt;/span&gt;
                root /srv/www&lt;span class="p"&gt;;&lt;/span&gt;
                index index.html&lt;span class="p"&gt;;&lt;/span&gt;
                error_page &lt;span class="m"&gt;404&lt;/span&gt; /404.html&lt;span class="p"&gt;;&lt;/span&gt;

                location /404.html &lt;span class="o"&gt;{&lt;/span&gt;
                        internal&lt;span class="p"&gt;;&lt;/span&gt;
                &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@atlantis:~/workspace/miniserver$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo nos quedaría publicar los puertos de una forma inteligente, subir contenido por SFTP o SCP en &lt;code&gt;/srv/www&lt;/code&gt;, y observar el resultado en el navegador.&lt;/p&gt;</content><category term="docker"></category><category term="s6"></category><category term="nginx"></category><category term="ssh"></category></entry><entry><title>Un balanceador dinámico para Docker: traefik</title><link href="https://www.linuxsysadmin.ml/2018/09/un-balanceador-dinamico-para-docker-traefik.html" rel="alternate"></link><published>2018-09-17T10:00:00+02:00</published><updated>2018-09-17T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-09-17:/2018/09/un-balanceador-dinamico-para-docker-traefik.html</id><summary type="html">&lt;p&gt;Cuando escalamos nuestros servicios o añadimos nuevos en &lt;strong&gt;Docker&lt;/strong&gt;, suele ser un problema la configuración del balanceador. Se necesita modificar su configuración y reiniciarlo para que la nueva configuración aplique. Con el tiempo han aparecido nuevas soluciones para simplificar estos casos, con configuraciones dinámicas. Una de estas soluciones es &lt;strong&gt;Traefik …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Cuando escalamos nuestros servicios o añadimos nuevos en &lt;strong&gt;Docker&lt;/strong&gt;, suele ser un problema la configuración del balanceador. Se necesita modificar su configuración y reiniciarlo para que la nueva configuración aplique. Con el tiempo han aparecido nuevas soluciones para simplificar estos casos, con configuraciones dinámicas. Una de estas soluciones es &lt;strong&gt;Traefik&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Traefik&lt;/strong&gt; es un proxy reverso y balanceador moderno que facilita el despliegue de microservicios. Se integra con algunos componentes de nuestra infraestructura (&lt;strong&gt;Docker&lt;/strong&gt;, &lt;strong&gt;Docker Swarm&lt;/strong&gt;, &lt;strong&gt;Kubernetes&lt;/strong&gt;, &lt;strong&gt;Consul&lt;/strong&gt;, &lt;strong&gt;Amazon ECS&lt;/strong&gt;, ...) y se configura automáticamente leyendo sus metadatos. Suele bastar con apuntar &lt;strong&gt;Traefik&lt;/strong&gt; al orquestador que usemos.&lt;/p&gt;
&lt;h2&gt;Un ejemplo con Docker&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Traefik&lt;/strong&gt; es un binario único hecho en lenguaje &lt;strong&gt;Go&lt;/strong&gt; y como viene siendo habitual, lo podemos instalar simplemente "tirándolo por ahí". También se nos ofrece como una imagen oficial de &lt;strong&gt;Docker&lt;/strong&gt;. Vamos a usar esta última por simplicidad.&lt;/p&gt;
&lt;p&gt;Lo primero que tenemos que tener en cuenta es que &lt;strong&gt;Traefik&lt;/strong&gt; va a pasar las peticiones a otros servicios, y para ello tiene que poder alcanzarlos. En el caso de &lt;strong&gt;Docker&lt;/strong&gt; sin cluster, los contenedores tienen conectividad si estan en la misma red. Eso se puede conseguir de dos formas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ponemos todos los servicios en el mismo &lt;em&gt;docker-compose.yml&lt;/em&gt; para que vayan todos a la misma red&lt;/li&gt;
&lt;li&gt;Definimos una red global para que varios &lt;em&gt;docker-compose.yml&lt;/em&gt; se encarguen solamente de sus servicios relevantes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: Optamos por la segunda para separar los servicios por proyectos; así podemos reiniciarlos fácilmente sin afectar a sus vecinos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace$ docker network create global
ebd9af59c9d2c8e2ce61db17885b777a343a6b354465f2a5b4cddba5bf92b9b7
gerard@atlantis:~/workspace$
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;El balanceador&lt;/h3&gt;
&lt;p&gt;Ahora necesitamos levantar el contenedor que va a ejecutar &lt;strong&gt;Traefik&lt;/strong&gt;, y por comodidad, lo haremos con &lt;strong&gt;docker-compose&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace$ cat traefik/docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  traefik:
    image: traefik
    command: --api --docker --docker.exposedbydefault&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt;
    ports:
      - &lt;span class="s2"&gt;&amp;quot;80:80&amp;quot;&lt;/span&gt;
      - &lt;span class="s2"&gt;&amp;quot;8080:8080&amp;quot;&lt;/span&gt;
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - global
networks:
  global:
     external: &lt;span class="nb"&gt;true&lt;/span&gt;
gerard@atlantis:~/workspace$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Básicamente instruimos a &lt;strong&gt;Traefik&lt;/strong&gt; para que use &lt;strong&gt;Docker&lt;/strong&gt;, le mapeamos el &lt;em&gt;socket&lt;/em&gt; para que pueda consultar los metadatos de los servicios. Como detalle adicional levantamos el &lt;em&gt;dashboard&lt;/em&gt; con el &lt;em&gt;flag&lt;/em&gt; &lt;code&gt;--api&lt;/code&gt;, lo enchufamos a la red &lt;code&gt;global&lt;/code&gt; que hemos creado antes y -por preferencia personal- no exponemos ningún servicio por defecto.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace$ &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; traefik/ &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; docker-compose up -d&lt;span class="o"&gt;)&lt;/span&gt;
Creating traefik_traefik_1 ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@atlantis:~/workspace$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto ya podemos acceder al &lt;em&gt;dashboard&lt;/em&gt; en &lt;a href="http://localhost:8080/"&gt;http://localhost:8080/&lt;/a&gt;, aunque no tenemos ningún dominio registrado, con lo que obtendremos errores 404 en el puerto 80.&lt;/p&gt;
&lt;h3&gt;Un servicio de ejemplo&lt;/h3&gt;
&lt;p&gt;Para la demostración, vamos a utilizar una imagen que vuelca el &lt;em&gt;hostname&lt;/em&gt; el contenedor; eso nos sirve para verificar el balanceo. Como no quiero reinventar la rueda, y para agilizar, vamos a utilizar la imagen &lt;code&gt;emilevauge/whoami&lt;/code&gt; que podemos sacar de &lt;em&gt;DockerHub&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace$ cat whoami/docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  whoami:
    image: emilevauge/whoami
    labels:
      - &lt;span class="s2"&gt;&amp;quot;traefik.frontend.rule=Host:whoami.docker.localhost&amp;quot;&lt;/span&gt;
      - &lt;span class="s2"&gt;&amp;quot;traefik.enable=true&amp;quot;&lt;/span&gt;
    networks:
      - global
networks:
  global:
    external: &lt;span class="nb"&gt;true&lt;/span&gt;
gerard@atlantis:~/workspace$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Un punto interesante de mencionar es que &lt;strong&gt;Traefik&lt;/strong&gt; viene con una configuración estándar, pero se puede modificar algunas cosas que afectan a los contenedores mediante &lt;em&gt;labels&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;La más evidente es &lt;code&gt;traefik.enable&lt;/code&gt;, que sobreescribe el comportamiente de no exponer por defecto los servicios; con ello evitamos que se expongan servicios que no deseamos hacer públicos (bases de datos, depliegues bue-green, otros servicios, ...).&lt;/p&gt;
&lt;p&gt;Otra &lt;em&gt;label&lt;/em&gt; interesante es &lt;code&gt;traefik.frontend.rule&lt;/code&gt; que básicamente indica que este contenedor es uno de los miembros del &lt;em&gt;pool&lt;/em&gt; de balanceo cuando se pida el &lt;em&gt;host&lt;/em&gt; indicado. Una &lt;em&gt;label&lt;/em&gt; que podemos necesitar es &lt;code&gt;traefik.port&lt;/code&gt;, que indica contra que puerto del contenedor hay que lanzar las peticiones; por defecto se pasan al puerto 80 (que es donde escucha la imagen elegida).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: Para una lista completa, podemos ir a &lt;a href="https://docs.traefik.io/configuration/backends/docker/#on-containers"&gt;la documentación&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Levantamos el servicio con una sola instancia de momento:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace$ &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; whoami/ &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; docker-compose up -d&lt;span class="o"&gt;)&lt;/span&gt;
Creating whoami_whoami_1 ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@atlantis:~/workspace$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Comprobamos que funciona como esperamos:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace$ curl http://localhost:80/
&lt;span class="m"&gt;404&lt;/span&gt; page not found
gerard@atlantis:~/workspace$ curl -H &lt;span class="s2"&gt;&amp;quot;Host: whoami.docker.localhost&amp;quot;&lt;/span&gt; http://localhost:80/
Hostname: 831400b7faf2
IP: &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1
IP: &lt;span class="m"&gt;172&lt;/span&gt;.25.0.3
GET / HTTP/1.1
Host: whoami.docker.localhost
User-Agent: curl/7.52.1
Accept: */*
Accept-Encoding: gzip
X-Forwarded-For: &lt;span class="m"&gt;172&lt;/span&gt;.25.0.1
X-Forwarded-Host: whoami.docker.localhost
X-Forwarded-Port: &lt;span class="m"&gt;80&lt;/span&gt;
X-Forwarded-Proto: http
X-Forwarded-Server: dd0bdbb8d6fb
X-Real-Ip: &lt;span class="m"&gt;172&lt;/span&gt;.25.0.1

gerard@atlantis:~/workspace$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y vemos como, sin tocar el balanceador, ha aparecido un nuevo &lt;em&gt;virtualhost&lt;/em&gt; que pasa las peticiones a nuestro contenedor. Ahora vamos a escalar el servicio:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace$ &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; whoami/ &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; docker-compose up -d --scale &lt;span class="nv"&gt;whoami&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
Starting whoami_whoami_1 ... &lt;span class="k"&gt;done&lt;/span&gt;
Creating whoami_whoami_2 ... &lt;span class="k"&gt;done&lt;/span&gt;
Creating whoami_whoami_3 ... &lt;span class="k"&gt;done&lt;/span&gt;
Creating whoami_whoami_4 ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@atlantis:~/workspace$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y así sin tocar nada más, &lt;strong&gt;Traefik&lt;/strong&gt; se ha dado cuenta del cambio en el número de contenedores y ha añadido los 3 nuevos en el &lt;em&gt;pool&lt;/em&gt; de balanceo de &lt;code&gt;whoami.docker.localhost&lt;/code&gt;, como indican sus &lt;em&gt;labels&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace$ &lt;span class="k"&gt;for&lt;/span&gt; i in &lt;span class="k"&gt;$(&lt;/span&gt;seq &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt; curl -sH &lt;span class="s2"&gt;&amp;quot;Host: whoami.docker.localhost&amp;quot;&lt;/span&gt; http://localhost:80/ &lt;span class="p"&gt;|&lt;/span&gt; grep Hostname&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;done&lt;/span&gt;
Hostname: 831400b7faf2
Hostname: 7de8d5739178
Hostname: 01aa52cb5c66
Hostname: 4f64cac4a4d2
Hostname: 831400b7faf2
Hostname: 7de8d5739178
Hostname: 01aa52cb5c66
Hostname: 4f64cac4a4d2
gerard@atlantis:~/workspace$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y estas son las &lt;em&gt;labels&lt;/em&gt; de cada contenedor:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace$ &lt;span class="k"&gt;for&lt;/span&gt; c in whoami_whoami_&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;,2,3,4&lt;span class="o"&gt;}&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$c&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; docker inspect &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;c&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep traefik&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;done&lt;/span&gt;
whoami_whoami_1
                &lt;span class="s2"&gt;&amp;quot;traefik.enable&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;,
                &lt;span class="s2"&gt;&amp;quot;traefik.frontend.rule&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;Host:whoami.docker.localhost&amp;quot;&lt;/span&gt;
whoami_whoami_2
                &lt;span class="s2"&gt;&amp;quot;traefik.enable&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;,
                &lt;span class="s2"&gt;&amp;quot;traefik.frontend.rule&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;Host:whoami.docker.localhost&amp;quot;&lt;/span&gt;
whoami_whoami_3
                &lt;span class="s2"&gt;&amp;quot;traefik.enable&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;,
                &lt;span class="s2"&gt;&amp;quot;traefik.frontend.rule&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;Host:whoami.docker.localhost&amp;quot;&lt;/span&gt;
whoami_whoami_4
                &lt;span class="s2"&gt;&amp;quot;traefik.enable&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;,
                &lt;span class="s2"&gt;&amp;quot;traefik.frontend.rule&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;Host:whoami.docker.localhost&amp;quot;&lt;/span&gt;
gerard@atlantis:~/workspace$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: No es necesario que tengáis un solo tipo de contenedores con la &lt;em&gt;label&lt;/em&gt; del &lt;em&gt;host&lt;/em&gt;. Se puede hacer un balanceo de contenedores distintos, con distintos puertos y funciones. Esto es útil en el caso de un cambio de versión sin corte; basta con añadir la nueva versión con otro &lt;em&gt;docker-compose.yml&lt;/em&gt; y retirar el servicio viejo poco después.&lt;/p&gt;
&lt;h2&gt;Conclusión&lt;/h2&gt;
&lt;p&gt;Con la facilidad que supone crear un &lt;em&gt;docker-compose.yml&lt;/em&gt; para añadir un nuevo &lt;em&gt;virtualhost&lt;/em&gt; en &lt;strong&gt;Traefik&lt;/strong&gt;, podemos desplegar servicios y microservicios sin mucha complicación, y sin estar pendientes del balanceador. Eso reduce la necesidad de un administrador dedicado, pero hace que las cosas se puedan descontrolar fácilmente.&lt;/p&gt;
&lt;p&gt;Cuando os déis cuenta que el servidor único con &lt;strong&gt;Docker&lt;/strong&gt; se os queda corto, váis a necesitar un cluster más adecuado, como &lt;strong&gt;Docker Swarm&lt;/strong&gt; o &lt;strong&gt;Kubernetes&lt;/strong&gt;. La integración de &lt;strong&gt;Traefik&lt;/strong&gt; con ambos es muy simple, y no váis a necesitar mucha más investigación.&lt;/p&gt;
&lt;p&gt;Cabe mencionar que &lt;strong&gt;Traefik&lt;/strong&gt; se integra también con varis servidores de SSL (por ejemplo &lt;strong&gt;LetsEncrypt&lt;/strong&gt;) y nos puede gestionar fácilmente la terminación SSL y las redirecciones de un protocolo a otro. Tampoco hemos hablado del magnífico &lt;em&gt;dashboard&lt;/em&gt; y de sus métricas; creo que os encantará verlo a vosotros mismos.&lt;/p&gt;</content><category term="traefik"></category><category term="docker"></category><category term="balanceador"></category></entry><entry><title>Una unidad de red remota con SSHFS</title><link href="https://www.linuxsysadmin.ml/2018/09/una-unidad-de-red-remota-con-sshfs.html" rel="alternate"></link><published>2018-09-10T10:00:00+02:00</published><updated>2018-09-10T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-09-10:/2018/09/una-unidad-de-red-remota-con-sshfs.html</id><summary type="html">&lt;p&gt;Es muy cómodo dejar un fichero en una carpeta local y saber que ese fichero está a salvo, en la nube; seguramente los que utilizáis un servicio de sincronización en la nube váis a estar de acuerdo. La empresas no suelen permitirlo, pero podéis utilizar un servicio local por &lt;strong&gt;SSH …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Es muy cómodo dejar un fichero en una carpeta local y saber que ese fichero está a salvo, en la nube; seguramente los que utilizáis un servicio de sincronización en la nube váis a estar de acuerdo. La empresas no suelen permitirlo, pero podéis utilizar un servicio local por &lt;strong&gt;SSH&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;El ingenio se llama &lt;strong&gt;SSHFS&lt;/strong&gt; y lanza las operaciones de lectura y escritura contra una máquina remota que solamente debe ofrecer &lt;strong&gt;SSH&lt;/strong&gt;, de forma que para nosotros parece que se trate de un sistema de ficheros local.&lt;/p&gt;
&lt;h2&gt;El servidor&lt;/h2&gt;
&lt;p&gt;Para utilizar &lt;strong&gt;SSHFS&lt;/strong&gt; solo necesitamos un servidor que ofrezca &lt;strong&gt;SSH&lt;/strong&gt;, aunque también funciona por &lt;strong&gt;SFTP&lt;/strong&gt;. Como comodidad adicional, vamos a utilizar &lt;a href="https://www.linuxsysadmin.ml/2016/05/autenticacion-ssh-por-claves.html"&gt;autenticacíon por claves&lt;/a&gt;. Como conisderación de seguridad, vamos a enjaular al usuario que guarde los datos y lo vamos a lmitar a &lt;strong&gt;SFTP&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: Se asume que el servidor ya tiene el servidor &lt;strong&gt;SSH&lt;/strong&gt; instalado; de no ser así, basta con hacer un &lt;code&gt;sudo apt install openssh-server&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Lo primero es crear un usuario para que &lt;strong&gt;SSHFS&lt;/strong&gt; pueda entrar al servidor por &lt;strong&gt;SSH&lt;/strong&gt;/&lt;strong&gt;SFTP&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@server:~$ sudo adduser nas
...
gerard@server:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Le asignamos la parte pública de la clave &lt;strong&gt;SSH&lt;/strong&gt; en el fichero &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; para evitar que tengamos que introducir la contraseña cada vez que montemos la carpeta remota en el cliente, y para evitar ataques de fuerza bruta.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;nas@server:~$ cat .ssh/authorized_keys
ssh-rsa ...
nas@server:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En este punto, ya podríamos entrar, pero como hemos decidido enjaular, tenemos que ajustar algunos permisos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;La carpeta en donde lo enjaulemos (su &lt;em&gt;home&lt;/em&gt;) debe pertenecer a &lt;strong&gt;root&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Esto nos obliga a tener una carpeta de trabajo en donde nuestro usuario pueda escribir&lt;/li&gt;
&lt;li&gt;Vamos a cambiar el propietario de la carpeta &lt;code&gt;.ssh&lt;/code&gt; para evitar que este usuario &lt;a href="https://www.linuxsysadmin.ml/2018/08/cambiando-la-posicion-del-fichero-authorized-keys.html"&gt;la pueda eliminar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con todo esto, la carpeta nos va a quedar así:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;nas@server:/home$ tree -I gerard -augp
.
└── &lt;span class="o"&gt;[&lt;/span&gt;drwxr-xr-x root     root    &lt;span class="o"&gt;]&lt;/span&gt;  nas
    ├── &lt;span class="o"&gt;[&lt;/span&gt;drwxr-xr-x nas      nas     &lt;span class="o"&gt;]&lt;/span&gt;  archives
    └── &lt;span class="o"&gt;[&lt;/span&gt;drwxr-xr-x root     root    &lt;span class="o"&gt;]&lt;/span&gt;  .ssh
        └── &lt;span class="o"&gt;[&lt;/span&gt;-rw-r--r-- root     root    &lt;span class="o"&gt;]&lt;/span&gt;  authorized_keys

&lt;span class="m"&gt;3&lt;/span&gt; directories, &lt;span class="m"&gt;1&lt;/span&gt; file
nas@server:/home$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Estamos listos para enjaular; modificamos el fichero &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt; para enjaular a este usuario:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@server:~$ cat /etc/ssh/sshd_config
...
Match User nas
        ChrootDirectory /home/%u
        ForceCommand internal-sftp
gerard@server:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo nos queda recargar el servicio &lt;strong&gt;SSH&lt;/strong&gt; para que lea la nueva configuración.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@server:~$ sudo service ssh restart
gerard@server:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;El cliente&lt;/h2&gt;
&lt;p&gt;Para poder montar el sistema de ficheros remoto, se necesita el paquete &lt;strong&gt;sshfs&lt;/strong&gt;, así que lo instalamos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@client:~$ sudo apt-get install sshfs
...
gerard@client:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Necesitamos una carpeta en donde montar el sistema de ficheros remoto, que llamaremos &lt;code&gt;NAS&lt;/code&gt;; como no la tengo, la creo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@client:~$ mkdir NAS
gerard@client:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para montar la carpeta remota solo necesitamos un comando; asumimos que la parte privada de la clave &lt;strong&gt;SSH&lt;/strong&gt; está en &lt;code&gt;.ssh/id_rsa&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@client:~$ sshfs nas@server:/archives NAS -o &lt;span class="nv"&gt;idmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;user
gerard@client:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A partir de ahora podemos trabajar en la carpeta como si de una carpeta local se tratara, por ejemplo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@client:~$ &lt;span class="nb"&gt;cd&lt;/span&gt; NAS/
gerard@client:~/NAS$ &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;123&lt;/span&gt; &amp;gt; file
gerard@client:~/NAS$ touch emptyfile
gerard@client:~/NAS$ ls -lh
total &lt;span class="m"&gt;4&lt;/span&gt;,0K
-rw-r--r-- &lt;span class="m"&gt;1&lt;/span&gt; nas nas &lt;span class="m"&gt;0&lt;/span&gt; ago  &lt;span class="m"&gt;8&lt;/span&gt; &lt;span class="m"&gt;12&lt;/span&gt;:25 emptyfile
-rw-r--r-- &lt;span class="m"&gt;1&lt;/span&gt; nas nas &lt;span class="m"&gt;4&lt;/span&gt; ago  &lt;span class="m"&gt;8&lt;/span&gt; &lt;span class="m"&gt;12&lt;/span&gt;:25 file
gerard@client:~/NAS$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y como es de esperar, estos ficheros han acabado en el servidor:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@server:~$ tree -augp /home/nas/
/home/nas/
├── &lt;span class="o"&gt;[&lt;/span&gt;drwxr-xr-x nas      nas     &lt;span class="o"&gt;]&lt;/span&gt;  archives
│   ├── &lt;span class="o"&gt;[&lt;/span&gt;-rw-r--r-- nas      nas     &lt;span class="o"&gt;]&lt;/span&gt;  emptyfile
│   └── &lt;span class="o"&gt;[&lt;/span&gt;-rw-r--r-- nas      nas     &lt;span class="o"&gt;]&lt;/span&gt;  file
└── &lt;span class="o"&gt;[&lt;/span&gt;drwxr-xr-x root     root    &lt;span class="o"&gt;]&lt;/span&gt;  .ssh
    └── &lt;span class="o"&gt;[&lt;/span&gt;-rw-r--r-- root     root    &lt;span class="o"&gt;]&lt;/span&gt;  authorized_keys

&lt;span class="m"&gt;2&lt;/span&gt; directories, &lt;span class="m"&gt;3&lt;/span&gt; files
gerard@server:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En el momento que nos cansemos de utilizar la unidad de red, solo tenemos que desmontarla, y la seguiremos viendo con lo que tenía antes del montaje.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@client:~$ fusermount -u NAS
gerard@client:~$ ls -lh NAS/
total &lt;span class="m"&gt;0&lt;/span&gt;
gerard@client:~$
&lt;/pre&gt;&lt;/div&gt;</content><category term="unidad"></category><category term="remota"></category><category term="ssh"></category><category term="sshfs"></category></entry><entry><title>Cambiando la posición del fichero authorized_keys</title><link href="https://www.linuxsysadmin.ml/2018/08/cambiando-la-posicion-del-fichero-authorized-keys.html" rel="alternate"></link><published>2018-08-13T09:00:00+02:00</published><updated>2018-08-13T09:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-08-13:/2018/08/cambiando-la-posicion-del-fichero-authorized-keys.html</id><summary type="html">&lt;p&gt;Un requerimiento de seguridad estándar en mi trabajo, es que los servidores SFTP no permitan la autenticación con &lt;em&gt;passwords&lt;/em&gt; normales, y estamos obligados a usar autenticación por claves. El otro día tuvimos una queja de un usuario que no podía entrar porque había eliminado su carpeta &lt;code&gt;.ssh&lt;/code&gt; de forma consciente …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Un requerimiento de seguridad estándar en mi trabajo, es que los servidores SFTP no permitan la autenticación con &lt;em&gt;passwords&lt;/em&gt; normales, y estamos obligados a usar autenticación por claves. El otro día tuvimos una queja de un usuario que no podía entrar porque había eliminado su carpeta &lt;code&gt;.ssh&lt;/code&gt; de forma consciente.&lt;/p&gt;
&lt;p&gt;En casos como este no nos queda más remedio que reirnos un rato y restablecer su fichero &lt;code&gt;authorized_keys&lt;/code&gt; desde un &lt;em&gt;backup&lt;/em&gt;. Sin embargo, hay varias preguntas que se nos deberían plantear en estos casos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;¿Por qué este usuario tenía permisos para escribir la carpeta &lt;code&gt;.ssh&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;¿Por qué estaba su fichero &lt;code&gt;authorized_keys&lt;/code&gt; en su &lt;em&gt;home&lt;/em&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Siempre debes pensar que un usuario puede meter la pata, y como son muchos, siempre tenemos muchos manazas con problemas similares. No hay nada que se pueda hacer en este sentido, pero debemos plantearnos si podemos evitar que el problema ocurrido se pueda repetir.&lt;/p&gt;
&lt;p&gt;En este caso concreto, no tardamos mucho en evitar que se repitiera, ya que disponiamos de dos métodos sencillos para evitarlo:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Quitarle los permisos de escritura en la carpeta &lt;code&gt;.ssh&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Quitar la carpeta &lt;code&gt;.ssh&lt;/code&gt; de su línea de tiro, concretamente, fuera de su jaula.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Cabe decir que nos decantamos por la 2, y aquí explico como se hace.&lt;/p&gt;
&lt;h2&gt;Mover el fichero authorized_keys de sitio&lt;/h2&gt;
&lt;p&gt;Empezamos con un &lt;em&gt;setup&lt;/em&gt; estándar en donde &lt;strong&gt;bob&lt;/strong&gt; puede entrar usando su clave ssh:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~$ sftp -i id_bob bob@sftpserver
Connected to sftpserver.
sftp&amp;gt; ls -la
drwxr-xr-x    &lt;span class="m"&gt;4&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;        &lt;span class="m"&gt;0&lt;/span&gt;            &lt;span class="m"&gt;4096&lt;/span&gt; Jul &lt;span class="m"&gt;18&lt;/span&gt; &lt;span class="m"&gt;15&lt;/span&gt;:42 .
drwxr-xr-x    &lt;span class="m"&gt;4&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;        &lt;span class="m"&gt;0&lt;/span&gt;            &lt;span class="m"&gt;4096&lt;/span&gt; Jul &lt;span class="m"&gt;18&lt;/span&gt; &lt;span class="m"&gt;15&lt;/span&gt;:42 ..
drwxr-xr-x    &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;1001&lt;/span&gt;     &lt;span class="m"&gt;1001&lt;/span&gt;         &lt;span class="m"&gt;4096&lt;/span&gt; Jul &lt;span class="m"&gt;18&lt;/span&gt; &lt;span class="m"&gt;15&lt;/span&gt;:42 .ssh
drwxr-xr-x    &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;1001&lt;/span&gt;     &lt;span class="m"&gt;1001&lt;/span&gt;         &lt;span class="m"&gt;4096&lt;/span&gt; Jul &lt;span class="m"&gt;18&lt;/span&gt; &lt;span class="m"&gt;15&lt;/span&gt;:38 archives
sftp&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Con los permisos mostrados, no le sería difícil eliminar su clave, que es la única manera permitida de entrar en su cuenta; así pues, vamos a mover la posición de dicho fichero para que no lo vea el usuario en su &lt;em&gt;home&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;El truco consiste en modificar la directiva &lt;code&gt;AuthorizedKeysFile&lt;/code&gt; en el fichero &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt;. Mas información en &lt;a href="https://linux.die.net/man/5/sshd_config"&gt;las páginas &lt;em&gt;man&lt;/em&gt;&lt;/a&gt;. Especificamente nos interesa el &lt;em&gt;token&lt;/em&gt; &lt;code&gt;%u&lt;/code&gt;, que es el nombre del usuario que intenta logarse, en este caso, &lt;strong&gt;bob&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Como decisión de diseño, vamos a poner las claves en &lt;code&gt;/srv/sshkeys/&amp;lt;usuario&amp;gt;&lt;/code&gt;, aunque esto es arbitrario y susceptible a cambio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sftpserver:~$ cat /etc/ssh/sshd_config
...
AuthorizedKeysFile /srv/sshkeys/%u
...
Match Group sftponly
  ChrootDirectory %h
  ForceCommand internal-sftp
  PasswordAuthentication no
gerard@sftpserver:~$ sudo service ssh restart
gerard@sftpserver:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Nos cargamos la carpeta &lt;code&gt;.ssh&lt;/code&gt; del usuario &lt;strong&gt;bob&lt;/strong&gt; y la ponemos en &lt;code&gt;/srv/sshkeys/bob&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sftpserver:~$ ls -la /home/bob/
total &lt;span class="m"&gt;12&lt;/span&gt;
drwxr-xr-x &lt;span class="m"&gt;3&lt;/span&gt; root root &lt;span class="m"&gt;4096&lt;/span&gt; jul &lt;span class="m"&gt;19&lt;/span&gt; &lt;span class="m"&gt;08&lt;/span&gt;:46 .
drwxr-xr-x &lt;span class="m"&gt;4&lt;/span&gt; root root &lt;span class="m"&gt;4096&lt;/span&gt; jul &lt;span class="m"&gt;18&lt;/span&gt; &lt;span class="m"&gt;17&lt;/span&gt;:35 ..
drwxr-xr-x &lt;span class="m"&gt;2&lt;/span&gt; bob  bob  &lt;span class="m"&gt;4096&lt;/span&gt; jul &lt;span class="m"&gt;18&lt;/span&gt; &lt;span class="m"&gt;17&lt;/span&gt;:38 archives
gerard@sftpserver:~$ cat /srv/sshkeys/bob
ssh-rsa ...
gerard@sftpserver:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y sin sorpresas, la mecánica no cambia nada, solo que en este caso, el usuario no tiene el fichero &lt;code&gt;authorized_keys&lt;/code&gt; a tiro, y por lo tanto, no puede liarla.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~$ sftp -i id_bob bob@sftpserver
Connected to sftpserver.
sftp&amp;gt; ls -la
drwxr-xr-x    &lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;        &lt;span class="m"&gt;0&lt;/span&gt;            &lt;span class="m"&gt;4096&lt;/span&gt; Jul &lt;span class="m"&gt;19&lt;/span&gt; &lt;span class="m"&gt;06&lt;/span&gt;:46 .
drwxr-xr-x    &lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;        &lt;span class="m"&gt;0&lt;/span&gt;            &lt;span class="m"&gt;4096&lt;/span&gt; Jul &lt;span class="m"&gt;19&lt;/span&gt; &lt;span class="m"&gt;06&lt;/span&gt;:46 ..
drwxr-xr-x    &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="m"&gt;1001&lt;/span&gt;     &lt;span class="m"&gt;1001&lt;/span&gt;         &lt;span class="m"&gt;4096&lt;/span&gt; Jul &lt;span class="m"&gt;18&lt;/span&gt; &lt;span class="m"&gt;15&lt;/span&gt;:38 archives
sftp&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto nos ahorramos problemas futuros del tipo "he borrado la carpeta &lt;code&gt;.ssh&lt;/code&gt;".&lt;/p&gt;</content><category term="ssh"></category><category term="sftp"></category><category term="authorized_keys"></category><category term="jaula"></category></entry><entry><title>Un servidor pypi local con Docker</title><link href="https://www.linuxsysadmin.ml/2018/07/un-servidor-pypi-local-con-docker.html" rel="alternate"></link><published>2018-07-09T09:00:00+02:00</published><updated>2018-07-09T09:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-07-09:/2018/07/un-servidor-pypi-local-con-docker.html</id><summary type="html">&lt;p&gt;Estaba yo el otro día investigando una nueva librería de &lt;strong&gt;python&lt;/strong&gt;, pero necesitaba de otra librería que se compilaba. Harto de perder el tiempo compilando cada vez esta librería, recuperé un antiguo artículo que me permitía distribuir el archivo &lt;em&gt;wheel&lt;/em&gt; ya compilado tantas veces yo quisiera; como no, usando &lt;strong&gt;docker …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Estaba yo el otro día investigando una nueva librería de &lt;strong&gt;python&lt;/strong&gt;, pero necesitaba de otra librería que se compilaba. Harto de perder el tiempo compilando cada vez esta librería, recuperé un antiguo artículo que me permitía distribuir el archivo &lt;em&gt;wheel&lt;/em&gt; ya compilado tantas veces yo quisiera; como no, usando &lt;strong&gt;docker&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;El artículo anterior es &lt;a href="https://www.linuxsysadmin.ml/2016/09/un-servidor-pypi-local.html"&gt;este&lt;/a&gt;, y su único objetivo era montar un servidor de &lt;strong&gt;pypi&lt;/strong&gt; local que nos permite alojar nuestros ficheros &lt;em&gt;wheel&lt;/em&gt;, aunque funciona genial para evitar múltiples compilaciones y como &lt;em&gt;caché&lt;/em&gt; de paquetes.&lt;/p&gt;
&lt;h2&gt;El servidor pypi&lt;/h2&gt;
&lt;p&gt;Para facilitar el montaje y la distribución de este servidor, se ha decidido hacer una imagen de &lt;strong&gt;docker&lt;/strong&gt; para encapsular lo necesario; de paso, el fichero &lt;em&gt;Dockerfile&lt;/em&gt; es una magnífica receta para evitar un montón de pasos manuales que salían en el artículo citado.&lt;/p&gt;
&lt;p&gt;El fichero &lt;em&gt;Dockerfile&lt;/em&gt; no puede ser más explícito; puesto que necesitamos instalar &lt;strong&gt;pypiserver&lt;/strong&gt; mediante &lt;strong&gt;pip&lt;/strong&gt;, los instalamos. También se define el comando que va a levantar el servidor y se crea la carpeta en donde deben residir nuestros paquetes. No veo muy útil crear un &lt;em&gt;virtualenv&lt;/em&gt;; un contenedor &lt;strong&gt;docker&lt;/strong&gt; ya es un entorno aislado en sí mismo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/pypiserver$ cat Dockerfile 
FROM alpine:3.7
RUN apk add --no-cache py2-pip tini &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    pip install pypiserver &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    mkdir /srv/packages
ENTRYPOINT &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/sbin/tini&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/bin/pypi-server&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;/srv/packages&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@sirius:~/workspace/pypiserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: El binario &lt;code&gt;pypi-server&lt;/code&gt; no cumple como un &lt;em&gt;init&lt;/em&gt; correcto, y el contenedor no acaba hasta que &lt;strong&gt;docker&lt;/strong&gt; lo mata. Para evitar ese problema, vamos a utilizar &lt;strong&gt;tini&lt;/strong&gt;, tal y como explicamos en &lt;a href="https://www.linuxsysadmin.ml/2017/09/un-proceso-inicial-para-docker-tini-y-dumb-init.html"&gt;otro artículo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Construimos la imagen con los comando habituales y le damos un &lt;em&gt;tag&lt;/em&gt; para su fácil uso cuando lo queramos levantar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/pypiserver$ docker build -t pypiserver .
...
Successfully tagged pypiserver:latest
gerard@sirius:~/workspace/pypiserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y solo queda revisar que la imagen existe y que su tamaño tiene sentido:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/pypiserver$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
pypiserver          latest              ad483ac352d7        &lt;span class="m"&gt;56&lt;/span&gt; seconds ago      &lt;span class="m"&gt;52&lt;/span&gt;.8MB
alpine              &lt;span class="m"&gt;3&lt;/span&gt;.7                 3fd9065eaf02        &lt;span class="m"&gt;5&lt;/span&gt; months ago        &lt;span class="m"&gt;4&lt;/span&gt;.15MB
gerard@sirius:~/workspace/pypiserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para el &lt;em&gt;runtime&lt;/em&gt;, vamos a utilizar &lt;em&gt;docker-compose&lt;/em&gt; para reducir la longitud del comando de levantamiento, dejándolo todo explícitamente declarado en el fichero &lt;em&gt;docker-compose.yml&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;La otra decisión de diseño es servir los &lt;em&gt;wheels&lt;/em&gt; desde una carpeta local, en donde podemos ponerlos y quitarlos con gran facilidad. Solo necesitamos declarar la carpeta para los paquetes, que voy a poner en la misma carpeta del &lt;em&gt;docker-compose.yml&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/pypiserver$ tree
.
├── packages
├── docker-compose.yml
└── Dockerfile

&lt;span class="m"&gt;1&lt;/span&gt; directory, &lt;span class="m"&gt;2&lt;/span&gt; files
gerard@sirius:~/workspace/pypiserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En el &lt;em&gt;docker-compose.yml&lt;/em&gt; nos limitamos a montar la carpeta local como volumen y publicar el puerto del contenedor en nuestro servidor; esto hará el contenedor transparente a los ojos del resto de servidores.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/pypiserver$ cat docker-compose.yml 
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  pypiserver:
    image: pypiserver
    container_name: pypiserver
    hostname: pypiserver
    volumes:
      - ./packages:/srv/packages:ro
    ports:
      - &lt;span class="s2"&gt;&amp;quot;8080:8080&amp;quot;&lt;/span&gt;
    restart: always
gerard@sirius:~/workspace/pypiserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ya no falta nada para levantarlo todo y lo hacemos sin más preámbulo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/pypiserver$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;pypiserver_default&amp;quot;&lt;/span&gt; with the default driver
Creating pypiserver
gerard@sirius:~/workspace/pypiserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto tenemos el servidor funcional.&lt;/p&gt;
&lt;h2&gt;Rellenando nuestro servidor de paquetes&lt;/h2&gt;
&lt;p&gt;El único requisito para poder servir paquetes es dejarlos en la carpeta que a ese fin hemos destinado. No es importante como llegan los paquetes ahí; podemos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Descargarlos de &lt;a href="https://pypi.org/"&gt;https://pypi.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pescarlos de nuestra caché local en &lt;code&gt;~/.cache/pip/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Obtenerlos mediante &lt;code&gt;pip wheel&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;En este caso vamos a utilizar el tercer método. Como no tengo &lt;strong&gt;pip&lt;/strong&gt; instalado en mi servidor, y para demostrar que solo necesito los fichero &lt;em&gt;wheel&lt;/em&gt;, voy a utilizar el &lt;strong&gt;pip&lt;/strong&gt; de un &lt;em&gt;virtualenv&lt;/em&gt; temporal, que destruiré al acabar. Otra opción sería crear un contenedor que los dejara en esa carpeta, que también montaría como un volumen.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/pypiserver$ virtualenv env
Running virtualenv with interpreter /usr/bin/python2
New python executable in env/bin/python2
Also creating executable in env/bin/python
Installing setuptools, pip...done.
gerard@sirius:~/workspace/pypiserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ni siquiera hace falta activar el &lt;em&gt;virtualenv&lt;/em&gt;. Lo único que hace el &lt;em&gt;script&lt;/em&gt; de activación es poner la carpeta &lt;em&gt;bin/&lt;/em&gt; en el &lt;em&gt;PATH&lt;/em&gt;, pero para lanzar el comando una vez no lo necesito...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/pypiserver$ ./env/bin/pip wheel -w packages/ falcon mongoengine
...
gerard@sirius:~/workspace/pypiserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El comando &lt;code&gt;pip wheel&lt;/code&gt; solo descarga los paquetes y los convierte en &lt;em&gt;wheels&lt;/em&gt;, y los deja en la carpeta indicada, que es desde donde los servimos. Como ya tenemos lo que queríamos (los &lt;em&gt;wheels&lt;/em&gt;), podemos eliminar el &lt;em&gt;virtualenv&lt;/em&gt;, que nos deja la carpeta limpia de cosas innecesarias.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/pypiserver$ rm -R env/
gerard@sirius:~/workspace/pypiserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos ver que solo hemos conservado los ficheros &lt;em&gt;wheel&lt;/em&gt; solicitados y sus dependencias; no hay más que lo estrictamente necesario.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/pypiserver$ tree
.
├── packages
│   ├── falcon-1.4.1-py2.py3-none-any.whl
│   ├── mongoengine-0.15.0-py2-none-any.whl
│   ├── pymongo-3.6.1-cp27-none-linux_x86_64.whl
│   ├── python_mimeparse-1.6.0-py2.py3-none-any.whl
│   └── six-1.11.0-py2.py3-none-any.whl
├── docker-compose.yml
└── Dockerfile

&lt;span class="m"&gt;1&lt;/span&gt; directory, &lt;span class="m"&gt;7&lt;/span&gt; files
gerard@sirius:~/workspace/pypiserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Usando nuestro servidor local&lt;/h2&gt;
&lt;p&gt;El comando &lt;strong&gt;pip&lt;/strong&gt; nos ofrece dos formas de añadir nuestro servidor: como URL única o como URL añadida a la normal. Haced un &lt;code&gt;pip install --help&lt;/code&gt; para más detalles. Yo me decanto por usar solamente mi servidor, que aparentemente, tiene todo lo que necesito.&lt;/p&gt;
&lt;p&gt;Supongamos que necesito el paquete &lt;strong&gt;falcon&lt;/strong&gt; en el servidor &lt;strong&gt;snowy&lt;/strong&gt;; lo instalamos con nuestro servidor local (&lt;em&gt;flag&lt;/em&gt; &lt;code&gt;-i&lt;/code&gt;) y punto. Observad las URLs de las que se descarga el paquete:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@snowy:~# pip install --trusted-host &lt;span class="m"&gt;172&lt;/span&gt;.17.0.1 -i http://172.17.0.1:8080/ falcon
Looking in indexes: http://172.17.0.1:8080/
Collecting falcon
  Downloading http://172.17.0.1:8080/packages/falcon-1.4.1-py2.py3-none-any.whl &lt;span class="o"&gt;(&lt;/span&gt;159kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 163kB &lt;span class="m"&gt;7&lt;/span&gt;.8MB/s 
Collecting six&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.4.0 &lt;span class="o"&gt;(&lt;/span&gt;from falcon&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading http://172.17.0.1:8080/packages/six-1.11.0-py2.py3-none-any.whl
Collecting python-mimeparse&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.5.2 &lt;span class="o"&gt;(&lt;/span&gt;from falcon&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading http://172.17.0.1:8080/packages/python_mimeparse-1.6.0-py2.py3-none-any.whl
Installing collected packages: six, python-mimeparse, falcon
Successfully installed falcon-1.4.1 python-mimeparse-1.6.0 six-1.11.0
root@snowy:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: El servidor local es HTTP plano, lo que lo convierte en un servidor no confiable; eso nos obliga  a poner el &lt;em&gt;flag&lt;/em&gt; &lt;code&gt;--trusted-host&lt;/code&gt; para que &lt;strong&gt;pip&lt;/strong&gt; lo quiera usar. Otra opción sería utilizar HTTPS, lo que implica poner un &lt;em&gt;proxy reverso&lt;/em&gt; delante, por ejemplo con &lt;strong&gt;nginx&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Otro servidor podría querer descargar los mismos paquetes u otros sin que eso nos repercuta en problemas, tal y como esperamos:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@stormy:~# pip install --trusted-host &lt;span class="m"&gt;172&lt;/span&gt;.17.0.1 -i http://172.17.0.1:8080/ mongoengine
Looking in indexes: http://172.17.0.1:8080/
Collecting mongoengine
  Downloading http://172.17.0.1:8080/packages/mongoengine-0.15.0-py2-none-any.whl &lt;span class="o"&gt;(&lt;/span&gt;99kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 102kB &lt;span class="m"&gt;6&lt;/span&gt;.6MB/s 
Collecting pymongo&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.7.1 &lt;span class="o"&gt;(&lt;/span&gt;from mongoengine&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading http://172.17.0.1:8080/packages/pymongo-3.6.1-cp27-none-linux_x86_64.whl &lt;span class="o"&gt;(&lt;/span&gt;256kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 266kB &lt;span class="m"&gt;9&lt;/span&gt;.0MB/s 
Collecting six &lt;span class="o"&gt;(&lt;/span&gt;from mongoengine&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading http://172.17.0.1:8080/packages/six-1.11.0-py2.py3-none-any.whl
Installing collected packages: pymongo, six, mongoengine
Successfully installed mongoengine-0.15.0 pymongo-3.6.1 six-1.11.0
root@stormy:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Siempre y cuando utilizen los mismos tipos de CPU y versiones de &lt;strong&gt;python&lt;/strong&gt;, pueden utilizar los mismos &lt;em&gt;wheels&lt;/em&gt;. Eso nos evita compilarlos y reduce notoriamente el tiempo de red invertido en descargarlos.&lt;/p&gt;</content><category term="python"></category><category term="PyPI"></category><category term="wheel"></category><category term="docker"></category></entry><entry><title>MongoDB sharding con docker</title><link href="https://www.linuxsysadmin.ml/2018/07/mongodb-sharding-con-docker.html" rel="alternate"></link><published>2018-07-02T09:00:00+02:00</published><updated>2018-07-02T09:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-07-02:/2018/07/mongodb-sharding-con-docker.html</id><summary type="html">&lt;p&gt;El otro día estaba revisando viejos artículos, y me tropecé con &lt;a href="https://www.linuxsysadmin.ml/2016/05/mongodb-sharding-con-ansible.html"&gt;uno anterior&lt;/a&gt;. Este estaba montado con &lt;strong&gt;ansible&lt;/strong&gt;, y se me pasó por la cabeza reescribirlo usando contenedores con &lt;strong&gt;docker&lt;/strong&gt;. Así pues, vamos a montar exactamente el mismo &lt;em&gt;cluster&lt;/em&gt;, pero con el cambio que la última revolución tecnológica nos aporta …&lt;/p&gt;</summary><content type="html">&lt;p&gt;El otro día estaba revisando viejos artículos, y me tropecé con &lt;a href="https://www.linuxsysadmin.ml/2016/05/mongodb-sharding-con-ansible.html"&gt;uno anterior&lt;/a&gt;. Este estaba montado con &lt;strong&gt;ansible&lt;/strong&gt;, y se me pasó por la cabeza reescribirlo usando contenedores con &lt;strong&gt;docker&lt;/strong&gt;. Así pues, vamos a montar exactamente el mismo &lt;em&gt;cluster&lt;/em&gt;, pero con el cambio que la última revolución tecnológica nos aporta.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sharded cluster" src="https://www.linuxsysadmin.ml/images/sharding_arquitectura_logica.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Aunque lo ideal sería desplegar todas las instancias en varias máquinas diferentes, voy a pasar; por comodidad, voy a desplegar todos los contenedores en una sola máquina mediante &lt;strong&gt;docker-compose&lt;/strong&gt;. De esta forma puedo aprovechar las mismas imágenes sin una ocupación de disco elevada.&lt;/p&gt;
&lt;p&gt;Como de costumbre, vamos a crear una carpeta para contener el proyecto y la vamos a llamar &lt;code&gt;sharding&lt;/code&gt;. En ella voy a depositar los ficheros &lt;code&gt;Dockerfile&lt;/code&gt; necesarios para la construcción de las imágenes, y de paso, el &lt;code&gt;docker-compose.yml&lt;/code&gt; y las configuraciones que vamos a montar como volúmenes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ tree
.
├── build
│   ├── mongo
│   │   └── Dockerfile
│   ├── mongod
│   │   └── Dockerfile
│   └── mongos
│       └── Dockerfile
├── docker-compose.yml
├── mongod_aquila.conf
├── mongod_config.conf
├── mongod_cygnus.conf
├── mongod_lyra.conf
└── mongos.conf

&lt;span class="m"&gt;4&lt;/span&gt; directories, &lt;span class="m"&gt;9&lt;/span&gt; files
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Construyendo las imágenes&lt;/h2&gt;
&lt;p&gt;El primer paso para levantar el entorno son las imágenes que lo sostienen. Necesitamos 3 imágenes: una para el proceso &lt;code&gt;mongod&lt;/code&gt; (que sostiene los &lt;em&gt;shards&lt;/em&gt; y los &lt;em&gt;config server&lt;/em&gt;), una para el proceso &lt;code&gt;mongos&lt;/code&gt; (punto de entrada al &lt;em&gt;cluster&lt;/em&gt;) y otra para el cliente &lt;code&gt;mongo&lt;/code&gt; (que nos sirve para atar el &lt;em&gt;cluster&lt;/em&gt;).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ cat build/mongod/Dockerfile 
FROM alpine:3.7
RUN apk add --no-cache mongodb &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /usr/bin/mongo /usr/bin/mongos /usr/bin/mongoperf &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    install -d -o mongodb -g mongodb -m &lt;span class="m"&gt;0755&lt;/span&gt; /srv/mongodb
USER mongodb
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/bin/mongod&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;--config&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;/etc/mongod.conf&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ cat build/mongos/Dockerfile 
FROM alpine:3.7
RUN apk add --no-cache mongodb &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /usr/bin/mongo /usr/bin/mongod /usr/bin/mongoperf
USER mongodb
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/bin/mongos&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;--config&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;/etc/mongos.conf&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ cat build/mongo/Dockerfile 
FROM alpine:3.7
RUN apk add --no-cache mongodb &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /usr/bin/mongod /usr/bin/mongos /usr/bin/mongoperf
USER mongodb
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo nos queda construirlas usando los comandos habituales:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ docker build -t mongo-server build/mongod/
...
Successfully tagged mongo-server:latest
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ docker build -t mongo-proxy build/mongos/
...
Successfully tagged mongo-proxy:latest
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ docker build -t mongo-client build/mongo/
...
Successfully tagged mongo-client:latest
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Levantando todos los procesos&lt;/h2&gt;
&lt;p&gt;La parte más tediosa de levantar un &lt;em&gt;cluster&lt;/em&gt; es levantar todos los procesos implicados. En el caso del &lt;em&gt;cluster&lt;/em&gt; de ejemplo, necesitamos un mínimo de 13 procesos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 &lt;code&gt;mongos&lt;/code&gt; o más para poder utilizar e &lt;em&gt;cluster&lt;/em&gt; de forma transparente&lt;/li&gt;
&lt;li&gt;3 &lt;code&gt;mongod&lt;/code&gt; en configuración de &lt;em&gt;replica set&lt;/em&gt; para actuar como &lt;em&gt;config servers&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;3 &lt;code&gt;mongod&lt;/code&gt; en configuración de &lt;em&gt;replica set&lt;/em&gt; para actuar como el &lt;em&gt;shard aquila&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;3 &lt;code&gt;mongod&lt;/code&gt; en configuración de &lt;em&gt;replica set&lt;/em&gt; para actuar como el &lt;em&gt;shard lyra&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;3 &lt;code&gt;mongod&lt;/code&gt; en configuración de &lt;em&gt;replica set&lt;/em&gt; para actuar como el &lt;em&gt;shard cygnus&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El contenedor para ejecutar el cliente &lt;code&gt;mongo&lt;/code&gt; no es necesario; lo normal es que cada aplicación consuma directamente los procesos &lt;code&gt;mongos&lt;/code&gt; utilizando el &lt;em&gt;driver&lt;/em&gt;. Para operar el &lt;em&gt;cluster&lt;/em&gt; vamos a levantar el cliente de forma puntual, eliminando el contenedor al acabar.&lt;/p&gt;
&lt;p&gt;Para facilitar el levantado de procesos, vamos a utilizar &lt;strong&gt;docker-compose&lt;/strong&gt;; aquí os dejo el fichero &lt;code&gt;docker-compose.yml&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ cat docker-compose.yml 
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  mongos01:
    image: mongo-proxy
    container_name: mongos01
    hostname: mongos01
    volumes:
      - ./mongos.conf:/etc/mongos.conf:ro
    restart: always
  config01:
    image: mongo-server
    container_name: config01
    hostname: config01
    volumes:
      - ./mongod_config.conf:/etc/mongod.conf:ro
    restart: always
  config02:
    image: mongo-server
    container_name: config02
    hostname: config02
    volumes:
      - ./mongod_config.conf:/etc/mongod.conf:ro
    restart: always
  config03:
    image: mongo-server
    container_name: config03
    hostname: config03
    volumes:
      - ./mongod_config.conf:/etc/mongod.conf:ro
    restart: always
  aquila01:
    image: mongo-server
    container_name: aquila01
    hostname: aquila01
    volumes:
      - ./mongod_aquila.conf:/etc/mongod.conf:ro
    restart: always
  aquila02:
    image: mongo-server
    container_name: aquila02
    hostname: aquila02
    volumes:
      - ./mongod_aquila.conf:/etc/mongod.conf:ro
    restart: always
  aquila03:
    image: mongo-server
    container_name: aquila03
    hostname: aquila03
    volumes:
      - ./mongod_aquila.conf:/etc/mongod.conf:ro
    restart: always
  lyra01:
    image: mongo-server
    container_name: lyra01
    hostname: lyra01
    volumes:
      - ./mongod_lyra.conf:/etc/mongod.conf:ro
    restart: always
  lyra02:
    image: mongo-server
    container_name: lyra02
    hostname: lyra02
    volumes:
      - ./mongod_lyra.conf:/etc/mongod.conf:ro
    restart: always
  lyra03:
    image: mongo-server
    container_name: lyra03
    hostname: lyra03
    volumes:
      - ./mongod_lyra.conf:/etc/mongod.conf:ro
    restart: always
  cygnus01:
    image: mongo-server
    container_name: cygnus01
    hostname: cygnus01
    volumes:
      - ./mongod_cygnus.conf:/etc/mongod.conf:ro
    restart: always
  cygnus02:
    image: mongo-server
    container_name: cygnus02
    hostname: cygnus02
    volumes:
      - ./mongod_cygnus.conf:/etc/mongod.conf:ro
    restart: always
  cygnus03:
    image: mongo-server
    container_name: cygnus03
    hostname: cygnus03
    volumes:
      - ./mongod_cygnus.conf:/etc/mongod.conf:ro
    restart: always
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: Es importante que el &lt;em&gt;hostname&lt;/em&gt; y el &lt;em&gt;container_name&lt;/em&gt; sean el mismo; las &lt;em&gt;replicas&lt;/em&gt; utilizan el &lt;em&gt;hostname&lt;/em&gt; para su descubrimiento, pero el &lt;em&gt;container_name&lt;/em&gt; al conectarse entre ellas.&lt;/p&gt;
&lt;p&gt;Cada elemento dentro del &lt;em&gt;cluster&lt;/em&gt; necesita un parámetro &lt;code&gt;replSetName&lt;/code&gt; indicando el nombre de la &lt;em&gt;replica set&lt;/em&gt; a la que pertenecen. Otro parámetro cambiante es el &lt;code&gt;clusterRole&lt;/code&gt;, dependiendo si la &lt;em&gt;replica set&lt;/em&gt; va a ejercer como &lt;em&gt;config server&lt;/em&gt; o como &lt;em&gt;shard&lt;/em&gt;. Los miembros del mismo &lt;em&gt;replica set&lt;/em&gt; comparten configuración, así que solo necesitamos 4 distintas.&lt;/p&gt;
&lt;p&gt;Empezaremos exponiendo la configuración de los &lt;em&gt;config server&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ cat mongod_config.conf 
processManagement:
  fork: &lt;span class="nb"&gt;false&lt;/span&gt;

net:
  bindIp: &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0
  port: &lt;span class="m"&gt;27019&lt;/span&gt;
  unixDomainSocket:
    enabled: &lt;span class="nb"&gt;false&lt;/span&gt;

storage:
  dbPath: /srv/mongodb
  engine: wiredTiger
  journal:
    enabled: &lt;span class="nb"&gt;true&lt;/span&gt;

replication:
  replSetName: config

sharding:
  clusterRole: configsvr
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La configuración de los &lt;em&gt;shards&lt;/em&gt; es prácticamente la misma; solo hace falta cambiar el &lt;code&gt;clusterRole&lt;/code&gt; el &lt;code&gt;replSetName&lt;/code&gt; y el puerto usado. Empezaremos exponiendo la configuración del primer &lt;em&gt;shard&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ cat mongod_aquila.conf 
processManagement:
  fork: &lt;span class="nb"&gt;false&lt;/span&gt;

net:
  bindIp: &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0
  port: &lt;span class="m"&gt;27018&lt;/span&gt;
  unixDomainSocket:
    enabled: &lt;span class="nb"&gt;false&lt;/span&gt;

storage:
  dbPath: /srv/mongodb
  engine: wiredTiger
  journal:
    enabled: &lt;span class="nb"&gt;true&lt;/span&gt;

replication:
  replSetName: aquila

sharding:
  clusterRole: shardsvr
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: El &lt;em&gt;cluster&lt;/em&gt; original ponía el árbitro en otro puerto para poder ir a la misma máquina. Esto ya no es necesario con &lt;strong&gt;docker&lt;/strong&gt; y nos ahorra poner una configuración nueva.&lt;/p&gt;
&lt;p&gt;Los otros &lt;em&gt;shards&lt;/em&gt; son prácticamente iguales, cambiando solamente el nombre de la &lt;em&gt;replica set&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ diff mongod_aquila.conf mongod_lyra.conf 
17c17
&amp;lt;   replSetName: aquila
---
&amp;gt;   replSetName: lyra
gerard@sirius:~/workspace/sharding$ diff mongod_aquila.conf mongod_cygnus.conf 
17c17
&amp;lt;   replSetName: aquila
---
&amp;gt;   replSetName: cygnus
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto tenemos todo lo necesario para levantar los procesos, así que no lo demoramos más.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;sharding_default&amp;quot;&lt;/span&gt; with the default driver
Creating cygnus03
Creating config03
Creating mongos01
Creating aquila02
Creating lyra03
Creating cygnus02
Creating cygnus01
Creating config01
Creating lyra01
Creating aquila03
Creating lyra02
Creating config02
Creating aquila01
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Atando el cluster&lt;/h2&gt;
&lt;p&gt;Para atar completamente el &lt;em&gt;cluster&lt;/em&gt; se necesita hacer dos cosas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Atar los &lt;em&gt;replica sets&lt;/em&gt; que conformarán los &lt;em&gt;shards&lt;/em&gt; y los &lt;em&gt;config server&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Añadir los &lt;em&gt;shards&lt;/em&gt; ya atados a través de un &lt;em&gt;mongos&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Estas tareas administrativas requieren de un cliente &lt;code&gt;mongo&lt;/code&gt; que no queremos tener de forma permanente, así que tendremos un contenedor de "usar y tirar". De esta forma, cuando acabemos lo destruiremos y no tendremos partes innecesarias.&lt;/p&gt;
&lt;p&gt;Levantar un contenedor con la imagen que contiene el cliente &lt;code&gt;mongo&lt;/code&gt; no tiene misterio. El único detalle es que lo vamos a añadir a la misma red que creó el &lt;em&gt;docker-compose.yml&lt;/em&gt;; eso nos garantiza que podamos usar los &lt;em&gt;container_name&lt;/em&gt; en vez de ir buscando las direcciones IP de cada contenedor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/workspace/sharding$ docker run -ti --rm --net sharding_default mongo-client
/ $ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: A partir de aquí todos los comandos se hacen en el &lt;em&gt;shell&lt;/em&gt; de &lt;em&gt;alpine linux&lt;/em&gt;. Desde esta sesión interactiva, vamos a ir abriendo sesiones de &lt;em&gt;mongo shell&lt;/em&gt; contra los procesos &lt;code&gt;mongod&lt;/code&gt; o &lt;code&gt;mongos&lt;/code&gt; que nos haga falta.&lt;/p&gt;
&lt;p&gt;Atar los &lt;em&gt;replica sets&lt;/em&gt; es siempre igual: entramos en uno de los miembros y le damos una configuración; otra opción es iniciar uno solo de los miembros y añadir los otros.&lt;/p&gt;
&lt;p&gt;Empezaremos con los &lt;em&gt;config servers&lt;/em&gt;, que a partir de la versión 3.2 de &lt;strong&gt;mongodb&lt;/strong&gt; pueden ser &lt;em&gt;replica sets&lt;/em&gt;, y que deben serlo a partir de la versión 3.4 (la que usamos). Entraremos en &lt;em&gt;config01&lt;/em&gt; y lo inicializamos, para añadir los otros dos en el mismo &lt;em&gt;mongo shell&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/ $ mongo --host config01 --port &lt;span class="m"&gt;27019&lt;/span&gt;
...
&amp;gt; rs.initiate&lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;info2&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;no configuration specified. Using a default configuration for the set&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;me&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;config01:27019&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
config:PRIMARY&amp;gt; rs.add&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;config02:27019&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
config:PRIMARY&amp;gt; rs.add&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;config03:27019&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
config:PRIMARY&amp;gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
bye
/ $ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: Los &lt;em&gt;replica sets&lt;/em&gt; destinados a ser &lt;em&gt;config servers&lt;/em&gt; no pueden contener árbitros; si lo intentáis, obtendréis un bonito mensaje de error, pero no habrá consecuencias.&lt;/p&gt;
&lt;p&gt;Repetiremos la fórmula para cada uno de los otros &lt;em&gt;shards&lt;/em&gt;; entramos en el primer contenedor de cada &lt;em&gt;shard&lt;/em&gt;, donde lo inicializamos y añadimos los otros dos. Para ser fieles al artículo original, el tercer contenedor de cada &lt;em&gt;shard&lt;/em&gt; será un árbitro.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/ $ mongo --host aquila01 --port &lt;span class="m"&gt;27018&lt;/span&gt;
...
&amp;gt; rs.initiate&lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;info2&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;no configuration specified. Using a default configuration for the set&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;me&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;aquila01:27018&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
aquila:PRIMARY&amp;gt; rs.add&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;aquila02:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
aquila:PRIMARY&amp;gt; rs.addArb&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;aquila03:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
aquila:PRIMARY&amp;gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
bye
/ $ 
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/ $ mongo --host lyra01 --port &lt;span class="m"&gt;27018&lt;/span&gt;
...
&amp;gt; rs.initiate&lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;info2&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;no configuration specified. Using a default configuration for the set&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;me&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;lyra01:27018&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
lyra:PRIMARY&amp;gt; rs.add&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lyra02:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
lyra:PRIMARY&amp;gt; rs.addArb&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lyra03:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
lyra:PRIMARY&amp;gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
bye
/ $ 
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/ $ mongo --host cygnus01 --port &lt;span class="m"&gt;27018&lt;/span&gt;
...
&amp;gt; rs.initiate&lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;info2&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;no configuration specified. Using a default configuration for the set&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;me&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;cygnus01:27018&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
cygnus:PRIMARY&amp;gt; rs.add&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cygnus02:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
cygnus:PRIMARY&amp;gt; rs.addArb&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cygnus03:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
cygnus:PRIMARY&amp;gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
bye
/ $ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora tenemos 4 &lt;em&gt;replica sets&lt;/em&gt;, uno configurado como &lt;em&gt;config server&lt;/em&gt; y apuntado por el proceso &lt;code&gt;mongos&lt;/code&gt;, y otros 3 que serán los &lt;em&gt;shards&lt;/em&gt;. Vamos a iniciar un &lt;em&gt;mongo shell&lt;/em&gt; contra el proceso &lt;code&gt;mongos&lt;/code&gt;, desde donde vamos a acabar las configuraciones.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/ $ mongo --host mongos01 --port &lt;span class="m"&gt;27017&lt;/span&gt;
...
mongos&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;De hecho, en este punto ya tenemos un &lt;em&gt;cluster&lt;/em&gt; funcional, pero como no tiene &lt;em&gt;shards&lt;/em&gt;, no hay donde guardar datos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mongos&amp;gt; sh.status&lt;span class="o"&gt;()&lt;/span&gt;
--- Sharding Status --- 
  sharding version: &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;minCompatibleVersion&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;5&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;currentVersion&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;6&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;clusterId&amp;quot;&lt;/span&gt; : ObjectId&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;5b182ae62446c4f43cbab312&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
  shards:
  active mongoses:
        &lt;span class="s2"&gt;&amp;quot;3.4.10&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
  autosplit:
        Currently enabled: yes
  balancer:
        Currently enabled:  yes
        Currently running:  no
NaN
        Failed balancer rounds in last &lt;span class="m"&gt;5&lt;/span&gt; attempts:  &lt;span class="m"&gt;0&lt;/span&gt;
        Migration Results &lt;span class="k"&gt;for&lt;/span&gt; the last &lt;span class="m"&gt;24&lt;/span&gt; hours: 
                No recent migrations
  databases:

mongos&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para añadir &lt;em&gt;shards&lt;/em&gt; solamente tenemos que utilizar el método &lt;code&gt;sh.addShard()&lt;/code&gt; para especificar la &lt;em&gt;replica set&lt;/em&gt; que va a actuar como &lt;em&gt;shard&lt;/em&gt;; hay que añadir la &lt;em&gt;replica set&lt;/em&gt; siguiendo la fórmula &lt;code&gt;rsName/server1:port,...,serverN:port&lt;/code&gt;, aunque si especificamos uno solo nombre, basta.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mongos&amp;gt; sh.addShard&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;aquila/aquila01:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;shardAdded&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;aquila&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
mongos&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: A pesar de haber dado solamente el nombre &lt;em&gt;aquila01&lt;/em&gt;, el resto de servidores ha sido descubierto por el &lt;em&gt;cluster&lt;/em&gt; de forma automática; aún así, los árbitros no aparecen en el listado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mongos&amp;gt; sh.status&lt;span class="o"&gt;()&lt;/span&gt;
--- Sharding Status --- 
  sharding version: &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;minCompatibleVersion&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;5&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;currentVersion&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;6&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;clusterId&amp;quot;&lt;/span&gt; : ObjectId&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;5b182ae62446c4f43cbab312&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
  shards:
        &lt;span class="o"&gt;{&lt;/span&gt;  &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;aquila&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;host&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;aquila/aquila01:27018,aquila02:27018&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;state&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
  active mongoses:
        &lt;span class="s2"&gt;&amp;quot;3.4.10&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
  autosplit:
        Currently enabled: yes
  balancer:
        Currently enabled:  yes
        Currently running:  no
NaN
        Failed balancer rounds in last &lt;span class="m"&gt;5&lt;/span&gt; attempts:  &lt;span class="m"&gt;0&lt;/span&gt;
        Migration Results &lt;span class="k"&gt;for&lt;/span&gt; the last &lt;span class="m"&gt;24&lt;/span&gt; hours: 
                No recent migrations
  databases:

mongos&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a repetir la fórmula para añadir los otros &lt;em&gt;shards&lt;/em&gt;, que es básicamente la misma:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mongos&amp;gt; sh.addShard&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lyra/lyra01:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;shardAdded&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;lyra&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
mongos&amp;gt; sh.addShard&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cygnus/cygnus01:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;shardAdded&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;cygnus&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
mongos&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y de esta forma, ya podemos ver el &lt;em&gt;cluster&lt;/em&gt; acabado, con sus 3 &lt;em&gt;shards&lt;/em&gt; añadidos sin problemas.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mongos&amp;gt; sh.status&lt;span class="o"&gt;()&lt;/span&gt;
--- Sharding Status --- 
  sharding version: &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;minCompatibleVersion&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;5&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;currentVersion&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;6&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;clusterId&amp;quot;&lt;/span&gt; : ObjectId&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;5b182ae62446c4f43cbab312&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
  shards:
        &lt;span class="o"&gt;{&lt;/span&gt;  &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;aquila&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;host&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;aquila/aquila01:27018,aquila02:27018&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;state&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;{&lt;/span&gt;  &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;cygnus&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;host&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;cygnus/cygnus01:27018,cygnus02:27018&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;state&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;{&lt;/span&gt;  &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;lyra&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;host&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;lyra/lyra01:27018,lyra02:27018&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;state&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
  active mongoses:
        &lt;span class="s2"&gt;&amp;quot;3.4.10&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
  autosplit:
        Currently enabled: yes
  balancer:
        Currently enabled:  yes
        Currently running:  no
NaN
        Failed balancer rounds in last &lt;span class="m"&gt;5&lt;/span&gt; attempts:  &lt;span class="m"&gt;0&lt;/span&gt;
        Migration Results &lt;span class="k"&gt;for&lt;/span&gt; the last &lt;span class="m"&gt;24&lt;/span&gt; hours: 
                No recent migrations
  databases:

mongos&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Como no necesitamos más el contenedor del &lt;em&gt;mongo shell&lt;/em&gt;, salimos de él para que el sistema lo pueda reciclar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mongos&amp;gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
bye
/ $ &lt;span class="nb"&gt;exit&lt;/span&gt;
gerard@sirius:~/workspace/sharding$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto estamos listos para introducir nuestros datos, aunque añadir más procesos &lt;code&gt;mongos&lt;/code&gt; nos dará alta disponibilidad para el acceso de nuestras aplicaciones, aunque los &lt;em&gt;shards&lt;/em&gt; y los &lt;em&gt;config servers&lt;/em&gt; ya disfrutan de ella.&lt;/p&gt;</content><category term="mongodb"></category><category term="docker"></category><category term="sharding"></category><category term="cluster"></category></entry><entry><title>Un reciclaje de Tomcat usando Docker</title><link href="https://www.linuxsysadmin.ml/2018/06/un-reciclaje-de-tomcat-usando-docker.html" rel="alternate"></link><published>2018-06-25T10:00:00+02:00</published><updated>2018-06-25T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-06-25:/2018/06/un-reciclaje-de-tomcat-usando-docker.html</id><summary type="html">&lt;p&gt;Hace poco he cambiado de trabajo por motivos personales. En mi nueva posición me he encontrado con un cambio en las tecnologías usadas; lo que me he encontrado es algo que hacía tiempo que no tocaba: basan sus sistemas en &lt;strong&gt;Java&lt;/strong&gt; y &lt;strong&gt;Tomcat&lt;/strong&gt;. He necesitado un ligero reciclaje en ellos …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Hace poco he cambiado de trabajo por motivos personales. En mi nueva posición me he encontrado con un cambio en las tecnologías usadas; lo que me he encontrado es algo que hacía tiempo que no tocaba: basan sus sistemas en &lt;strong&gt;Java&lt;/strong&gt; y &lt;strong&gt;Tomcat&lt;/strong&gt;. He necesitado un ligero reciclaje en ellos.&lt;/p&gt;
&lt;p&gt;Por supuesto, no voy a perder el tiempo en instalar un servidor y luego el &lt;strong&gt;Tomcat&lt;/strong&gt; pertinente; experiencias pasadas nos demuestran que es mucho más fácil utilizar imágenes &lt;strong&gt;Docker&lt;/strong&gt;, que son fáciles de usar y no necesitan tiempo de &lt;em&gt;setup&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Vamos a partir de la imagen de &lt;strong&gt;Tomcat&lt;/strong&gt; oficial que podemos encontrar &lt;a href="https://hub.docker.com/_/tomcat/"&gt;aquí&lt;/a&gt;. Uno de los grandes aciertos de los que mantienen las imágenes es que, por muchos &lt;em&gt;tags&lt;/em&gt; que generen, todos funcionan bajo los mismos parámetros; esto nos permite hacer pruebas con el &lt;em&gt;tag&lt;/em&gt; de &lt;em&gt;Alpine Linux&lt;/em&gt;, y luego -si fuera necesario- saltar a algunos de los &lt;em&gt;tags&lt;/em&gt; basados en &lt;em&gt;Debian&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;En los ejemplos también se hace uso de un fichero &lt;em&gt;.war&lt;/em&gt; de prueba, llamado &lt;em&gt;sample.war&lt;/em&gt;, que he sacado de &lt;a href="https://tomcat.apache.org/tomcat-7.0-doc/appdev/sample/"&gt;aquí&lt;/a&gt;, concretamente &lt;a href="https://tomcat.apache.org/tomcat-7.0-doc/appdev/sample/sample.war"&gt;este&lt;/a&gt;. Lo que hace nuestra aplicación es irrelevante en este momento, y solamente lo vamos a utilizar para probar su disponibilidad.&lt;/p&gt;
&lt;p&gt;El concepto que debemos tener claro en una instalación de &lt;strong&gt;Tomcat&lt;/strong&gt; es la carpeta en la que está instalado, marcada por la variable de entorno CATALINA_HOME; el resto es relativo a esta carpeta. En esta imagen concreta tenemos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Carpeta base&lt;/strong&gt;: CATALINA_HOME &amp;rarr; /usr/local/tomcat&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Carpeta de aplicaciones&lt;/strong&gt;: CATALINA_HOME/webapps &amp;rarr; /usr/local/tomcat/webapps&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Carpeta de logs&lt;/strong&gt;: CATALINA_HOME/logs &amp;rarr; /usr/local/tomcat/logs&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Un despliegue básico&lt;/h2&gt;
&lt;p&gt;La opción más fácil para exponer nuestras aplicaciones es dejarlas simplemente en la carpeta de aplicaciones y reiniciar el servidor de aplicaciones. No es la opción más correcta desde el punto de vista de la disponibilidad, pero con un uso inteligente de los balanceadores es una opción válida.&lt;/p&gt;
&lt;p&gt;La idea de fondo es que &lt;strong&gt;Tomcat&lt;/strong&gt; descomprime el fichero &lt;em&gt;.war&lt;/em&gt; en el momento de levantarse. Solo tenemos que asegurar que el fichero &lt;em&gt;.war&lt;/em&gt; está en su sitio. Para ir rápido, no voy a generar imágenes nuevas y voy a inyectar el &lt;em&gt;.war&lt;/em&gt; mediante el uso de volúmenes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/tomcat$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  appserver:
    image: tomcat:alpine
    container_name: appserver
    hostname: appserver
    ports:
      - &lt;span class="s2"&gt;&amp;quot;8080:8080&amp;quot;&lt;/span&gt;
    volumes:
      - ./tomcat/sample.war:/usr/local/tomcat/webapps/sample.war:ro
    restart: always
gerard@atlantis:~/workspace/tomcat$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo nos queda levantar el servicio, pudiendo encontrar el resultado en &lt;a href="http://localhost:8080/sample/"&gt;http://localhost:8080/sample/&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/tomcat$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;tomcat_default&amp;quot;&lt;/span&gt; with the default driver
Creating appserver ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@atlantis:~/workspace/tomcat$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Exponiendo el manager&lt;/h2&gt;
&lt;p&gt;Cuando tenemos una aplicación es natural que hayan evolutivos y se necesite cambiar el fichero. Como &lt;strong&gt;Tomcat&lt;/strong&gt; maneja varias aplicaciones, un reinicio las afecta a todas y causa un &lt;em&gt;downtime&lt;/em&gt; importante, por no mencionar que el tiempo para levantar el servicio se dispara.&lt;/p&gt;
&lt;p&gt;Afortunadamente, &lt;strong&gt;Tomcat&lt;/strong&gt; nos ofrece una aplicación que tiene como única finalidad, administrar otras aplicaciones. Es el &lt;em&gt;manager&lt;/em&gt; y lo podemos localizar en &lt;a href="http://localhost:8080/manager/html"&gt;http://localhost:8080/manager/html&lt;/a&gt;, o siguiendo un botón desde la página inicial en &lt;a href="http://localhost:8080/"&gt;http://localhost:8080/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Este &lt;em&gt;manager&lt;/em&gt; se puede acceder definiendo un usuario en &lt;code&gt;conf/tomcat-users.xml&lt;/code&gt;, y por defecto permite entrar a los usuario en la máquina local. Este comportamiento se puede cambiar mediante un fichero de contexto, que pondremos en &lt;code&gt;conf/Catalina/localhost/&amp;lt;aplicación&amp;gt;.xml&lt;/code&gt;. Veamos como habilitar el &lt;em&gt;manager&lt;/em&gt; y el &lt;em&gt;host-manager&lt;/em&gt;, mediante el añadido de los ficheros de configuración usando volúmenes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/tomcat$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  appserver:
    image: tomcat:alpine
    container_name: appserver
    hostname: appserver
    ports:
      - &lt;span class="s2"&gt;&amp;quot;8080:8080&amp;quot;&lt;/span&gt;
    volumes:
      - ./tomcat/tomcat-users.xml:/usr/local/tomcat/conf/tomcat-users.xml:ro
      - ./tomcat/context.xml:/usr/local/tomcat/conf/Catalina/localhost/manager.xml:ro
      - ./tomcat/context.xml:/usr/local/tomcat/conf/Catalina/localhost/host-manager.xml:ro
    restart: always
gerard@atlantis:~/workspace/tomcat$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/tomcat$ cat tomcat/tomcat-users.xml
&amp;lt;?xml &lt;span class="nv"&gt;version&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;1.0&amp;quot;&lt;/span&gt; &lt;span class="nv"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;UTF-8&amp;quot;&lt;/span&gt;?&amp;gt;
&amp;lt;tomcat-users &lt;span class="nv"&gt;xmlns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://tomcat.apache.org/xml&amp;quot;&lt;/span&gt;
              xmlns:xsi&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot;&lt;/span&gt;
              xsi:schemaLocation&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://tomcat.apache.org/xml tomcat-users.xsd&amp;quot;&lt;/span&gt;
              &lt;span class="nv"&gt;version&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;1.0&amp;quot;&lt;/span&gt;&amp;gt;
  &amp;lt;role &lt;span class="nv"&gt;rolename&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;manager-gui&amp;quot;&lt;/span&gt;/&amp;gt;
  &amp;lt;role &lt;span class="nv"&gt;rolename&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;admin-gui&amp;quot;&lt;/span&gt;/&amp;gt;
  &amp;lt;user &lt;span class="nv"&gt;username&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;manager&amp;quot;&lt;/span&gt; &lt;span class="nv"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;manager1234&amp;quot;&lt;/span&gt; &lt;span class="nv"&gt;roles&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;manager-gui,admin-gui&amp;quot;&lt;/span&gt;/&amp;gt;
&amp;lt;/tomcat-users&amp;gt;
gerard@atlantis:~/workspace/tomcat$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/tomcat$ cat tomcat/context.xml
&amp;lt;Context &lt;span class="nv"&gt;privileged&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt; &lt;span class="nv"&gt;antiResourceLocking&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;false&amp;quot;&lt;/span&gt;
         &lt;span class="nv"&gt;docBase&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;catalina&lt;/span&gt;&lt;span class="p"&gt;.home&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;/webapps/manager&amp;quot;&lt;/span&gt;&amp;gt;
    &amp;lt;Valve &lt;span class="nv"&gt;className&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;org.apache.catalina.valves.RemoteAddrValve&amp;quot;&lt;/span&gt; &lt;span class="nv"&gt;allow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;^.*&lt;/span&gt;$&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; /&amp;gt;
&amp;lt;/Context&amp;gt;
gerard@atlantis:~/workspace/tomcat$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Como el contenido del fichero &lt;code&gt;context.xml&lt;/code&gt; vale para habilitar ambas aplicaciones, podemos crear la ilusión dentro del contenedor de que son 2 ficheros, aunque realmente se trata del mismo.&lt;/p&gt;
&lt;p&gt;Con esto deberíamos ser capaces de acceder al &lt;em&gt;manager&lt;/em&gt; y al &lt;em&gt;host-manager&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Añadiendo virtualhosts&lt;/h2&gt;
&lt;p&gt;Cada aplicación que pongamos en &lt;strong&gt;Tomcat&lt;/strong&gt; acaba disponible en &lt;code&gt;http://dominio:8080/&amp;lt;aplicacion&amp;gt;/&lt;/code&gt;. Esto nos plantea una serie de cuestiones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Como montamos una aplicación por (sub)dominio?&lt;/li&gt;
&lt;li&gt;Como nos libramos de la coletilla &lt;code&gt;/&amp;lt;aplicacion&amp;gt;/&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;Como añado autenticación básica?&lt;/li&gt;
&lt;li&gt;Como tratamos una terminación SSL?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El servidor de aplicaciones &lt;strong&gt;Tomcat&lt;/strong&gt; no se encarga de eso. Si bien es cierto que se puede encargar de la parte de SSL, es una configuración complicada; la parte de los dominios se debería tratar en la aplicación misma, con lo que es trabajo extra y la limpieza de la URL es imposible.&lt;/p&gt;
&lt;p&gt;En estos casos, la recomendación es tener un &lt;em&gt;proxy reverso&lt;/em&gt; delante que cree estas ilusiones de forma fácil y confiable; mi recomendación personal es poner un &lt;strong&gt;nginx&lt;/strong&gt;, que resume todo el truco en un fichero de configuración y que podemos inyectar también como un volumen.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/tomcat$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  webserver:
    image: sirrtea/nginx:alpine
    container_name: webserver
    hostname: webserver
    ports:
      - &lt;span class="s2"&gt;&amp;quot;8080:80&amp;quot;&lt;/span&gt;
    volumes:
      - ./nginx/sample.conf:/etc/nginx/conf.d/sample.conf:ro
    depends_on:
      - appserver
    restart: always
  appserver:
    image: tomcat:alpine
    container_name: appserver
    hostname: appserver
    volumes:
      - ./tomcat/sample.war:/usr/local/tomcat/webapps/sample.war:ro
    restart: always
gerard@atlantis:~/workspace/tomcat$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/workspace/tomcat$ cat nginx/sample.conf
server &lt;span class="o"&gt;{&lt;/span&gt;
        listen &lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        server_name sample.example.com&lt;span class="p"&gt;;&lt;/span&gt;
        location / &lt;span class="o"&gt;{&lt;/span&gt;
                proxy_pass http://appserver:8080/sample/&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@atlantis:~/workspace/tomcat$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Tras levantar el conjunto de contenedores con &lt;em&gt;docker-compose&lt;/em&gt;, sucede la magia:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solo llegaremos al &lt;strong&gt;Tomcat&lt;/strong&gt; mediante el dominio &lt;code&gt;sample.example.com&lt;/code&gt;, puerto 80&lt;/li&gt;
&lt;li&gt;Todas las peticiones hechas a &lt;code&gt;/loremipsum&lt;/code&gt; van parar a &lt;code&gt;appserver:8080/sample/loremipsum&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;El &lt;strong&gt;Nginx&lt;/strong&gt; consume la parte de la aplicación &lt;code&gt;/sample&lt;/code&gt; en la URL&lt;/li&gt;
&lt;li&gt;No hay SSL en este ejemplo, pero ponerlo no entraña ninguna dificultad extra&lt;/li&gt;
&lt;li&gt;No se ha puesto autenticación básica, pero tampoco cuesta demasiado&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con esto podemos ocultar el &lt;em&gt;manager&lt;/em&gt; y otras aplicaciones, que podríamos exponer mediante otros (sub)dominios, o puertos; eso nos permite hacer un uso inteligente de un &lt;em&gt;firewall&lt;/em&gt; para evitar la exposición de partes privadas de nuestro proyecto.&lt;/p&gt;</content><category term="tomcat"></category><category term="docker"></category></entry><entry><title>Un servidor git con frontal web: Gitea</title><link href="https://www.linuxsysadmin.ml/2018/06/un-servidor-git-con-frontal-web-gitea.html" rel="alternate"></link><published>2018-06-11T10:00:00+02:00</published><updated>2018-06-11T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-06-11:/2018/06/un-servidor-git-con-frontal-web-gitea.html</id><summary type="html">&lt;p&gt;Los servidores de &lt;strong&gt;git&lt;/strong&gt; son muy útiles, pero si solo lo accedemos mediante terminal, se quedan limitados a pocos usuarios avanzados. Sin embargo, las soluciones con interfaz web, como &lt;strong&gt;GitHub&lt;/strong&gt; llegan a todo tipo de usuarios. En un intento de abaratar costes, se han hecho varios clones, entre ellos, &lt;strong&gt;Gitea …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Los servidores de &lt;strong&gt;git&lt;/strong&gt; son muy útiles, pero si solo lo accedemos mediante terminal, se quedan limitados a pocos usuarios avanzados. Sin embargo, las soluciones con interfaz web, como &lt;strong&gt;GitHub&lt;/strong&gt; llegan a todo tipo de usuarios. En un intento de abaratar costes, se han hecho varios clones, entre ellos, &lt;strong&gt;Gitea&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Realmente hay muchos clones, como &lt;strong&gt;Gitlab&lt;/strong&gt; o &lt;strong&gt;Gogs&lt;/strong&gt;; de hecho, &lt;strong&gt;Gitea&lt;/strong&gt; es un clon de &lt;strong&gt;Gogs&lt;/strong&gt; hecho en lenguaje &lt;strong&gt;Go&lt;/strong&gt;. Y lo que me llama especialmente la atención es la facilidad en que lo pude instalar: se trata simplemente de ejecutar un contenedor &lt;strong&gt;Docker&lt;/strong&gt; que, a diferencia de otros, ocupa relativamente poco espacio de disco.&lt;/p&gt;
&lt;h2&gt;Instalación&lt;/h2&gt;
&lt;p&gt;Lo primero es hacer un &lt;code&gt;docker pull gitea/gitea&lt;/code&gt;, que es la forma de traernos la imagen desde su correspondiente &lt;a href="https://hub.docker.com/r/gitea/gitea/"&gt;repositorio de &lt;strong&gt;DockerHub&lt;/strong&gt;&lt;/a&gt;. La misma documentación del repositorio indica como debe ejecutarse.&lt;/p&gt;
&lt;p&gt;Como punto interesante, &lt;strong&gt;Gitea&lt;/strong&gt; utiliza una base de datos para guardar toda aquella información de la página web que no queda reflejada en el propio repositorio. Esta pequeña configuración se indica de forma web, en la primera invocación de la interfaz; las posibilidades son varias: &lt;strong&gt;MySQL&lt;/strong&gt;, &lt;strong&gt;MSSQL&lt;/strong&gt;, &lt;strong&gt;PostgreSQL&lt;/strong&gt; e incluso una base de datos local &lt;strong&gt;SQLite3&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Es vuestra decisión elegir la que usar, pero por brevedad voy a levantar la instancia de pruebas sin un servidor de base de datos dedicado, confiando en &lt;strong&gt;SQLite3&lt;/strong&gt;. Por otra parte, los repositorios se alojan en el sistema de ficheros local; para evitar perderlos en caso de reinicio, lo voy a poner como un volumen local. Os pongo un &lt;em&gt;docker-compose.yml&lt;/em&gt; de ejemplo, para simplificar el despliegue:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/tools/gitea$ cat docker-compose.yml 
version: &lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;
services:
  gitea:
    image: gitea/gitea
    volumes:
      - ./data:/data
    ports:
      - &lt;span class="s2"&gt;&amp;quot;3000:3000&amp;quot;&lt;/span&gt;
      - &lt;span class="s2"&gt;&amp;quot;22:22&amp;quot;&lt;/span&gt;
gerard@sirius:~/tools/gitea$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Evidentemente, la carpeta de datos de &lt;strong&gt;Gitea&lt;/strong&gt; debe existir en el servidor, con lo que la he creado en la misma carpeta:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/tools/gitea$ tree
.
├── data
└── docker-compose.yml

&lt;span class="m"&gt;1&lt;/span&gt; directory, &lt;span class="m"&gt;1&lt;/span&gt; file
gerard@sirius:~/tools/gitea$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo nos queda levantar los contenedores usando los comandos habituales de &lt;em&gt;docker-compose&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/tools/gitea$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;gitea_default&amp;quot;&lt;/span&gt; with the default driver
Creating gitea_gitea_1
gerard@sirius:~/tools/gitea$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Nuestra instancia de &lt;strong&gt;Gitea&lt;/strong&gt; queda expuesta en &lt;em&gt;localhost&lt;/em&gt;, concretamente el puerto 3000 para la web, y el puerto 22 para la comunicación SSH con la que se lanzan las peticiones de &lt;em&gt;clone&lt;/em&gt;, &lt;em&gt;push&lt;/em&gt; o &lt;em&gt;pull&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Cabe decir que la primera vez que entremos en la web habrá que configurar algunos detalles; solo recomiendo tocar los de la base de datos y aquellos que son meramente cosméticos, como por ejemplo el título del sitio. Si no rellenáis la parte del administrador, este rol va a recaer en el primer usuario que se registre; aseguraos de ser vosotros.&lt;/p&gt;
&lt;p&gt;Os dejo una imagen que vale más que mil palabras.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Frontal de Gitea" src="https://www.linuxsysadmin.ml/images/gitea.jpg"&gt;&lt;/p&gt;</content><category term="git"></category><category term="gitea"></category><category term="docker"></category></entry><entry><title>Un balanceador dinámico con consul-template</title><link href="https://www.linuxsysadmin.ml/2018/05/un-balanceador-dinamico-con-consul-template.html" rel="alternate"></link><published>2018-05-07T10:00:00+02:00</published><updated>2018-05-07T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-05-07:/2018/05/un-balanceador-dinamico-con-consul-template.html</id><summary type="html">&lt;p&gt;Aquellos que leéis mis artículos habitualmente ya sabéis lo que es un balanceador de carga, especialmente los de peticiones HTTP; en especial conocemos &lt;strong&gt;nginx&lt;/strong&gt; y &lt;strong&gt;haproxy&lt;/strong&gt;. La parte mala de estos servicios es que la configuración es estática e inmutable, y en un mundo &lt;em&gt;cloud&lt;/em&gt;, eso no es lo ideal …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Aquellos que leéis mis artículos habitualmente ya sabéis lo que es un balanceador de carga, especialmente los de peticiones HTTP; en especial conocemos &lt;strong&gt;nginx&lt;/strong&gt; y &lt;strong&gt;haproxy&lt;/strong&gt;. La parte mala de estos servicios es que la configuración es estática e inmutable, y en un mundo &lt;em&gt;cloud&lt;/em&gt;, eso no es lo ideal.&lt;/p&gt;
&lt;p&gt;En el momento en que pasamos de servidores tradicionales al modelo &lt;em&gt;cloud&lt;/em&gt;, nos damos cuenta que no es importante que el servidor X o el servidor Y funcionen; lo que queremos es &lt;strong&gt;dar un servicio&lt;/strong&gt;, y no nos importan los servidores que sean; incluso podemos aumentar o decrementar su número con facilidad.&lt;/p&gt;
&lt;p&gt;En estos casos, es muy conveniente tener un servicio de &lt;em&gt;discovery&lt;/em&gt;, que nos sepa decir qué servidores tenemos y qué servicios hay alojados en ellos; &lt;strong&gt;Consul&lt;/strong&gt; es uno de ellos, que ya vimos con anterioridad.&lt;/p&gt;
&lt;p&gt;Sin embargo, seguimos teniendo que reconfigurar los balanceadores manualmente y recargando su configuración. Para ello se creó &lt;strong&gt;consul-template&lt;/strong&gt;, que no es más que un proceso que se dedica a construir ficheros de configuración cuando &lt;strong&gt;consul&lt;/strong&gt; le indica que ha habido un cambio relevante; en este momento, &lt;strong&gt;consul-template&lt;/strong&gt; regenerará la configuración del servicio y opcionalmente lanzará un comando indicado.&lt;/p&gt;
&lt;p&gt;Juntando nuestro servicio de balanceador con &lt;strong&gt;consul-template&lt;/strong&gt; podemos conseguir fácilmente la ilusión de un balancear dinámico: &lt;strong&gt;consul-template&lt;/strong&gt; regenerará la configuración del balanceador y lanzará el comando necesario para que el balanceador la recargue.&lt;/p&gt;
&lt;h2&gt;Un ejemplo: balanceando peticiones web con HAProxy&lt;/h2&gt;
&lt;p&gt;Como decisión de diseño, y para simplificar vamos a ver el siguiente escenario:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tenemos un servidor web en &lt;em&gt;localhost:8001&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Tenemos un servidor web en &lt;em&gt;localhost:8002&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Vamos a exponer en &lt;em&gt;localhost:80&lt;/em&gt; las peticiones, con &lt;strong&gt;HAProxy&lt;/strong&gt; y con un algoritmo de &lt;em&gt;round-robin&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;El balanceador es un contenedor &lt;strong&gt;Docker&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;consul-template&lt;/strong&gt; también ejecuta como un contendor &lt;strong&gt;Docker&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;De hecho, todo esto también sirve para otros servicios, como por ejemplo, &lt;strong&gt;nginx&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Ejecutando consul&lt;/h3&gt;
&lt;p&gt;Lo primero es ejecutar un proceso &lt;strong&gt;consul&lt;/strong&gt; con los servicios declarados y con sus respectivos &lt;em&gt;checks&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/services/consul$ cat consul.json
&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;services&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web1&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;port&amp;quot;&lt;/span&gt;: &lt;span class="m"&gt;8001&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;,
    &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web2&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;port&amp;quot;&lt;/span&gt;: &lt;span class="m"&gt;8002&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;]&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;checks&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web1&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;service_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web1&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;http&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;http://localhost:8001/&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;interval&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;5s&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;timeout&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;5s&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;,
    &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web2&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;service_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web2&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;http&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;http://localhost:8002/&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;interval&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;5s&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;timeout&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;5s&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@atlantis:~/projects/services/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Es especialmente crítico que ambos servicios y ambos &lt;em&gt;checks&lt;/em&gt; tengan identificadores diferentes, porque sino, &lt;strong&gt;consul&lt;/strong&gt; no los percibe como cosas diferentes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/services/consul$ ./consul agent -dev --advertise &lt;span class="m"&gt;10&lt;/span&gt;.0.2.15 -client &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0 -config-file consul.json
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Starting Consul agent...
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Consul agent running!
           Version: &lt;span class="s1"&gt;&amp;#39;v1.0.6&amp;#39;&lt;/span&gt;
           Node ID: &lt;span class="s1"&gt;&amp;#39;7d05eed1-f9db-2b02-499f-1bcdb37bf73c&amp;#39;&lt;/span&gt;
         Node name: &lt;span class="s1"&gt;&amp;#39;atlantis&amp;#39;&lt;/span&gt;
        Datacenter: &lt;span class="s1"&gt;&amp;#39;dc1&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;Segment: &lt;span class="s1"&gt;&amp;#39;&amp;lt;all&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
            Server: &lt;span class="nb"&gt;true&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;Bootstrap: &lt;span class="nb"&gt;false&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
       Client Addr: &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.0.0.0&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;HTTP: &lt;span class="m"&gt;8500&lt;/span&gt;, HTTPS: -1, DNS: &lt;span class="m"&gt;8600&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
      Cluster Addr: &lt;span class="m"&gt;10&lt;/span&gt;.0.2.15 &lt;span class="o"&gt;(&lt;/span&gt;LAN: &lt;span class="m"&gt;8301&lt;/span&gt;, WAN: &lt;span class="m"&gt;8302&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
           Encrypt: Gossip: false, TLS-Outgoing: false, TLS-Incoming: &lt;span class="nb"&gt;false&lt;/span&gt;

&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Log data will now stream in as it occurs:
...
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y lo dejamos funcionado.&lt;/p&gt;
&lt;h3&gt;El balanceador y consul-template&lt;/h3&gt;
&lt;p&gt;El balanceador no tiene ningún misterio; se trata de un &lt;strong&gt;haproxy&lt;/strong&gt; normal y corriente, con la única peculiaridad de que la carpeta &lt;em&gt;/etc/haproxy/&lt;/em&gt; es un volúmen, de forma que el contenedor de &lt;strong&gt;consul-template&lt;/strong&gt; lo pueda exportar y escribir en él. De esta forma podemos "dar un cambiazo" al fichero de configuración desde otro contenedor.&lt;/p&gt;
&lt;p&gt;Estaría bien tener en la imagen del balanceador un &lt;em&gt;script&lt;/em&gt; que supiera como recargar la configuración del balanceador de forma fina y delicada, de forma que el otro contenedor simplemente ejecutaría un &lt;code&gt;docker exec&lt;/code&gt; para "pedirle" que lo hiciera, sin entrar en detalles de como se hace. Para agilizar el artículo, nos limitaremos a hacer un &lt;code&gt;docker restart&lt;/code&gt;, que no es ideal, pero nos vale de momento.&lt;/p&gt;
&lt;p&gt;Por su parte, el contenedor que ejecuta &lt;strong&gt;consul-template&lt;/strong&gt; tampoco tiene ningún misterio. Se limita a exportar la carpeta de configuración de &lt;strong&gt;haproxy&lt;/strong&gt; y ejecutar &lt;strong&gt;consul-template&lt;/strong&gt;, de forma continua y limitándose a crear &lt;code&gt;/etc/haproxy/haproxy.cfg&lt;/code&gt; a partir de la información del &lt;strong&gt;consul&lt;/strong&gt; local y la plantilla suministrada.&lt;/p&gt;
&lt;p&gt;Si se diera el caso de un cambio en el servicio implicado, &lt;strong&gt;consul-template&lt;/strong&gt; regeneraría la configuración de &lt;strong&gt;haproxy&lt;/strong&gt;; como &lt;em&gt;bonus&lt;/em&gt;, va a reiniciar el contenedor de &lt;strong&gt;haproxy&lt;/strong&gt; para que este aplique la nueva configuración. No es la mejor manera de hacer las cosas, pero el "como reinicar un &lt;strong&gt;haproxy&lt;/strong&gt;" no es la parte relevante del artículo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Para más información de como controlar un contenedor alojado en el mismo &lt;em&gt;host&lt;/em&gt; en que corre el nuestro, podemos seguir &lt;a href="https://www.linuxsysadmin.ml/2018/04/controlando-docker-desde-un-contenedor.html"&gt;este otro artículo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;El contenedor de &lt;strong&gt;consul-template&lt;/strong&gt; solo tiene lo necesario para usar el comando &lt;code&gt;docker&lt;/code&gt; y el mismo &lt;code&gt;consul-template&lt;/code&gt;. Los añado como referencia:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/balancer$ tree templater_build/
templater_build/
├── consul-template
└── Dockerfile

&lt;span class="m"&gt;0&lt;/span&gt; directories, &lt;span class="m"&gt;2&lt;/span&gt; files
gerard@atlantis:~/projects/balancer$ cat templater_build/Dockerfile
FROM alpine:3.7
RUN apk add --no-cache docker &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /usr/bin/docker-proxy &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /usr/bin/docker-containerd-shim &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /usr/bin/docker-runc &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /usr/bin/docker-containerd-ctr &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /usr/bin/docker-containerd &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /usr/bin/dockerd
COPY consul-template /usr/bin/
gerard@atlantis:~/projects/balancer$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;También añado como referencia el &lt;em&gt;docker-compose.yml&lt;/em&gt; con el que se levantan ambos contenedores.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/balancer$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;
services:
  loadbalancer:
    image: sirrtea/haproxy:alpine
    container_name: balancer
    hostname: balancer
    network_mode: host
    volumes:
      - /etc/haproxy
  templater:
    image: templater
    container_name: templater
    hostname: templater
    network_mode: host
    volumes:
      - ./haproxy.ctmpl:/tmp/haproxy.ctmpl:ro
      - /var/run/docker.sock:/var/run/docker.sock
    volumes_from:
      - loadbalancer
    command: consul-template -template &lt;span class="s2"&gt;&amp;quot;/tmp/haproxy.ctmpl:/etc/haproxy/haproxy.cfg:docker restart balancer&amp;quot;&lt;/span&gt;
gerard@atlantis:~/projects/balancer$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Fijáos especialmente en:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;El volumen en &lt;em&gt;/etc/haproxy/&lt;/em&gt;, para poder compartir el fichero de configuración.&lt;/li&gt;
&lt;li&gt;El volumen &lt;em&gt;/var/run/docker.sock&lt;/em&gt; para controlar el servidor &lt;strong&gt;docker&lt;/strong&gt; del &lt;em&gt;host&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;La plantilla, que también se añade como volumen.&lt;/li&gt;
&lt;li&gt;Y como curiosidad, el comando &lt;strong&gt;consul-template&lt;/strong&gt; necesario, con el comando de &lt;em&gt;restart&lt;/em&gt; del contendor &lt;em&gt;balancer&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La plantilla de &lt;strong&gt;haproxy&lt;/strong&gt; tampoco tiene ninguna complejidad...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/balancer$ cat haproxy.ctmpl
global
    chroot /var/lib/haproxy
    user haproxy
    group haproxy

defaults
    mode http

listen stats
    &lt;span class="nb"&gt;bind&lt;/span&gt; *:8080
    stats &lt;span class="nb"&gt;enable&lt;/span&gt;
    stats uri /

listen web
    &lt;span class="nb"&gt;bind&lt;/span&gt; *:80
    balance roundrobin
&lt;span class="o"&gt;{{&lt;/span&gt; range service &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}}&lt;/span&gt;
    server &lt;span class="o"&gt;{{&lt;/span&gt; .ID &lt;span class="o"&gt;}}&lt;/span&gt; &lt;span class="o"&gt;{{&lt;/span&gt; .Address &lt;span class="o"&gt;}}&lt;/span&gt;:&lt;span class="o"&gt;{{&lt;/span&gt; .Port &lt;span class="o"&gt;}}&lt;/span&gt;
&lt;span class="o"&gt;{{&lt;/span&gt; end &lt;span class="o"&gt;}}&lt;/span&gt;
gerard@atlantis:~/projects/balancer$
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Resultado&lt;/h3&gt;
&lt;p&gt;Com ambos servicios &lt;em&gt;web&lt;/em&gt; funcionando todo va como se espera (también lo podemos comprobar en la página de estadísticas de &lt;strong&gt;haproxy&lt;/strong&gt;, en el puerto 8080):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/balancer$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; balancer cat /etc/haproxy/haproxy.cfg &lt;span class="p"&gt;|&lt;/span&gt; grep server
    server web1 &lt;span class="m"&gt;10&lt;/span&gt;.0.2.15:8001
    server web2 &lt;span class="m"&gt;10&lt;/span&gt;.0.2.15:8002
gerard@atlantis:~/projects/balancer$ curl http://localhost:80/
web1
gerard@atlantis:~/projects/balancer$ curl http://localhost:80/
web2
gerard@atlantis:~/projects/balancer$ curl http://localhost:80/
web1
gerard@atlantis:~/projects/balancer$ curl http://localhost:80/
web2
gerard@atlantis:~/projects/balancer$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Si se cae, por ejemplo, el servicio &lt;em&gt;web1&lt;/em&gt; en el puerto 8001, &lt;strong&gt;consul&lt;/strong&gt; lo detecta. En este momento, &lt;strong&gt;consul-template&lt;/strong&gt; regenera la configuración y reinica el contenedor &lt;em&gt;balancer&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/balancer$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; balancer cat /etc/haproxy/haproxy.cfg &lt;span class="p"&gt;|&lt;/span&gt; grep server
    server web2 &lt;span class="m"&gt;10&lt;/span&gt;.0.2.15:8002
gerard@atlantis:~/projects/balancer$ curl http://localhost:80/
web2
gerard@atlantis:~/projects/balancer$ curl http://localhost:80/
web2
gerard@atlantis:~/projects/balancer$ curl http://localhost:80/
web2
gerard@atlantis:~/projects/balancer$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Se cae el servicio &lt;em&gt;web2&lt;/em&gt;, y nos quedamos sin servicio completamente, pero la configuración queda como se espera:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/balancer$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; balancer cat /etc/haproxy/haproxy.cfg &lt;span class="p"&gt;|&lt;/span&gt; grep server
gerard@atlantis:~/projects/balancer$ curl http://localhost:80/
&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;503 Service Unavailable&amp;lt;/h1&amp;gt;
No server is available to handle this request.
&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
gerard@atlantis:~/projects/balancer$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo nos quedaría restablecer el servicio tanto en &lt;em&gt;web1&lt;/em&gt; como en &lt;em&gt;web2&lt;/em&gt;, y verificar que el servicio global se restablece:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/balancer$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; balancer cat /etc/haproxy/haproxy.cfg &lt;span class="p"&gt;|&lt;/span&gt; grep server
    server web1 &lt;span class="m"&gt;10&lt;/span&gt;.0.2.15:8001
    server web2 &lt;span class="m"&gt;10&lt;/span&gt;.0.2.15:8002
gerard@atlantis:~/projects/balancer$ curl http://localhost:80/
web1
gerard@atlantis:~/projects/balancer$ curl http://localhost:80/
web2
gerard@atlantis:~/projects/balancer$ curl http://localhost:80/
web1
gerard@atlantis:~/projects/balancer$ curl http://localhost:80/
web2
gerard@atlantis:~/projects/balancer$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El siguiente paso sería añadir nuevos nodos con &lt;strong&gt;consul&lt;/strong&gt; para observar como la configuración de &lt;strong&gt;hoproxy&lt;/strong&gt; crece. De esta forma, no tendremos que preocuparnos de la configuración del balanceador nunca más; solamente de tener la plantilla actualizada si añadimos más aplicaciones.&lt;/p&gt;</content><category term="consul"></category><category term="service discovery"></category><category term="balanceador"></category></entry><entry><title>Levantando un cluster de consul</title><link href="https://www.linuxsysadmin.ml/2018/04/levantando-un-cluster-de-consul.html" rel="alternate"></link><published>2018-04-30T10:00:00+02:00</published><updated>2018-04-30T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-04-30:/2018/04/levantando-un-cluster-de-consul.html</id><summary type="html">&lt;p&gt;Ya vimos que &lt;strong&gt;consul&lt;/strong&gt; nos permitía mantener una foto del estado de nuestros servidores y de los servicios que corren en ellos. Es todavía más importante cuando contamos con varios servidores, y todos declaran sus partes a un servidor central, de forma que tenemos una foto global de la situación …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ya vimos que &lt;strong&gt;consul&lt;/strong&gt; nos permitía mantener una foto del estado de nuestros servidores y de los servicios que corren en ellos. Es todavía más importante cuando contamos con varios servidores, y todos declaran sus partes a un servidor central, de forma que tenemos una foto global de la situación.&lt;/p&gt;
&lt;p&gt;Para este ejemplo, vamos a contar con 3 servidores; uno de los cuales va a actuar de &lt;em&gt;servidor&lt;/em&gt; y el resto harán de &lt;em&gt;clientes&lt;/em&gt;. Los servidores son:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;node1&lt;/strong&gt; &amp;rarr; Dirección IP 10.0.0.2 (será el &lt;em&gt;servidor&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;node2&lt;/strong&gt; &amp;rarr; Dirección IP 10.0.0.3 (será un &lt;em&gt;cliente&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;node3&lt;/strong&gt; &amp;rarr; Dirección IP 10.0.0.4 (será un &lt;em&gt;cliente&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: Es posible poner varios &lt;em&gt;servidores&lt;/em&gt; para obtener alta disponibilidad, pero al no ser un servício crítico, no vamos a extender el artículo innecesariamente.&lt;/p&gt;
&lt;h2&gt;Instalación&lt;/h2&gt;
&lt;p&gt;Ya vimos en &lt;a href="https://www.linuxsysadmin.ml/2018/04/monitorizacion-y-service-discovery-con-consul.html"&gt;otro articulo&lt;/a&gt; que &lt;strong&gt;consul&lt;/strong&gt; no necesita instalación, ya que es un binario estático. Para su fácil distribución entre las máquinas, lo he empaquetado en una imagen de &lt;strong&gt;docker&lt;/strong&gt;, acompañado de una carpeta &lt;em&gt;/data/&lt;/em&gt; que es donde &lt;strong&gt;consul&lt;/strong&gt; deja sus ficheros operativos. Vamos a inyectar la configuración desde el servidor mediante &lt;em&gt;host volumes&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Para tener el artículo completo, adjunto el contexto con el que se contruyó la imagen que usamos en el mismo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/consul/build$ cat Dockerfile
FROM scratch
ADD rootfs.tar.gz /
ENTRYPOINT &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/consul&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@atlantis:~/projects/consul/build$ tar tf rootfs.tar.gz
consul
data/
gerard@atlantis:~/projects/consul/build$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Esto nos deja una imagen de unos 28mb. He usado el &lt;em&gt;tag&lt;/em&gt; &lt;code&gt;sirrtea/consul:1.0.5&lt;/code&gt; y lo he subido de forma temporal a &lt;a href="https://hub.docker.com/"&gt;DockerHub&lt;/a&gt;, que va a funcionar como repositorio de imágenes. No hace falta decir que 1.0.5 es la versión de &lt;strong&gt;consul&lt;/strong&gt; en el momento de escribir el artículo...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Los &lt;em&gt;clientes&lt;/em&gt; son volátiles; no necesitamos persistir su carpeta de datos porque no guardan nada importante. Sin embargo, la carpeta &lt;em&gt;/data/&lt;/em&gt; de los servidores guardan información importante del &lt;em&gt;cluster&lt;/em&gt; y debe asegurarse que no se pierden en el reinicio del contenedor.&lt;/p&gt;
&lt;h2&gt;Levantando el master&lt;/h2&gt;
&lt;p&gt;Ponemos en &lt;strong&gt;node1&lt;/strong&gt; un carpeta con el fichero &lt;em&gt;docker-compose.yml&lt;/em&gt; y la configuración vacía, en donde lo tendremos todo ordenado.&lt;/p&gt;
&lt;p&gt;Es importante recalcar que la configuración la mapeamos desde el &lt;em&gt;host&lt;/em&gt; y también la carpeta de datos, para que al reiniciar el contenedor no de pierda. Si eso pasara, habría que volver a añadir manualmente todos los otros nodos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;
services:
  consul:
    image: sirrtea/consul:1.0.5
    container_name: consul
    hostname: consul
    network_mode: host
    volumes:
      - ./consul.json:/consul.json
      - ./data:/data
    command: agent -node node1 -advertise &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2 -data-dir /data --config-file /consul.json -server -bootstrap-expect&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
    restart: always
gerard@node1:~/consul$ cat consul.json
&lt;span class="o"&gt;{}&lt;/span&gt;
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Levantamos el servidor con &lt;strong&gt;docker-compose&lt;/strong&gt; de la forma habitual:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ docker-compose up -d
Creating consul ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos ver que el &lt;em&gt;cluster&lt;/em&gt; solo tiene el servidor, ya que no hemos puesto otros nodos:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; consul /consul members
Node   Address        Status  Type    Build  Protocol  DC   Segment
node1  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:8301  alive   server  &lt;span class="m"&gt;1&lt;/span&gt;.0.5  &lt;span class="m"&gt;2&lt;/span&gt;         dc1  &amp;lt;all&amp;gt;
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo nos queda por ver que el &lt;em&gt;servidor&lt;/em&gt; asume el rol de &lt;em&gt;leader&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ docker-compose logs &lt;span class="p"&gt;|&lt;/span&gt; grep -o &lt;span class="s2"&gt;&amp;quot;New leader elected.*&amp;quot;&lt;/span&gt;
New leader elected: node1
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Añadiendo los clientes&lt;/h2&gt;
&lt;p&gt;De forma similar, vamos a crear una carpeta en &lt;strong&gt;node2&lt;/strong&gt; y en &lt;strong&gt;node3&lt;/strong&gt; (y en un futuro en el resto de nodos) para contener el fichero &lt;em&gt;docker-compose.yml&lt;/em&gt; y la configuración de &lt;strong&gt;consul&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Vamos a poner una configuración vacía; más adelante ya añadiremos servicios y &lt;em&gt;health checks&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node2:~/consul$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;
services:
  consul:
    image: sirrtea/consul:1.0.5
    container_name: consul
    hostname: consul
    network_mode: host
    volumes:
      - ./consul.json:/consul.json
    command: agent -node node2 -advertise &lt;span class="m"&gt;10&lt;/span&gt;.0.0.3 -data-dir /data --config-file /consul.json -join node1
    restart: always
gerard@node2:~/consul$ cat consul.json
&lt;span class="o"&gt;{}&lt;/span&gt;
gerard@node2:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: La operación de &lt;em&gt;join&lt;/em&gt; se puede hacer en el comando de inicio o manualmente &lt;em&gt;a posteriori&lt;/em&gt;; la primera forma nos simplifica bastante el trabajo.&lt;/p&gt;
&lt;p&gt;Levantamos el contenedor de &lt;strong&gt;consul&lt;/strong&gt; y verificamos que el &lt;em&gt;cluster&lt;/em&gt; conoce al nuevo miembro:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node2:~/consul$ docker-compose up -d
Creating consul ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@node2:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; consul /consul members
Node   Address        Status  Type    Build  Protocol  DC   Segment
node1  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:8301  alive   server  &lt;span class="m"&gt;1&lt;/span&gt;.0.5  &lt;span class="m"&gt;2&lt;/span&gt;         dc1  &amp;lt;all&amp;gt;
node2  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.3:8301  alive   client  &lt;span class="m"&gt;1&lt;/span&gt;.0.5  &lt;span class="m"&gt;2&lt;/span&gt;         dc1  &amp;lt;default&amp;gt;
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Repetimos lo mismo para el &lt;strong&gt;node3&lt;/strong&gt; (cuidado con la IP de &lt;em&gt;advertise&lt;/em&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; consul /consul members
Node   Address        Status  Type    Build  Protocol  DC   Segment
node1  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:8301  alive   server  &lt;span class="m"&gt;1&lt;/span&gt;.0.5  &lt;span class="m"&gt;2&lt;/span&gt;         dc1  &amp;lt;all&amp;gt;
node2  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.3:8301  alive   client  &lt;span class="m"&gt;1&lt;/span&gt;.0.5  &lt;span class="m"&gt;2&lt;/span&gt;         dc1  &amp;lt;default&amp;gt;
node3  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.4:8301  alive   client  &lt;span class="m"&gt;1&lt;/span&gt;.0.5  &lt;span class="m"&gt;2&lt;/span&gt;         dc1  &amp;lt;default&amp;gt;
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La misma verificación se puede hacer solicitando la dirección de los nuevos nodos:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ dig @127.0.0.1 -p &lt;span class="m"&gt;8600&lt;/span&gt; node1.node.consul +short
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2
gerard@node1:~/consul$ dig @127.0.0.1 -p &lt;span class="m"&gt;8600&lt;/span&gt; node2.node.consul +short
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3
gerard@node1:~/consul$ dig @127.0.0.1 -p &lt;span class="m"&gt;8600&lt;/span&gt; node3.node.consul +short
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.4
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Añadiendo servicios&lt;/h2&gt;
&lt;p&gt;Supongamos ahora que queremos declarar un servicio &lt;em&gt;web&lt;/em&gt; en &lt;strong&gt;node1&lt;/strong&gt; y &lt;strong&gt;node2&lt;/strong&gt;. Simplemente vamos a añadir una configuración adecuada y a reiniciar el contenedor de &lt;strong&gt;consul&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A pesar de que &lt;strong&gt;node1&lt;/strong&gt; es un &lt;em&gt;servidor&lt;/em&gt; y &lt;strong&gt;node2&lt;/strong&gt; es un &lt;em&gt;cliente&lt;/em&gt;, eso solo afecta a la mecánica del &lt;em&gt;cluster&lt;/em&gt;. Siguen siendo dos &lt;em&gt;agentes&lt;/em&gt; normales a todos los efectos; se configuran y se operan de la misma forma.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ cat consul.json
&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;services&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;port&amp;quot;&lt;/span&gt;: &lt;span class="m"&gt;8080&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;]&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;checks&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;service_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;http&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;http://localhost:8080/&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;interval&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;5s&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;timeout&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;5s&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@node1:~/consul$ docker-compose restart
Restarting consul ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: al tener la carpeta &lt;em&gt;/data/&lt;/em&gt; en el &lt;em&gt;host&lt;/em&gt; en donde corre el &lt;em&gt;servidor&lt;/em&gt;, no perdemos la lista de &lt;em&gt;clientes&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; consul /consul members
Node   Address        Status  Type    Build  Protocol  DC   Segment
node1  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:8301  alive   server  &lt;span class="m"&gt;1&lt;/span&gt;.0.5  &lt;span class="m"&gt;2&lt;/span&gt;         dc1  &amp;lt;all&amp;gt;
node2  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.3:8301  alive   client  &lt;span class="m"&gt;1&lt;/span&gt;.0.5  &lt;span class="m"&gt;2&lt;/span&gt;         dc1  &amp;lt;default&amp;gt;
node3  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.4:8301  alive   client  &lt;span class="m"&gt;1&lt;/span&gt;.0.5  &lt;span class="m"&gt;2&lt;/span&gt;         dc1  &amp;lt;default&amp;gt;
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y lo mismo para &lt;strong&gt;node2&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node2:~/consul$ cat consul.json
&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;services&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;port&amp;quot;&lt;/span&gt;: &lt;span class="m"&gt;8080&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;]&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;checks&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;service_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;http&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;http://localhost:8080/&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;interval&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;5s&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;timeout&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;5s&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@node2:~/consul$ docker-compose restart
Restarting consul ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@node2:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo nos queda por observar que el servidor DNS integrado nos devuelve ambos, ya que están saludables:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ dig @127.0.0.1 -p &lt;span class="m"&gt;8600&lt;/span&gt; web.service.consul +short
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Caídas e incrementos de servicio&lt;/h2&gt;
&lt;p&gt;Si se cayera, por ejemplo, la web de &lt;strong&gt;node1&lt;/strong&gt;, el servidor DNS no la devolvería, al no estar saludable:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ dig @127.0.0.1 -p &lt;span class="m"&gt;8600&lt;/span&gt; web.service.consul +short
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Este detalle hace que podamos añadir el servicio en otro nodo &lt;strong&gt;antes&lt;/strong&gt; del mismo servicio. Como la web no funciona, &lt;strong&gt;consul&lt;/strong&gt; no devolvería el nuevo nodo hasta que la web estuviera instalada, funcionando y saludable.&lt;/p&gt;
&lt;p&gt;Así pues, vamos a declarar el servicio &lt;em&gt;web&lt;/em&gt; también en &lt;strong&gt;nodo3&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node3:~/consul$ cat consul.json
&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;services&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;port&amp;quot;&lt;/span&gt;: &lt;span class="m"&gt;8080&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;]&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;checks&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;service_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;web&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;http&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;http://localhost:8080/&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;interval&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;5s&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;timeout&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;5s&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@node3:~/consul$ docker-compose restart
Restarting consul ... &lt;span class="k"&gt;done&lt;/span&gt;
gerard@node3:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y sin sorpresas, el servidor DNS (y la API) de &lt;strong&gt;consul&lt;/strong&gt; no nos devolverían las 3 direcciones, ya que la tercera no funciona.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ dig @127.0.0.1 -p &lt;span class="m"&gt;8600&lt;/span&gt; web.service.consul +short
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo nos faltaría levantar la web en el &lt;strong&gt;nodo3&lt;/strong&gt; para que el servidor nos devolviera este nodo como proveedor del servicio &lt;em&gt;web&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@node1:~/consul$ dig @127.0.0.1 -p &lt;span class="m"&gt;8600&lt;/span&gt; web.service.consul
...
&lt;span class="p"&gt;;;&lt;/span&gt; QUESTION SECTION:
&lt;span class="p"&gt;;&lt;/span&gt;web.service.consul.            IN      A

&lt;span class="p"&gt;;;&lt;/span&gt; ANSWER SECTION:
web.service.consul.     &lt;span class="m"&gt;0&lt;/span&gt;       IN      A       &lt;span class="m"&gt;10&lt;/span&gt;.0.0.4
web.service.consul.     &lt;span class="m"&gt;0&lt;/span&gt;       IN      A       &lt;span class="m"&gt;10&lt;/span&gt;.0.0.3
web.service.consul.     &lt;span class="m"&gt;0&lt;/span&gt;       IN      A       &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2

&lt;span class="p"&gt;;;&lt;/span&gt; Query time: &lt;span class="m"&gt;0&lt;/span&gt; msec
&lt;span class="p"&gt;;;&lt;/span&gt; SERVER: &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1#8600&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;127&lt;/span&gt;.0.0.1&lt;span class="o"&gt;)&lt;/span&gt;
...
gerard@node1:~/consul$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A partir de ahora, es responsabilidad del que use este DNS elegir una entre las respuestas dadas.&lt;/p&gt;</content><category term="consul"></category><category term="service discovery"></category><category term="cluster"></category></entry><entry><title>Alta disponibilidad con Docker Swarm</title><link href="https://www.linuxsysadmin.ml/2018/04/alta-disponibilidad-con-docker-swarm.html" rel="alternate"></link><published>2018-04-09T10:00:00+02:00</published><updated>2018-04-09T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-04-09:/2018/04/alta-disponibilidad-con-docker-swarm.html</id><summary type="html">&lt;p&gt;He visto muchos artículos por internet que hacen maravillas para tener un &lt;em&gt;cluster&lt;/em&gt; de &lt;strong&gt;Docker Swarm&lt;/strong&gt; funcional. Puede que en versiones anteriores fuera así, pero cada vez se ha simplificado más el &lt;em&gt;setup&lt;/em&gt; para alinearse con la filosofía de la simplicidad, frente a otras soluciones más completas, pero más complejas …&lt;/p&gt;</summary><content type="html">&lt;p&gt;He visto muchos artículos por internet que hacen maravillas para tener un &lt;em&gt;cluster&lt;/em&gt; de &lt;strong&gt;Docker Swarm&lt;/strong&gt; funcional. Puede que en versiones anteriores fuera así, pero cada vez se ha simplificado más el &lt;em&gt;setup&lt;/em&gt; para alinearse con la filosofía de la simplicidad, frente a otras soluciones más completas, pero más complejas.&lt;/p&gt;
&lt;p&gt;Hoy vamos a mostrar como conseguir alta disponibilidad en un &lt;em&gt;cluster&lt;/em&gt; de &lt;strong&gt;docker swarm&lt;/strong&gt;, de forma que si se cae un &lt;em&gt;manager&lt;/em&gt;, otro asume su lugar y reestructura los servicios para continuar cumpliendo las especificaciones indicadas.&lt;/p&gt;
&lt;p&gt;Para hacerlo, vamos a disponer de 5 máquinas, todas con &lt;strong&gt;Docker&lt;/strong&gt; instalado, siendo irrelevante el sistema operativo, incluso mezclado:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;swarm1&lt;/strong&gt; &amp;rarr; 10.0.0.2 (será un &lt;em&gt;manager&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;swarm2&lt;/strong&gt; &amp;rarr; 10.0.0.3 (será un &lt;em&gt;manager&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;swarm3&lt;/strong&gt; &amp;rarr; 10.0.0.4 (será un &lt;em&gt;manager&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;swarm4&lt;/strong&gt; &amp;rarr; 10.0.0.5 (será un &lt;em&gt;worker&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;swarm5&lt;/strong&gt; &amp;rarr; 10.0.0.6 (será un &lt;em&gt;worker&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Es importante recalcar que el &lt;em&gt;cluster&lt;/em&gt; se rige por un protocolo de &lt;em&gt;gossip&lt;/em&gt; tipo &lt;strong&gt;Raft&lt;/strong&gt;, lo que significa que necesita que más de la mitad de los &lt;em&gt;managers&lt;/em&gt; estén funcionales, con lo que un número impar de ellos es lo ideal; pondremos 3 para este ejemplo.&lt;/p&gt;
&lt;h2&gt;Creación del cluster&lt;/h2&gt;
&lt;p&gt;Como todo &lt;em&gt;cluster&lt;/em&gt; de &lt;strong&gt;Docker Swarm&lt;/strong&gt;, empezamos inicializando un solo nodo, que va a ser el &lt;em&gt;manager&lt;/em&gt; y de paso, el &lt;em&gt;leader&lt;/em&gt; (el &lt;em&gt;manager&lt;/em&gt; que manda).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@swarm1:~$ docker swarm init
Swarm initialized: current node &lt;span class="o"&gt;(&lt;/span&gt;t2x1d9ep99ff5o7ggf0f7dgzh&lt;span class="o"&gt;)&lt;/span&gt; is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-221kzb5ajbg3p9cu7cnxj0zp8fper4dbnzn9ntjkjxrs0mamoz-avftdi8c5t7l4y59zcjq4k6r4 &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:2377

To add a manager to this swarm, run &lt;span class="s1"&gt;&amp;#39;docker swarm join-token manager&amp;#39;&lt;/span&gt; and follow the instructions.

gerard@swarm1:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos comprobar que ya tenemos un &lt;em&gt;cluster&lt;/em&gt; con un solo nodo, que es &lt;em&gt;manager&lt;/em&gt; y &lt;em&gt;leader&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@swarm1:~$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
t2x1d9ep99ff5o7ggf0f7dgzh *   swarm1              Ready               Active              Leader
gerard@swarm1:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para añadir nodos, solo hace falta hacer un &lt;code&gt;docker swarm join&lt;/code&gt;, y su rol va a depender del &lt;em&gt;token&lt;/em&gt; con el que nos unamos. El comando &lt;code&gt;docker swarm init&lt;/code&gt; ya nos indicó el &lt;em&gt;token&lt;/em&gt; para un &lt;em&gt;worker&lt;/em&gt; y como sacar el &lt;em&gt;token&lt;/em&gt; para un &lt;em&gt;manager&lt;/em&gt;. Saquemos los dos de nuevo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@swarm1:~$ docker swarm join-token manager
To add a manager to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-221kzb5ajbg3p9cu7cnxj0zp8fper4dbnzn9ntjkjxrs0mamoz-agq9fp4a86ahs8ll7oa2z56j7 &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:2377

gerard@swarm1:~$ docker swarm join-token worker
To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-221kzb5ajbg3p9cu7cnxj0zp8fper4dbnzn9ntjkjxrs0mamoz-avftdi8c5t7l4y59zcjq4k6r4 &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:2377

gerard@swarm1:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo necesitamos entrar en cada uno de los nodos y lanzar el comando suministrado. En este caso vamos a lanzar el comando para crear un &lt;em&gt;manager&lt;/em&gt; en &lt;strong&gt;swarm2&lt;/strong&gt; y &lt;strong&gt;swarm3&lt;/strong&gt;, mientras que haremos &lt;em&gt;workers&lt;/em&gt; de &lt;strong&gt;swarm4&lt;/strong&gt; y &lt;strong&gt;swarm5&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@swarm2:~$ docker swarm join --token SWMTKN-1-221kzb5ajbg3p9cu7cnxj0zp8fper4dbnzn9ntjkjxrs0mamoz-agq9fp4a86ahs8ll7oa2z56j7 &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:2377
This node joined a swarm as a manager.
gerard@swarm2:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@swarm3:~$ docker swarm join --token SWMTKN-1-221kzb5ajbg3p9cu7cnxj0zp8fper4dbnzn9ntjkjxrs0mamoz-agq9fp4a86ahs8ll7oa2z56j7 &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:2377
This node joined a swarm as a manager.
gerard@swarm3:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@swarm4:~$ docker swarm join --token SWMTKN-1-221kzb5ajbg3p9cu7cnxj0zp8fper4dbnzn9ntjkjxrs0mamoz-avftdi8c5t7l4y59zcjq4k6r4 &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:2377
This node joined a swarm as a worker.
gerard@swarm4:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@swarm5:~$ docker swarm join --token SWMTKN-1-221kzb5ajbg3p9cu7cnxj0zp8fper4dbnzn9ntjkjxrs0mamoz-avftdi8c5t7l4y59zcjq4k6r4 &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:2377
This node joined a swarm as a worker.
gerard@swarm5:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto tenemos el &lt;em&gt;cluster&lt;/em&gt; completo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@swarm1:~$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
t2x1d9ep99ff5o7ggf0f7dgzh *   swarm1              Ready               Active              Leader
p8td0udza1chfy3ehb01wkm7p     swarm2              Ready               Active              Reachable
st9wv2k1fdmocam3dpmnas20c     swarm3              Ready               Active              Reachable
0t7m80of6t0nwo8zwn7vupwix     swarm4              Ready               Active
8sl99fhp1hnc3bt620fxdnn6y     swarm5              Ready               Active
gerard@swarm1:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Es importante recalcar que, aunque hay 3 &lt;em&gt;managers&lt;/em&gt;, solo uno es &lt;em&gt;leader&lt;/em&gt;, y es el que lleva la voz cantante del &lt;em&gt;cluster&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Pruebas de alta disponibilidad&lt;/h2&gt;
&lt;p&gt;Ya vimos en &lt;a href="https://www.linuxsysadmin.ml/2017/11/uso-basico-de-un-cluster-docker-swarm.html"&gt;otro artículo&lt;/a&gt; que en caso de caída de un &lt;em&gt;worker&lt;/em&gt;, el &lt;em&gt;manager&lt;/em&gt; se encarga de recolocar los contenedores para seguir ofreciendo el servicio. Así que nos vamos a limitar a tirar &lt;em&gt;managers&lt;/em&gt;, que es lo que era vulnerable en el &lt;em&gt;cluster&lt;/em&gt; anterior.&lt;/p&gt;
&lt;p&gt;Empezaremos tirando &lt;strong&gt;swarm1&lt;/strong&gt; que es un &lt;em&gt;manager&lt;/em&gt; y un &lt;em&gt;leader&lt;/em&gt;; esto es lo que queda:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@swarm2:~$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
t2x1d9ep99ff5o7ggf0f7dgzh     swarm1              Unknown             Active              Unreachable
p8td0udza1chfy3ehb01wkm7p *   swarm2              Ready               Active              Reachable
st9wv2k1fdmocam3dpmnas20c     swarm3              Ready               Active              Leader
0t7m80of6t0nwo8zwn7vupwix     swarm4              Ready               Active
8sl99fhp1hnc3bt620fxdnn6y     swarm5              Ready               Active
gerard@swarm2:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Sin sorpresas, &lt;strong&gt;swarm1&lt;/strong&gt; es dado por &lt;em&gt;unreachable&lt;/em&gt; y no pasa nada, más allá de elegir otro &lt;em&gt;leader&lt;/em&gt; para que siga manteniendo el estado del &lt;em&gt;cluster&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Intentemos ahora quitar otro de los &lt;em&gt;managers&lt;/em&gt;, lo que haría que solo quedara uno:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@swarm3:~$ docker swarm leave
Error response from daemon: You are attempting to leave the swarm on a node that is participating as a manager. Removing this node leaves &lt;span class="m"&gt;1&lt;/span&gt; managers out of &lt;span class="m"&gt;3&lt;/span&gt;. Without a Raft quorum your swarm will be inaccessible. The only way to restore a swarm that has lost consensus is to reinitialize it with &lt;span class="sb"&gt;`&lt;/span&gt;--force-new-cluster&lt;span class="sb"&gt;`&lt;/span&gt;. Use &lt;span class="sb"&gt;`&lt;/span&gt;--force&lt;span class="sb"&gt;`&lt;/span&gt; to suppress this message.
gerard@swarm3:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Eso no nos lo deja hacer, ya que entonces no tendríamos &lt;em&gt;quorum&lt;/em&gt; y meteríamos la pata a lo grande. Eso es lo mismo que va a pasar si apagamos el servidor.&lt;/p&gt;
&lt;p&gt;Para tener &lt;em&gt;quorum&lt;/em&gt; con 3 &lt;em&gt;managers&lt;/em&gt;, necesitamos que &lt;strong&gt;más de la mitad&lt;/strong&gt; estén funcionales; esto nos obliga a mantener más de 1.5 funcionales, dos en este caso. Si hay previsiones de que se caigan o se paren más, habría que tener más &lt;em&gt;managers&lt;/em&gt;, para que la tolerancia a nodos caídos fuera superior.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;1 manager&lt;/strong&gt; &amp;rarr; quorum &amp;gt; 0.5, necesitamos = 1, tolerancia = 0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2 managers&lt;/strong&gt; &amp;rarr; quorum &amp;gt; 1, necesitamos = 2, tolerancia = 0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3 managers&lt;/strong&gt; &amp;rarr; quorum &amp;gt; 1.5, necesitamos = 2, tolerancia = 1&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;5 managers&lt;/strong&gt; &amp;rarr; quorum &amp;gt; 2.5, necesitamos = 3, tolerancia = 2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Solo nos queda ver que en caso de recuperación del nodo &lt;strong&gt;swarm1&lt;/strong&gt;, el &lt;em&gt;cluster&lt;/em&gt; lo reconoce y todo vuelve a la normalidad, excepto que no se desbanca al nuevo &lt;em&gt;leader&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@swarm2:~$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
t2x1d9ep99ff5o7ggf0f7dgzh     swarm1              Ready               Active              Reachable
p8td0udza1chfy3ehb01wkm7p *   swarm2              Ready               Active              Reachable
st9wv2k1fdmocam3dpmnas20c     swarm3              Ready               Active              Leader
0t7m80of6t0nwo8zwn7vupwix     swarm4              Ready               Active
8sl99fhp1hnc3bt620fxdnn6y     swarm5              Ready               Active
gerard@swarm2:~$
&lt;/pre&gt;&lt;/div&gt;</content><category term="docker"></category><category term="swarm"></category><category term="cluster"></category><category term="alta disponibilidad"></category></entry><entry><title>Usando autenticación en MongoDB</title><link href="https://www.linuxsysadmin.ml/2018/03/usando-autenticacion-en-mongodb.html" rel="alternate"></link><published>2018-03-26T10:00:00+02:00</published><updated>2018-03-26T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-03-26:/2018/03/usando-autenticacion-en-mongodb.html</id><summary type="html">&lt;p&gt;Usar autenticación en las bases de datos de nuestros entornos, por muy privados que sean, suele ser una buena idea. Nos sirve para separar los accesos a un servicio compartido y evitar sobreescrituras cuando accidentalmente dos servicios usan la misma base de datos por un error de algún usuario despistado …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Usar autenticación en las bases de datos de nuestros entornos, por muy privados que sean, suele ser una buena idea. Nos sirve para separar los accesos a un servicio compartido y evitar sobreescrituras cuando accidentalmente dos servicios usan la misma base de datos por un error de algún usuario despistado.&lt;/p&gt;
&lt;p&gt;No es la primera vez que un &lt;em&gt;cluster&lt;/em&gt; compartido entre varios proyectos acaba con la destrucción de datos accidental; el uso habitual de &lt;em&gt;copy-paste&lt;/em&gt; en nuestros &lt;em&gt;docker-compose.yml&lt;/em&gt; o en otros ficheros de configuración, nos plantea un posible riesgo cuando alguien se olvida de cambiar el nombre de la base de datos.&lt;/p&gt;
&lt;p&gt;Estos casos son fácilmente evitables si las plantillas contienen unos parámetros de &lt;strong&gt;usuario&lt;/strong&gt; y &lt;strong&gt;password&lt;/strong&gt; no usables, y solamente la correcta combinación de ambos con la base de datos dan acceso a los datos. De esta forma, las diferentes bases de datos serian accesibles por diferentes usuarios y haría falta conocer todos los datos de acceso para usar la base de datos de otra aplicación.&lt;/p&gt;
&lt;h2&gt;Preparación&lt;/h2&gt;
&lt;p&gt;En &lt;strong&gt;MongoDB&lt;/strong&gt;, los usuarios pertenecen a una base de datos, y no son globales. Esto significa que tenemos que crearlos en una base de datos concreta y que cualquiera que quiera autenticarse debe hacerlo contra la base de datos que contenga su usuario. La autorización en sí garantiza mediante la aplicación de diferentes &lt;em&gt;roles&lt;/em&gt;, a nivel de base de datos o a nivel global.&lt;/p&gt;
&lt;p&gt;Como decisión de diseño, vamos a poner todos los usuarios en la misma base de datos, que va a ser &lt;em&gt;admin&lt;/em&gt;. Para ello necesitamos abrir un cliente a la base de datos, que por simplicidad va a ser el &lt;em&gt;shell&lt;/em&gt; básico, binario &lt;code&gt;mongo&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Crear un usuario de administración&lt;/h3&gt;
&lt;p&gt;Se trata simplemente de crear un usuario, con el &lt;em&gt;role&lt;/em&gt; suficiente para hacer las tareas necesarias. Como se trata de un usuario muy exclusivo, le voy a dar el &lt;em&gt;role root&lt;/em&gt;, que básicamente me lo permite hacer todo. Podéis adaptar vuestro comando eligiendo el &lt;em&gt;role&lt;/em&gt; que más os convenga, según &lt;a href="https://docs.mongodb.com/manual/reference/built-in-roles/#built-in-roles"&gt;la documentación&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; use admin
switched to db admin
&amp;gt; db.createUser(
...   {
...     user: &amp;quot;admin&amp;quot;,
...     pwd: &amp;quot;s3cr3t&amp;quot;,
...     roles: [ { role: &amp;quot;root&amp;quot;, db: &amp;quot;admin&amp;quot; } ]
...   }
... )
Successfully added user: {
        &amp;quot;user&amp;quot; : &amp;quot;admin&amp;quot;,
        &amp;quot;roles&amp;quot; : [
                {
                        &amp;quot;role&amp;quot; : &amp;quot;root&amp;quot;,
                        &amp;quot;db&amp;quot; : &amp;quot;admin&amp;quot;
                }
        ]
}
&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Aunque el &lt;em&gt;role root&lt;/em&gt; permite actuar en todas las bases de datos, hay que indicarle una cualquiera para que la especificación JSON sea correcta.&lt;/p&gt;
&lt;h3&gt;Activar la autenticación&lt;/h3&gt;
&lt;p&gt;Lo único necesario para que &lt;strong&gt;MongoDB&lt;/strong&gt; requiera autenticación es un parámetro de configuración, sea:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;El &lt;em&gt;flag&lt;/em&gt; &lt;code&gt;--auth&lt;/code&gt; cuando levantamos el servidor&lt;/li&gt;
&lt;li&gt;El parámetro de configuración &lt;code&gt;security.authorization: enabled&lt;/code&gt; en el fichero de configuración&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hay que reiniciar el proceso para que use este nuevo parámetro, tanto para activarlo como para desactivarlo. A partir de este punto, todas las operaciones a la base de datos, van a necesitar que la sesión esté autenticada, y que dicho usuario tenga el &lt;em&gt;role&lt;/em&gt; necesario para hacer lo que pide.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Hay una &lt;a href="https://docs.mongodb.com/manual/core/security-users/#localhost-exception"&gt;excepción de localhost&lt;/a&gt;; si no hay ningún usuario en la base de datos y nos conectamos a ella desde &lt;em&gt;localhost&lt;/em&gt;, no va a ser necesaria ninguna autenticación. Usando esta excepción podemos levantar el servicio siempre con &lt;code&gt;--auth&lt;/code&gt; o &lt;code&gt;security.authorization&lt;/code&gt; y crear el superusuario &lt;em&gt;a posteriori&lt;/em&gt;, sin reiniciar nada.&lt;/p&gt;
&lt;h2&gt;Uso&lt;/h2&gt;
&lt;p&gt;Ha llegado el momento de poner una aplicación nueva que use nuestro servicio de &lt;strong&gt;MongoDB&lt;/strong&gt;. Esa aplicación necesita su propio espacio de datos, lo que significa que necesita:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Una base de datos propia, que &lt;strong&gt;MongoDB&lt;/strong&gt; creará automáticamente cuando haga falta&lt;/li&gt;
&lt;li&gt;Un usuario con acceso a esa base de datos (y a ninguna más)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Añadir un usuario nuevo&lt;/h3&gt;
&lt;p&gt;Otra vez nos limitamos a crear un usuario, pero esta vez le vamos a dar el &lt;em&gt;role readWrite&lt;/em&gt; sobre su base de datos. Abrimos una sesión en el &lt;em&gt;mongo shell&lt;/em&gt;, autenticándonos en la base de datos &lt;em&gt;admin&lt;/em&gt; con un usuario con capacidad de crear usuarios, por ejemplo el que creamos antes: &lt;strong&gt;admin&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; use admin
switched to db admin
&amp;gt; db.auth(&amp;quot;admin&amp;quot;, &amp;quot;s3cr3t&amp;quot;)
1
&amp;gt; db.createUser(
...   {
...     user: &amp;quot;myapp&amp;quot;,
...     pwd: &amp;quot;myapp1234&amp;quot;,
...     roles: [ { role: &amp;quot;readWrite&amp;quot;, db: &amp;quot;myapp&amp;quot; } ]
...   }
... )
Successfully added user: {
        &amp;quot;user&amp;quot; : &amp;quot;myapp&amp;quot;,
        &amp;quot;roles&amp;quot; : [
                {
                        &amp;quot;role&amp;quot; : &amp;quot;readWrite&amp;quot;,
                        &amp;quot;db&amp;quot; : &amp;quot;myapp&amp;quot;
                }
        ]
}
&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto es suficiente, puesto que la base de datos se crea automáticamente cuando tenga alguna colección. En este caso, el usuario y la base de datos coinciden; esto es otra decisión de diseño.&lt;/p&gt;
&lt;h3&gt;Configuración de la aplicación&lt;/h3&gt;
&lt;p&gt;Cada cliente de &lt;strong&gt;MongoDB&lt;/strong&gt; tiene sus propias formas para autenticarse; podemos ver el uso de la función &lt;code&gt;db.auth()&lt;/code&gt; por parte del &lt;em&gt;mongo shell&lt;/em&gt;, en el ejemplo anterior. La documentación es extensa en este punto.&lt;/p&gt;
&lt;p&gt;Lo importante es que la autenticación se hace con el usuario y contraseña proporcionados, &lt;strong&gt;en la base de datos &lt;em&gt;admin&lt;/em&gt;&lt;/strong&gt;. El uso regular de la base de datos se hace &lt;strong&gt;en la base de datos de la aplicación&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Todas las librerías tienen una forma común muy cómoda para indicar la base de datos destino: la &lt;a href="https://docs.mongodb.com/manual/reference/connection-string/"&gt;URL de mongodb&lt;/a&gt;. En ellas se nos permite especificar la base de datos de uso y la de autenticación (parámetro &lt;code&gt;authSource&lt;/code&gt;), así como el usuario y la contraseña.&lt;/p&gt;
&lt;p&gt;Para el ejemplo anterior, la URL quedaría así, suponiendo que el servicio está en el &lt;em&gt;host&lt;/em&gt; &lt;strong&gt;mongo&lt;/strong&gt;: &lt;code&gt;mongodb://myapp:myapp1234@mongo:27017/myapp?authSource=admin&lt;/code&gt;&lt;/p&gt;</content><category term="mongodb"></category><category term="autenticación"></category></entry><entry><title>Los sidekick containers en Docker</title><link href="https://www.linuxsysadmin.ml/2018/03/los-sidekick-containers-en-docker.html" rel="alternate"></link><published>2018-03-19T10:00:00+01:00</published><updated>2018-03-19T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-03-19:/2018/03/los-sidekick-containers-en-docker.html</id><summary type="html">&lt;p&gt;Algunas veces nos hemos encontrado que necesitamos ejecutar dos procesos o más para un servicio, aunque uno de ellos es el servicio principal y el otro se limita a ayudar al otro de alguna manera. Mejor que ponerlos en el mismo contenedor, podemos limitarnos a usar el patrón &lt;em&gt;sidekick containers …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Algunas veces nos hemos encontrado que necesitamos ejecutar dos procesos o más para un servicio, aunque uno de ellos es el servicio principal y el otro se limita a ayudar al otro de alguna manera. Mejor que ponerlos en el mismo contenedor, podemos limitarnos a usar el patrón &lt;em&gt;sidekick containers&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;En vez de utilizar un gestor de procesos para levantar ambas funciones, lo que añade complejidad a la imagen global, podemos utilizar dos contenedores que se comuniquen mediante volúmenes.&lt;/p&gt;
&lt;p&gt;En estos casos, debemos ver ambos contenedores como un &lt;em&gt;pack&lt;/em&gt; indivisible, que deben ser desplegados en el mismo servidor, y se escalan a la vez. Para asegurar ese despliegue condicionado, cada orquestador tiene su método, como por ejemplo, los &lt;strong&gt;pods&lt;/strong&gt; de &lt;strong&gt;kubernetes&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Llamaremos al contenedor inicial como &lt;em&gt;principal&lt;/em&gt; y el resto, de apoyo, son los &lt;em&gt;sidekick&lt;/em&gt; containers. Las funciones de estos últimos son muy variopintas; por nombrar algunas, podemos decir:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Envío de logs a un servidor centralizado&lt;/li&gt;
&lt;li&gt;Hacer backups de la base de datos del contenedor principal&lt;/li&gt;
&lt;li&gt;Actualizar contenido para otro servicio de forma automática&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Con un poco de imaginación se os ocurrirán muchas más.&lt;/p&gt;
&lt;h2&gt;Un ejemplo: un servidor web con contenido en git&lt;/h2&gt;
&lt;p&gt;Tenemos un servidor web que sirve un contenido concreto. Tal como escalamos, necesitamos más copias del mismo y hay que sincronizarlo. Podríamos poner el contenido en la imagen, pero al cambiar este, habría que hacer un redespliegue masivo&lt;/p&gt;
&lt;p&gt;Entonces podemos utilizar el patrón &lt;em&gt;sidekick&lt;/em&gt; containers para hacer lo siguiente:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Un contenedor principal con el servidor web que elijamos, sirviendo desde un volumen.&lt;/li&gt;
&lt;li&gt;Un contenedor &lt;em&gt;sidekick&lt;/em&gt; que exporta el volumen del contenedor principal y lo va actualizando.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;De esta forma, no importa cuantas replicas tengamos de este par, ni tampoco si cambia el contenido de la fuente; cada &lt;em&gt;sidekick&lt;/em&gt; se dedicará mantener actualizado el contenido de su contenedor principal, por supesto sin intervención manual.&lt;/p&gt;
&lt;p&gt;Hay muchas formas de sincronizar el contenido desde una fuente, pero por decisión de diseño, vamos a suponer que tenemos la fuente en un repositori &lt;strong&gt;git&lt;/strong&gt;, que gestiona las copias incrementales y nos aligera la transferencia.&lt;/p&gt;
&lt;h3&gt;El servidor web&lt;/h3&gt;
&lt;p&gt;Se trata de un servidor web normal y corriente, sirviendo un &lt;em&gt;document root&lt;/em&gt; cualquiera. Para agilizar, voy a utilizar una imagen prefabricada de &lt;strong&gt;nginx&lt;/strong&gt;, con una configuración para servir &lt;em&gt;/srv/www/&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/sidekick$ cat web/Dockerfile
FROM sirrtea/nginx:alpine
COPY web.conf /etc/nginx/conf.d/
gerard@atlantis:~/projects/sidekick$ cat web/web.conf
server &lt;span class="o"&gt;{&lt;/span&gt;
    server_name _&lt;span class="p"&gt;;&lt;/span&gt;
    listen &lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    root /srv/www&lt;span class="p"&gt;;&lt;/span&gt;
    index index.html&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@atlantis:~/projects/sidekick$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a construir la imagen con los comandos habituales, y le vamos a poner el &lt;em&gt;tag&lt;/em&gt; &lt;strong&gt;web&lt;/strong&gt;, para usar en el resto del artículo.&lt;/p&gt;
&lt;h3&gt;El clonador de git&lt;/h3&gt;
&lt;p&gt;Básicamente se trata de un contenedor que ejecute periodicamente un &lt;code&gt;git pull&lt;/code&gt;, o un &lt;code&gt;git clone&lt;/code&gt; si la carpeta estaba vacía. Para poder reutilizar la imagen, voy a parametrizar el repositorio a usar, la carpeta en donde clonarlo y el tiempo de espera entre actualizaciones.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/sidekick$ cat updater/Dockerfile
FROM alpine:3.7
RUN apk add --no-cache git
COPY run.sh /
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/run.sh&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@atlantis:~/projects/sidekick$ cat updater/run.sh
&lt;span class="c1"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;DESTINATION&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; true&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -e .git &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
        git pull
    &lt;span class="k"&gt;else&lt;/span&gt;
        git clone &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;REPOSITORY&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; .
    &lt;span class="k"&gt;fi&lt;/span&gt;
    sleep &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;DELAY&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
gerard@atlantis:~/projects/sidekick$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Construiremos la imagen como siempre y vamos a ponerle el &lt;em&gt;tag&lt;/em&gt; &lt;strong&gt;updater&lt;/strong&gt;, para referencia del resto del artículo.&lt;/p&gt;
&lt;h3&gt;Juntando los contenedores&lt;/h3&gt;
&lt;p&gt;La idea es que el contenedor tipo &lt;strong&gt;web&lt;/strong&gt; sirva la carpeta &lt;em&gt;/srv/www/&lt;/em&gt;, que es un volumen. El contenedor tipo &lt;strong&gt;updater&lt;/strong&gt; va a exportar el volumen, y va a actualizar la carpeta del mismo, para que las peticiones al contenedor &lt;strong&gt;web&lt;/strong&gt; se encuentren con el contenido actalizado periodicamente.&lt;/p&gt;
&lt;p&gt;Vamos a utilizar &lt;strong&gt;docker-compose&lt;/strong&gt; para agilizar el levantamiento de ambos contenedores:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/sidekick$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;
services:
  web:
    image: web
    volumes:
      - /srv/www
    ports:
      - &lt;span class="s2"&gt;&amp;quot;8080:80&amp;quot;&lt;/span&gt;
  web_sidekick:
    image: updater
    environment:
      DESTINATION: /srv/www
      REPOSITORY: https://github.com/Sirtea/sidekick-example.git
      DELAY: &lt;span class="m"&gt;60&lt;/span&gt;
    volumes_from:
      - web
gerard@atlantis:~/projects/sidekick$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En este caso configuramos el contenedor &lt;strong&gt;web_sidekick&lt;/strong&gt; para clonar el repositorio &lt;code&gt;sidekick-example.git&lt;/code&gt;, en la carpeta &lt;code&gt;/srv/www&lt;/code&gt; y actualizarlo cada minuto.&lt;/p&gt;
&lt;p&gt;El truco reside en los volúmenes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;web&lt;/strong&gt; sirve la carpeta &lt;code&gt;/srv/www&lt;/code&gt;, que es un volumen&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;web_sidekick&lt;/strong&gt; hace dos cosas:&lt;ul&gt;
&lt;li&gt;Exporta los volúmenes de &lt;strong&gt;web&lt;/strong&gt;, de forma que la carpeta &lt;code&gt;/srv/www&lt;/code&gt; es la misma que sirve &lt;strong&gt;web&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Actualiza el contenido de la carpeta &lt;code&gt;/srv/www&lt;/code&gt; con lo que tengamos en el repositorio de &lt;strong&gt;git&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Y de esta forma, el contenedor &lt;strong&gt;web&lt;/strong&gt; sirve un contenido que va a ir cambiando tal como el desarrollador haga los correspondientes &lt;em&gt;commits&lt;/em&gt; en el repositorio.&lt;/p&gt;
&lt;h2&gt;Escalando el servicio&lt;/h2&gt;
&lt;p&gt;Si queremos poner más servidores web, la ecuación es simple: un contenedor &lt;em&gt;sidekick&lt;/em&gt; por cada contenedor principal.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cada servidor web tiene un volumen&lt;/li&gt;
&lt;li&gt;Hace falta un contenedor &lt;em&gt;sidekick&lt;/em&gt; para actualizar un volumen&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Eso convierte el contenedor principal y el contenedor &lt;em&gt;sidekick&lt;/em&gt; en un par indivisible, que actuan juntos en una relacion de simbiosis. En caso de que lo hagamos mal y no haya contenedor &lt;em&gt;sidekick&lt;/em&gt; para algún contenedor web, su contenido no se actualizaría y obtendríamos un error 404 en las páginas de ese servidor.&lt;/p&gt;</content><category term="docker"></category><category term="sidekick"></category></entry><entry><title>Un gateway con Debian, iptables y dnsmasq</title><link href="https://www.linuxsysadmin.ml/2018/03/un-gateway-con-debian-iptables-y-dnsmasq.html" rel="alternate"></link><published>2018-03-12T10:00:00+01:00</published><updated>2018-03-12T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-03-12:/2018/03/un-gateway-con-debian-iptables-y-dnsmasq.html</id><summary type="html">&lt;p&gt;En algunas ocasiones no nos basta con tener un servidor único. Queremos tener un conjunto de servidores que se comuniquen abiertamente entre ellos usando una red privada, pero solo queremos exponer al mundo una sola dirección IP. El resto de servidores necesitan acceso a internet a través de un representante …&lt;/p&gt;</summary><content type="html">&lt;p&gt;En algunas ocasiones no nos basta con tener un servidor único. Queremos tener un conjunto de servidores que se comuniquen abiertamente entre ellos usando una red privada, pero solo queremos exponer al mundo una sola dirección IP. El resto de servidores necesitan acceso a internet a través de un representante.&lt;/p&gt;
&lt;p&gt;Este representante, llamando &lt;em&gt;gateway&lt;/em&gt;, es un servidor con una interfaz en la red privada y una en la red pública, de forma que los paquetes de red puedan fluir desde cualquier servidor hasta internet a través de este representante.&lt;/p&gt;
&lt;p&gt;De forma obligatoria, un &lt;em&gt;gateway&lt;/em&gt; solo necesita hacer dos cosas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Permitir el reenvío (o &lt;em&gt;forward&lt;/em&gt;) de paquetes a través del servidor &lt;em&gt;gateway&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Enmascarar la dirección de origen mediante NAT para que los paquetes sepan como volver desde internet.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Idealmente, suelen hacer otras funciones y ofrecer servicios a sus servidores protegidos, como por ejemplo:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Funciones de &lt;em&gt;firewall&lt;/em&gt; para proteger de accesos no autorizados contra la red protegida.&lt;/li&gt;
&lt;li&gt;Servidor DHCP para asignar direcciones IP a los servidores de la red interna.&lt;/li&gt;
&lt;li&gt;Servidor DNS para que los servidores de la red interna se conozcan entre ellos.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Las 3 primeras funciones quedan fácilmente cubiertas en el mismo &lt;em&gt;kernel&lt;/em&gt; de un sistema &lt;em&gt;linux&lt;/em&gt;; los servidores DHCP y DNS se pueden añadir fácilmente con un servicio adicional llamado &lt;strong&gt;dnsmasq&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;El servidor base&lt;/h2&gt;
&lt;p&gt;Partimos de un servidor &lt;strong&gt;Debian stretch&lt;/strong&gt; con dos direcciones asignadas, una en cada red.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;enp0s3&lt;/strong&gt;: La interfaz que da al exterior, configurada como nos sea conveniente.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;enp0s8&lt;/strong&gt;: Esta es la interfaz que da a la red interna, con una dirección IP estática (en este caso 10.0.0.1/24).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: Vamos a retrasar la aplicación de todas las configuraciones hasta el final, momento en el que reiniciaremos el servidor.&lt;/p&gt;
&lt;h2&gt;Las funciones básicas: forward, masquerade y firewall&lt;/h2&gt;
&lt;p&gt;El primer paso es permitir el paso de paquetes de red a través de nuestro &lt;em&gt;gateway&lt;/em&gt;. Ello se consigue configurando los parámetros del sistema.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gateway:~# tail -1 /etc/sysctl.conf
net.ipv4.ip_forward &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
root@gateway:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El resto se consigue mediante el módulo del &lt;em&gt;kernel&lt;/em&gt; llamado &lt;em&gt;netfilter&lt;/em&gt;. La herramienta que modifican &lt;em&gt;netfilter&lt;/em&gt; se llama &lt;strong&gt;iptables&lt;/strong&gt;, y por comodidad, vamos a utilizar el paquete &lt;strong&gt;iptables-persistent&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gateway:~# apt-get install iptables-persistent
...
root@gateway:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora nos falta crear el fichero de reglas que será cargado en cada reinicio:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gateway:~# cat /etc/iptables/rules.v4
*nat
-A POSTROUTING -o enp0s3 -j MASQUERADE
COMMIT

*filter
-A INPUT -i lo -j ACCEPT
-A INPUT -i enp0s3 -p tcp -m tcp --dport &lt;span class="m"&gt;22&lt;/span&gt; -j ACCEPT
-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A INPUT -i enp0s3 -j DROP
COMMIT
root@gateway:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La regla de la tabla &lt;em&gt;nat&lt;/em&gt; es la que enmascara la dirección origen y pone la de salida del &lt;em&gt;gateway&lt;/em&gt;, para que los paquetes sepan volver.&lt;/p&gt;
&lt;p&gt;El resto de reglas en la tabla de &lt;em&gt;filter&lt;/em&gt; hacen la función de un &lt;em&gt;firewall&lt;/em&gt; básico, que acepta conexiones que vengan del mismo servidor, las conexiones SSH, las conexiones que ya estén establecidas o estén pasando a través y descarta todo el resto.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: Ahora solo haría falta reiniciar el servidor, pero lo retrasamos para instalar el resto de servicios.&lt;/p&gt;
&lt;h2&gt;Algunos servicios útiles: DHCP y DNS&lt;/h2&gt;
&lt;p&gt;Todos los servicios que queramos ofrecer en la red interna se hacen igual: instalar el paquete que lo proporciona y configurarlo. En este caso, el paquete &lt;strong&gt;dnsmasq&lt;/strong&gt; ofrece ambos servicios.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gateway:~# apt-get install dnsmasq
...
root@gateway:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La configuración del servicio se hace en el fichero &lt;em&gt;/etc/dnsmasq.conf&lt;/em&gt;. Sin embargo es un fichero muy largo; vamos a beneficiarnos de la carpeta &lt;em&gt;/etc/dnsmasq.d/&lt;/em&gt; para añadir nuestras configuraciones. Lo hago en dos ficheros, de forma que tengamos la configuración organizada en dos ficheros, en función de la frecuencia de modificación.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gateway:~# cat /etc/dnsmasq.d/00-base
&lt;span class="nv"&gt;interface&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;enp0s8
dhcp-range&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;.0.0.200,10.0.0.250,1h
root@gateway:~# cat /etc/dnsmasq.d/01-hosts
&lt;span class="c1"&gt;#dhcp-host=server,10.0.0.2,1h&lt;/span&gt;
root@gateway:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;De esta forma, cuando queramos añadir un nuevo servidor en la red interna con dirección fija, podemos configurarlo para usar DHCP, poner una nueva línea en el fichero &lt;em&gt;/etc/dnsmasq.d/01-hosts&lt;/em&gt; similar a la de comentada de arriba, y reinicar el servicio &lt;strong&gt;dnsmasq&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Como &lt;em&gt;bonus&lt;/em&gt;, el servidor &lt;em&gt;gateway&lt;/em&gt; va a poder resolver por DNS el nombre declarado por el servidor protegido, al que ya dio dirección por DHCP. Para que los servidores de la red interna puedan resolver por nombre la máquina &lt;em&gt;gateway&lt;/em&gt;, solo necesitamos añadir esa relación en el fichero &lt;em&gt;/etc/hosts&lt;/em&gt; del &lt;em&gt;gateway&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gateway:~# grep gateway /etc/hosts
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.1        gateway
root@gateway:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Es importante que nuestro servidor &lt;em&gt;gateway&lt;/em&gt; utilice &lt;em&gt;localhost&lt;/em&gt; como primer DNS, de forma que se beneficie de las funciones &lt;em&gt;DNS cache&lt;/em&gt; y pueda resolver los servidores de la red interna por su nombre. Esto se hace poniendo &lt;em&gt;127.0.0.1&lt;/em&gt; en el fichero &lt;em&gt;/etc/resolv.conf&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Como configuré la interfaz principal por DHCP y el fichero &lt;em&gt;/etc/resolv.conf&lt;/em&gt; se sobreescribe de forma automática, podemos instruir el cliente DHCP para que lo incluya antes de los que reciba por DHCP.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gateway:~# tail -1 /etc/dhcp/dhclient.conf
prepend domain-name-servers &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1&lt;span class="p"&gt;;&lt;/span&gt;
root@gateway:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Aplicar los cambios&lt;/h2&gt;
&lt;p&gt;En este punto podemos reiniciar el servidor, para que apliquen todos los cambios.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gateway:~# reboot
...
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Añadir un servidor en la red interna&lt;/h2&gt;
&lt;p&gt;Un servidor que se quiera unir a la red puede hacerlo con una dirección estática, pero es mas fácil hacerlo por DHCP; basta con tenerlo configurado para usar DHCP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Si clonamos una máquina virtual, es necesario asegurarse que la dirección MAC de la interfaz cambia, ya que la relación entre el servidor y su IP se guarda usando la dirección MAC, y por lo tanto, no deben haber dos iguales.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Podemos eliminar la relación &lt;em&gt;localhost&lt;/em&gt; del servidor con su nombre en &lt;em&gt;/etc/hosts&lt;/em&gt;; &lt;strong&gt;dnsmasq&lt;/strong&gt; resuelve eso por nosotros y para otros servidores de la red.&lt;/p&gt;
&lt;p&gt;Si queremos que tenga una dirección fija, se puede hacer fácilmente. Necesitamos configurar el servidor DHCP para que asigne una IP concreta basándonos en el nombre del servidor, y recargar este servicio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gateway:~# cat /etc/dnsmasq.d/01-hosts
dhcp-host&lt;span class="o"&gt;=&lt;/span&gt;database,10.0.0.5,1h
root@gateway:~# service dnsmasq restart
root@gateway:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo faltará darle un nombre al servidor candidato en en &lt;em&gt;/etc/hostname&lt;/em&gt; y reiniciarlo. Tanto la dirección IP, el &lt;em&gt;gateway&lt;/em&gt; por defecto y el servidor DNS se cargan en la resolución DHCP.&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="stretch"></category><category term="gateway"></category><category term="iptables"></category><category term="dnsmasq"></category></entry><entry><title>Automatizando el setup de un mongo replica set en docker</title><link href="https://www.linuxsysadmin.ml/2018/01/automatizando-el-setup-de-un-mongo-replica-set-en-docker.html" rel="alternate"></link><published>2018-01-29T10:00:00+01:00</published><updated>2018-01-29T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-01-29:/2018/01/automatizando-el-setup-de-un-mongo-replica-set-en-docker.html</id><summary type="html">&lt;p&gt;Algunas veces queremos probar nuestras aplicaciones en local y necesitamos una base de datos &lt;strong&gt;MongoDB&lt;/strong&gt;; en estos casos, &lt;strong&gt;Docker&lt;/strong&gt; nos presta un gran servicio. Es posible que en estos casos necesitemos un &lt;strong&gt;replica set&lt;/strong&gt; para probar; aunque &lt;strong&gt;Docker&lt;/strong&gt; sigue ayudando, la inicialización del &lt;em&gt;cluster&lt;/em&gt; sigue siendo un tedioso proceso manual …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Algunas veces queremos probar nuestras aplicaciones en local y necesitamos una base de datos &lt;strong&gt;MongoDB&lt;/strong&gt;; en estos casos, &lt;strong&gt;Docker&lt;/strong&gt; nos presta un gran servicio. Es posible que en estos casos necesitemos un &lt;strong&gt;replica set&lt;/strong&gt; para probar; aunque &lt;strong&gt;Docker&lt;/strong&gt; sigue ayudando, la inicialización del &lt;em&gt;cluster&lt;/em&gt; sigue siendo un tedioso proceso manual.&lt;/p&gt;
&lt;p&gt;Así que me puse a pensar... ¿Por qué no puedo atar el &lt;strong&gt;replica set&lt;/strong&gt; directamente en el proceso de &lt;em&gt;runtime&lt;/em&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt; ejecuta un solo proceso (que debe ser &lt;em&gt;mongod&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Necesitamos un proceso auxiliar para atar el &lt;em&gt;cluster&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;No queremos más contenedores que los que hacen falta&lt;/li&gt;
&lt;li&gt;No quiero imágenes específicas de un "&lt;em&gt;mongodb leader&lt;/em&gt;"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tras muchas vueltas a la cabeza, llegué a un conclusión interesante:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Podemos crear el comportamiento líder mediante variables de entorno&lt;/li&gt;
&lt;li&gt;Este líder puede lanzar un proceso en &lt;em&gt;background&lt;/em&gt; que se dedique a atar el &lt;em&gt;cluster&lt;/em&gt; y luego muera&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Para conseguir este doble proceso condicionado, nos vemos obligados a cambiar el comando &lt;em&gt;mongod&lt;/em&gt; por un &lt;em&gt;script&lt;/em&gt; que ejecute el &lt;em&gt;setup&lt;/em&gt; del &lt;em&gt;cluster&lt;/em&gt; (si procede) y el proceso &lt;em&gt;mongod&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;La imagen única&lt;/h2&gt;
&lt;p&gt;Partimos de un &lt;em&gt;Dockerfile&lt;/em&gt; bastante estándar; las únicas excepciones van a ser el &lt;em&gt;script&lt;/em&gt; inicial y otro &lt;em&gt;script&lt;/em&gt; auxiliar que configure nuestro &lt;em&gt;cluster&lt;/em&gt; en una sola de las máquinas.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/replica/mongo$ cat Dockerfile 
FROM debian:jessie-slim
RUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6 &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;deb http://repo.mongodb.org/apt/debian jessie/mongodb-org/3.4 main&amp;quot;&lt;/span&gt; &amp;gt; /etc/apt/sources.list.d/mongodb-org-3.4.list
RUN apt-get update &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    apt-get install -y mongodb-org-server mongodb-org-shell &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm -rf /var/lib/apt/lists/*
COPY mongod.conf /etc/
COPY start.sh setup_cluster.sh /
USER mongodb
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/start.sh&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@aldebaran:~/docker/replica/mongo$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La configuración de &lt;strong&gt;mongodb&lt;/strong&gt; es bastante estándar y solo se incluye por completitud:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/replica/mongo$ cat mongod.conf 
processManagement:
  fork: &lt;span class="nb"&gt;false&lt;/span&gt;

net:
  bindIp: &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0
  port: &lt;span class="m"&gt;27017&lt;/span&gt;
  unixDomainSocket:
    enabled: &lt;span class="nb"&gt;false&lt;/span&gt;

storage:
  dbPath: /var/lib/mongodb
  journal:
    enabled: &lt;span class="nb"&gt;true&lt;/span&gt;

replication:
   replSetName: rs
gerard@aldebaran:~/docker/replica/mongo$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y este es el truco: vamos a definir dos variables de entorno, llamadas &lt;strong&gt;ROLE&lt;/strong&gt; y &lt;strong&gt;REPLICAS&lt;/strong&gt;. La idea es que el que tenga que configurar el &lt;em&gt;cluster&lt;/em&gt; va a tener en &lt;strong&gt;ROLE&lt;/strong&gt; algún valor que le dé a entender que es el elegido, y la variable &lt;strong&gt;REPLICAS&lt;/strong&gt; que le indica los otros miembros a añadir en el &lt;em&gt;cluster&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;De hecho, las variables no necesitan estar en las réplicas, porque &lt;strong&gt;bash&lt;/strong&gt; va a interpretar la variable &lt;strong&gt;ROLE&lt;/strong&gt; inexistente como vacía y no va a lanzar el proceso de configuración, haciendo de la variable &lt;strong&gt;REPLICAS&lt;/strong&gt; algo innecesario.&lt;/p&gt;
&lt;p&gt;El contenedor con el rol &lt;em&gt;orchestrator&lt;/em&gt; va a levantar otro &lt;em&gt;script&lt;/em&gt; en &lt;em&gt;background&lt;/em&gt;, dejando así el flujo de ejecución para el proceso &lt;em&gt;mongod&lt;/em&gt;. Este &lt;em&gt;script&lt;/em&gt; tiene permisos de ejecución.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/replica/mongo$ cat start.sh 
&lt;span class="c1"&gt;#!/bin/bash&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$ROLE&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;orchestrator&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
    /setup_cluster.sh &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="nb"&gt;exec&lt;/span&gt; mongod --config /etc/mongod.conf
gerard@aldebaran:~/docker/replica/mongo$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El &lt;em&gt;script setup_cluster.sh&lt;/em&gt; (también ejecutable) es complejo, pero no complicado:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Espera a que todos los procesos &lt;em&gt;mongod&lt;/em&gt; respondan&lt;/li&gt;
&lt;li&gt;Conecta a su mismo contenedor para lanzar el &lt;code&gt;rs.initiate()&lt;/code&gt; y el &lt;code&gt;rs.add(...)&lt;/code&gt; de las &lt;em&gt;replicas&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Espera que todas las &lt;em&gt;replicas&lt;/em&gt; estén como secundarias, mediante &lt;code&gt;rs.status()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;y así queda esta primera versión:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/replica/mongo$ cat setup_cluster.sh 
&lt;span class="c1"&gt;#!/bin/bash&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; REPLICA in &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;REPLICAS&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="k"&gt;until&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; mongo --host &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;REPLICA&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &amp;gt;/dev/null &lt;span class="m"&gt;2&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt; sleep &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;done&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;span class="k"&gt;until&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; mongo &amp;gt;/dev/null &lt;span class="m"&gt;2&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt; sleep &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;done&lt;/span&gt;

&lt;span class="nv"&gt;replicas&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rs.initiate()&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; mongo &amp;gt;/dev/null &lt;span class="m"&gt;2&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; REPLICA in &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;REPLICAS&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rs.add(\&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;REPLICA&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;:27017\&amp;quot;)&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; mongo &amp;gt;/dev/null &lt;span class="m"&gt;2&lt;/span&gt;&amp;gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
    &lt;span class="nb"&gt;let&lt;/span&gt; &lt;span class="nv"&gt;replicas&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;replicas+1
&lt;span class="k"&gt;done&lt;/span&gt;

&lt;span class="nv"&gt;online&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;until&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;online&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; -eq &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;replicas&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    sleep &lt;span class="m"&gt;1&lt;/span&gt;
    &lt;span class="nv"&gt;online&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rs.status()&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; mongo &lt;span class="m"&gt;2&lt;/span&gt;&amp;gt;/dev/null &lt;span class="p"&gt;|&lt;/span&gt; grep -c &lt;span class="s2"&gt;&amp;quot;SECONDARY&amp;quot;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
gerard@aldebaran:~/docker/replica/mongo$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Testeando nuestro deploy&lt;/h2&gt;
&lt;p&gt;Levantar 3 contenedores es muy pesado y nada apetecible, así que haremos con &lt;strong&gt;Docker Compose&lt;/strong&gt;. Veamos un &lt;em&gt;docker-compose.yml&lt;/em&gt; de ejemplo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/replica$ cat docker-compose.yml 
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  rs01:
    image: mongo
    hostname: rs01
    container_name: rs01
    environment:
        ROLE: orchestrator
        REPLICAS: &lt;span class="s2"&gt;&amp;quot;rs02 rs03&amp;quot;&lt;/span&gt;
  rs02:
    image: mongo
    hostname: rs02
    container_name: rs02
  rs03:
    image: mongo
    hostname: rs03
    container_name: rs03
gerard@aldebaran:~/docker/replica$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;De esta forma, y de acuerdo a las decisiones de diseño, el contenedor &lt;em&gt;rs01&lt;/em&gt; inicializaría el &lt;em&gt;cluster&lt;/em&gt; consigo mismo y añadiría &lt;em&gt;rs02&lt;/em&gt; y &lt;em&gt;rs03&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/replica$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;replica_default&amp;quot;&lt;/span&gt; with the default driver
Creating rs01
Creating rs02
Creating rs03
gerard@aldebaran:~/docker/replica$ docker-compose ps
Name    Command    State   Ports 
--------------------------------
rs01   /start.sh   Up            
rs02   /start.sh   Up            
rs03   /start.sh   Up            
gerard@aldebaran:~/docker/replica$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo nos quedaría entrar en un &lt;em&gt;mongod&lt;/em&gt; cualquiera y pedirle el estado del &lt;em&gt;cluster&lt;/em&gt; con &lt;code&gt;rs.status()&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/replica$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; -ti rs02 mongo
MongoDB shell version v3.4.4
connecting to: mongodb://127.0.0.1:27017
MongoDB server version: &lt;span class="m"&gt;3&lt;/span&gt;.4.4
...  
rs:SECONDARY&amp;gt; rs.status&lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;set&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;rs&amp;quot;&lt;/span&gt;,
...  
    &lt;span class="s2"&gt;&amp;quot;members&amp;quot;&lt;/span&gt; : &lt;span class="o"&gt;[&lt;/span&gt;
        &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;0&lt;/span&gt;,
            &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;rs01:27017&amp;quot;&lt;/span&gt;,
...  
            &lt;span class="s2"&gt;&amp;quot;stateStr&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;PRIMARY&amp;quot;&lt;/span&gt;,
...  
        &lt;span class="o"&gt;}&lt;/span&gt;,
        &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;,
            &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;rs02:27017&amp;quot;&lt;/span&gt;,
...  
            &lt;span class="s2"&gt;&amp;quot;stateStr&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;SECONDARY&amp;quot;&lt;/span&gt;,
...  
            &lt;span class="s2"&gt;&amp;quot;self&amp;quot;&lt;/span&gt; : &lt;span class="nb"&gt;true&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;,
        &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;2&lt;/span&gt;,
            &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;rs03:27017&amp;quot;&lt;/span&gt;,
...  
            &lt;span class="s2"&gt;&amp;quot;stateStr&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;SECONDARY&amp;quot;&lt;/span&gt;,
...  
        &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;]&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
rs:SECONDARY&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y todo ha quedado como esperábamos, sin ninguna intervención manual por nuestra parte.&lt;/p&gt;</content><category term="mongodb"></category><category term="replica set"></category><category term="docker"></category><category term="docker-compose"></category></entry><entry><title>Una plataforma para desplegar contenedores: RancherOS</title><link href="https://www.linuxsysadmin.ml/2018/01/una-plataforma-para-desplegar-contenedores-rancheros.html" rel="alternate"></link><published>2018-01-15T10:00:00+01:00</published><updated>2018-01-15T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2018-01-15:/2018/01/una-plataforma-para-desplegar-contenedores-rancheros.html</id><summary type="html">&lt;p&gt;Aquellos que seguís este &lt;em&gt;blog&lt;/em&gt; de forma regular, habréis notado mi predilección por los contenedores &lt;strong&gt;docker&lt;/strong&gt;, en gran parte porque es con lo que trabajo en mi día a día. Hartos de usar la plataforma &lt;em&gt;custom&lt;/em&gt; que tenemos en la compañía buscamos una nueva, que simplifique el trabajo que hacemos …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Aquellos que seguís este &lt;em&gt;blog&lt;/em&gt; de forma regular, habréis notado mi predilección por los contenedores &lt;strong&gt;docker&lt;/strong&gt;, en gran parte porque es con lo que trabajo en mi día a día. Hartos de usar la plataforma &lt;em&gt;custom&lt;/em&gt; que tenemos en la compañía buscamos una nueva, que simplifique el trabajo que hacemos.&lt;/p&gt;
&lt;p&gt;Lo que tenemos actualmente es una amalgama de &lt;em&gt;hosts&lt;/em&gt; en donde desplegamos contenedores, en algunos servicios, y en otros los servicios que nos permiten hacer un &lt;em&gt;routing&lt;/em&gt; adecuado, por ejemplo, &lt;em&gt;proxies&lt;/em&gt; reversos, balanceadores y &lt;em&gt;firewalls&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Sin embargo, estos contenedores se ven entre sí, así como a los &lt;em&gt;hosts&lt;/em&gt; que los levantan. A todo esto, el conjunto de servicios desplegados empezaba a impactar en la capacidad de mantenerla. En especial, la demanda de segregación de red de algún cliente nos hizo buscar alternativas de forma acelerada.&lt;/p&gt;
&lt;p&gt;Aunque no es nuestro futuro inmediato, salimos del paso con una solución completa que se llama &lt;a href="http://rancher.com/"&gt;Rancher&lt;/a&gt;. Su filosofía es muy simple: necesitamos un servidor de control que tenga una foto del &lt;em&gt;cluster&lt;/em&gt; (por supuesto un contenedor &lt;strong&gt;docker&lt;/strong&gt;, y un conjunto de servidores que añadan recursos al mismo (mediante otro servidor &lt;strong&gt;docker&lt;/strong&gt; que corre un agente). De hecho, en el &lt;em&gt;deploy&lt;/em&gt; más simple, basta con tener ambas cosas en la misma máquina.&lt;/p&gt;
&lt;p&gt;Para despliegues mas profesionales, disponen de un fichero &lt;em&gt;.iso&lt;/em&gt; para instalar un sistema operativo para ejecutar solamente &lt;strong&gt;Rancher&lt;/strong&gt;, que tiene lo básico para ejecutar &lt;strong&gt;Docker&lt;/strong&gt; y levanta sus servicios como contenedores.&lt;/p&gt;
&lt;h2&gt;Instalación de Rancher&lt;/h2&gt;
&lt;h3&gt;Servidor&lt;/h3&gt;
&lt;p&gt;Para levantar el servidor de &lt;strong&gt;Rancher&lt;/strong&gt; vamos a seguir los pasos de &lt;a href="http://docs.rancher.com/rancher/latest/en/installing-rancher/installing-server/"&gt;la instalación&lt;/a&gt;. Tan simple como levantar un contenedor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~$ docker run -d --restart&lt;span class="o"&gt;=&lt;/span&gt;unless-stopped -p &lt;span class="m"&gt;8080&lt;/span&gt;:8080 rancher/server
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Este contenedor nos ofrece la base de datos de control y una bonita interfaz de usuario en el puerto 8080 para administrar nuestro &lt;em&gt;cluster&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Solo nos queda ver la interfaz de administración para comprobar que funciona, en &lt;a href="http://localhost:8080/"&gt;http://localhost:8080/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Rancher panel" src="https://www.linuxsysadmin.ml/images/rancher-panel.png"&gt;&lt;/p&gt;
&lt;h3&gt;Hosts&lt;/h3&gt;
&lt;p&gt;En la interfaz de administración vemos una pestaña "Infraestructure" en donde podemos seleccionar "Hosts". Si le damos al botón "Add Host", veremos que añadir un &lt;em&gt;host&lt;/em&gt; es trivial; a la larga todo se reduce en levantar otro contenedor &lt;em&gt;rancher/agent&lt;/em&gt; en ese &lt;em&gt;host&lt;/em&gt;, para que nuestro servidor lo reconozca y le pueda enviar órdenes.&lt;/p&gt;
&lt;h2&gt;Conceptos básicos de Rancher&lt;/h2&gt;
&lt;p&gt;Si jugamos un rato con la interfaz, vamos a ver varios conceptos, que aquí se resumen:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Entornos&lt;/strong&gt;: Agrupaciones lógicas de recursos (&lt;em&gt;hosts&lt;/em&gt;). Cada entorno tiene sus propios &lt;em&gt;hosts&lt;/em&gt;. Solo hay conectividad de contenedores cuando están en el mismo entorno, dentro de &lt;strong&gt;Rancher&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stacks&lt;/strong&gt;: Agrupaciones lógicas de servicios, se despliegan en un entorno concreto.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Servicios&lt;/strong&gt;: Un servicio es la unidad mínima escalable, y se compone de un &lt;em&gt;primary service&lt;/em&gt; (un contenedor) y de cualquier número de &lt;em&gt;sidekick containers&lt;/em&gt;. Esta es la unidad mínima escalable. Se garantiza que todos los contenedores de este servicio se van a desplegar en un solo &lt;em&gt;host&lt;/em&gt;, y en caso de escalarlo, este &lt;em&gt;pack&lt;/em&gt; se repite en otros &lt;em&gt;hosts&lt;/em&gt; de acuerdo con las leyes de afinidad. Solo se pueden usar volúmenes de un contenedor en el mismo servicio.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Balanceadores&lt;/strong&gt;: Básicamente se trata de contenedores &lt;em&gt;haproxy&lt;/em&gt; que nos permiten poner un balanceador sin tener que crear nosotros un servicio especializado.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Catálogo&lt;/strong&gt;: Es un conjunto de plantillas de servicios prefabricados que podemos usar para crear nuestras propias &lt;em&gt;stacks&lt;/em&gt;. Podemos añadir nuestra &lt;em&gt;stack&lt;/em&gt; para el fácil despliegue de &lt;em&gt;stacks&lt;/em&gt; genéricas, por ejemplo bases de datos o herramientas de monitorización.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;La parte importante es que podemos declarar nuestros contenedores dentro de los servicios de la misma forma que lo hacemos en &lt;strong&gt;docker&lt;/strong&gt;, pero con un bonito formulario web.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Los servicios se pueden actualizar usando nuevas versiones de nuestros contenedores. &lt;strong&gt;Rancher&lt;/strong&gt; va a crear un nuevo conjunto de servicios sin eliminar los antiguos. A &lt;em&gt;posteriori&lt;/em&gt; podemos hacer un "Finish upgrade", que eliminaría los viejos contenedores asumiendo que nos gusta el resultado, o podemos hacer un "Rollback", que eliminaría los contenedores nuevos para dejar los viejos, en caso de que la nueva versión no nos satisfaga.&lt;/p&gt;
&lt;h2&gt;Un caso práctico&lt;/h2&gt;
&lt;p&gt;Supongamos que tenemos una web con datos financieros, con acceso por parte de muchos visitantes y con un entrada de nuevos datos a la base de datos mediante carga de ficheros por SFTP.&lt;/p&gt;
&lt;p&gt;Vamos a suponer que trabajamos en un entorno de test, específico para este proyecto por aislamiento de red; eso nos simplifica las decisiones. En un futuro se podría crear otro entorno para producción o para otro proyecto, pero de momento nos vale. También es posible hacer entornos compartidos para varios proyectos.&lt;/p&gt;
&lt;p&gt;Algunas decisiones de diseño:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nuestra aplicación es un solo contenedor, que vamos a escalar para soportar la carga.&lt;/li&gt;
&lt;li&gt;Vamos a balancear la carga entre todos los contenedores de aplicación.&lt;/li&gt;
&lt;li&gt;La base de datos va a ir separada en otro contenedor, que todas las instancias de la aplicación puedan ver.&lt;/li&gt;
&lt;li&gt;El sistema de inyección de datos se va a componer de 3 contenedores: uno para el SFTP, uno para el volumen de datos y otro para el procesador de dichos datos.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Y con este &lt;em&gt;setup&lt;/em&gt;, podemos empezar a construir nuestro entorno. no importa cuantos &lt;em&gt;stacks&lt;/em&gt; hagamos, pero seguramente, la base de datos tendrá su propio &lt;em&gt;stack&lt;/em&gt; porque lo pondremos directamente del catálogo. Podemos poner todo el resto en un solo &lt;em&gt;stack&lt;/em&gt; o separarlos por subsistemas (web balanceada, bases de datos, inyector de datos).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;La parte mas fácil es la base de datos. Creamos un servicio simple para que todos lo utilicen.&lt;/li&gt;
&lt;li&gt;Los contenedores de aplicación van a ser otro servicio. Que sean servicios individuales nos permite escalarlos individualmente del resto de componentes.&lt;/li&gt;
&lt;li&gt;Creamos un balanceador para el servicio de aplicación. En este punto ya deberíamos poder acceder al entorno, aunque sin la entrada de datos.&lt;/li&gt;
&lt;li&gt;Los contenedores del sistema de inyección de datos deben formar parte de un único servicio, porque es la única forma de que puedan montar los volúmenes del contenedor de datos. Dejaremos este servicio escalado a 1, que nos va a poner un contenedor de cada en una sola de las máquinas. No es importante que contenedor es el primario, pero vamos a poner el SFTP por un sencillo motivo: solo los contenedores primarios pueden ser el objetivo del balanceador, que de momento no pondremos, pero tal vez algún día lo queramos.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Y con esto tendremos nuestro proyecto corriendo. Es el momento de guardar las &lt;em&gt;stacks&lt;/em&gt; en el catálogo, ya que eso va a simplificar el &lt;em&gt;deploy&lt;/em&gt; cuando creemos otro entorno.&lt;/p&gt;</content><category term="docker"></category><category term="cluster"></category><category term="rancher"></category></entry><entry><title>Redirecciones a nivel de nginx</title><link href="https://www.linuxsysadmin.ml/2017/11/redirecciones-a-nivel-de-nginx.html" rel="alternate"></link><published>2017-11-13T10:00:00+01:00</published><updated>2017-11-13T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-11-13:/2017/11/redirecciones-a-nivel-de-nginx.html</id><summary type="html">&lt;p&gt;No son pocas las veces que queremos hacer una redirección de algunos de nuestros dominios a otros. Puede que queramos añadir el clásico &lt;code&gt;www&lt;/code&gt; delante del dominio, o tal vez queramos forzar el uso de &lt;code&gt;https&lt;/code&gt;. Hacer copias de nuestro dominio no es viable, pero podemos usar redirecciones fijas 301 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;No son pocas las veces que queremos hacer una redirección de algunos de nuestros dominios a otros. Puede que queramos añadir el clásico &lt;code&gt;www&lt;/code&gt; delante del dominio, o tal vez queramos forzar el uso de &lt;code&gt;https&lt;/code&gt;. Hacer copias de nuestro dominio no es viable, pero podemos usar redirecciones fijas 301.&lt;/p&gt;
&lt;p&gt;Vamos a suponer en este artículo que tenemos un servidor &lt;strong&gt;nginx&lt;/strong&gt; configurado para servir &lt;em&gt;virtualhosts&lt;/em&gt;. En este ejemplo, vamos a poner uno por defecto y otro de ejemplo.&lt;/p&gt;
&lt;p&gt;Las reglas son simples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Serviremos la web de ejemplo para el dominio &lt;em&gt;www.example.com&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Queremos ver la misma web para el dominio &lt;em&gt;example.com&lt;/em&gt; y no queremos duplicar contenido&lt;/li&gt;
&lt;li&gt;Serviremos una web por defecto en cualquier otro caso, para ver si lo hemos hecho bien&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Preparación del ejemplo&lt;/h2&gt;
&lt;p&gt;Para que se pueda seguir el artículo, se proporciona el contexto necesario para construir una imagen &lt;strong&gt;docker&lt;/strong&gt; en donde probarlo. Empezamos por el &lt;em&gt;Dockerfile&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/nginx_redirects$ cat Dockerfile 
FROM alpine:3.5
RUN apk add --no-cache nginx &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    ln -s /dev/stdout /var/log/nginx/access.log &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    ln -s /dev/stderr /var/log/nginx/error.log &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    mkdir /run/nginx &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /etc/nginx/conf.d/default.conf
COPY nginx.conf /etc/nginx/
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/sbin/nginx&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-g&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;daemon off;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
COPY conf.d/* /etc/nginx/conf.d/
COPY www/ /srv/www/
gerard@aldebaran:~/docker/nginx_redirects$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y añadimos las configuraciones que &lt;strong&gt;nginx&lt;/strong&gt; necesita:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/nginx_redirects$ cat nginx.conf 
worker_processes &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
events &lt;span class="o"&gt;{&lt;/span&gt;
    worker_connections &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
http &lt;span class="o"&gt;{&lt;/span&gt;
    include mime.types&lt;span class="p"&gt;;&lt;/span&gt;
    default_type application/octet-stream&lt;span class="p"&gt;;&lt;/span&gt;
    sendfile on&lt;span class="p"&gt;;&lt;/span&gt;
    keepalive_timeout &lt;span class="m"&gt;65&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    include conf.d/*&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@aldebaran:~/docker/nginx_redirects$ cat conf.d/www 
server &lt;span class="o"&gt;{&lt;/span&gt;
    listen &lt;span class="m"&gt;80&lt;/span&gt; default&lt;span class="p"&gt;;&lt;/span&gt;
    server_name _&lt;span class="p"&gt;;&lt;/span&gt;
    root /srv/www/default&lt;span class="p"&gt;;&lt;/span&gt;
    index index.html&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

server &lt;span class="o"&gt;{&lt;/span&gt;
    listen &lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    server_name www.example.com&lt;span class="p"&gt;;&lt;/span&gt;
    root /srv/www/www.example.com&lt;span class="p"&gt;;&lt;/span&gt;
    index index.html&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@aldebaran:~/docker/nginx_redirects$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A modo de ejemplo, adjuntamos dos sitios de juguete que nos permitirnán probar nuestros &lt;em&gt;virtualhosts&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/nginx_redirects$ tree www/
www/
├── default
│   └── index.html
└── www.example.com
    └── index.html

&lt;span class="m"&gt;2&lt;/span&gt; directories, &lt;span class="m"&gt;2&lt;/span&gt; files
gerard@aldebaran:~/docker/nginx_redirects$ cat www/default/index.html 
default site
gerard@aldebaran:~/docker/nginx_redirects$ cat www/www.example.com/index.html 
example site
gerard@aldebaran:~/docker/nginx_redirects$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Construid la imagen y ponedla a correr, con los comandos habituales.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Como queremos probar &lt;em&gt;virtualhosts&lt;/em&gt; y no queremos marear con servidores DNS, podemos tirar del fichero &lt;em&gt;/etc/hosts&lt;/em&gt;, para que todos los dominios apunten a nuestro contenedor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~$ cat /etc/hosts
...  
&lt;span class="m"&gt;172&lt;/span&gt;.17.0.2  example.com www.example.com www.otherdomain.com
gerard@aldebaran:~$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Problema y solución&lt;/h2&gt;
&lt;p&gt;Lanzamos una batería de peticiones con &lt;strong&gt;curl&lt;/strong&gt; o alguna herramienta similar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~$ curl http://www.example.com/
example site
gerard@aldebaran:~$ curl http://www.otherdomain.com/
default site
gerard@aldebaran:~$ curl http://example.com/
default site
gerard@aldebaran:~$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Todos los dominios apuntan a la dirección IP del mismo contenedor, y de ello sacamos algunas conclusiones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;El dominio &lt;em&gt;www.example.com&lt;/em&gt; funciona según lo esperado&lt;/li&gt;
&lt;li&gt;El dominio por defecto salta ante las peticiones que no pertenecen al dominio &lt;em&gt;www.example.com&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Las peticiones al dominio &lt;em&gt;example.com&lt;/em&gt; caen en el &lt;em&gt;virtualhost&lt;/em&gt; por defecto&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Podríamos haber creado otro &lt;em&gt;virtualhost&lt;/em&gt; que sirviera una copia (o la misma carpeta) del dominio de ejemplo, pero no es elegante y en el caso de querer HTTPS no serviría. En estos casos, el estándar &lt;em&gt;de facto&lt;/em&gt; es hacer una redirección al dominio correcto, mediante un código HTTP 301. Esto le indicará al navegador que la redirección es permanente y hará que este "se acuerde" de la redirección.&lt;/p&gt;
&lt;p&gt;Para ello necesitamos un nuevo &lt;em&gt;virtualhost&lt;/em&gt;, o adaptar el que tenemos para que acepte expresiones regulares; no vamos a seguir este camino por simplicidad.&lt;/p&gt;
&lt;p&gt;Solo necesitaríamos utilizar el &lt;a href="http://nginx.org/en/docs/http/ngx_http_rewrite_module.html#return"&gt;rewrite module&lt;/a&gt; de &lt;strong&gt;nginx&lt;/strong&gt; para forzar un código de retorno a la URL correcta. Exponemos este ejemplo a continuación:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/nginx_redirects$ cat conf.d/www 
server &lt;span class="o"&gt;{&lt;/span&gt;
    listen &lt;span class="m"&gt;80&lt;/span&gt; default&lt;span class="p"&gt;;&lt;/span&gt;
    server_name _&lt;span class="p"&gt;;&lt;/span&gt;
    root /srv/www/default&lt;span class="p"&gt;;&lt;/span&gt;
    index index.html&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

server &lt;span class="o"&gt;{&lt;/span&gt;
    listen &lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    server_name www.example.com&lt;span class="p"&gt;;&lt;/span&gt;
    root /srv/www/www.example.com&lt;span class="p"&gt;;&lt;/span&gt;
    index index.html&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

server &lt;span class="o"&gt;{&lt;/span&gt;
    listen &lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    server_name example.com&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="m"&gt;301&lt;/span&gt; http://www.example.com&lt;span class="nv"&gt;$request_uri&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@aldebaran:~/docker/nginx_redirects$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Si recreamos la imagen y la ejecutamos de nuevo, vemos que nuestra web se comporta de la forma especificada al principio del artículo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~$ curl -I http://example.com/
HTTP/1.1 &lt;span class="m"&gt;301&lt;/span&gt; Moved Permanently
Server: nginx/1.10.3
Date: Tue, &lt;span class="m"&gt;06&lt;/span&gt; Jun &lt;span class="m"&gt;2017&lt;/span&gt; &lt;span class="m"&gt;14&lt;/span&gt;:47:57 GMT
Content-Type: text/html
Content-Length: &lt;span class="m"&gt;185&lt;/span&gt;
Connection: keep-alive
Location: http://www.example.com/

gerard@aldebaran:~$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;De esta forma, el navegador buscará la nueva página, de acuerdo a esta respuesta. Se fuerza la opción "follow redirects" en la salida del &lt;strong&gt;curl&lt;/strong&gt; para poder observar este comportamiento:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~$ curl http://www.example.com/
example site
gerard@aldebaran:~$ curl http://www.otherdomain.com/
default site
gerard@aldebaran:~$ curl -L http://example.com/
example site
gerard@aldebaran:~$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y lo mismo sería válido para forzar una redirección a HTTPS, aunque se deja como ejercicio al lector.&lt;/p&gt;</content><category term="nginx"></category><category term="redirecciones"></category></entry><entry><title>Uso básico de un cluster Docker Swarm</title><link href="https://www.linuxsysadmin.ml/2017/11/uso-basico-de-un-cluster-docker-swarm.html" rel="alternate"></link><published>2017-11-06T10:00:00+01:00</published><updated>2017-11-06T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-11-06:/2017/11/uso-basico-de-un-cluster-docker-swarm.html</id><summary type="html">&lt;p&gt;Usar un &lt;em&gt;cluster&lt;/em&gt; de &lt;strong&gt;docker swarm&lt;/strong&gt; no es transparente para nuestro uso; necesitamos cambiar de mentalidad y tener en cuenta algunos conceptos. Donde antes hablábamos de contenedores, aquí se habla de &lt;strong&gt;servicios&lt;/strong&gt;, que básicamente son un número variable de contenedores repartidos por los diferentes nodos del &lt;em&gt;cluster&lt;/em&gt; de forma balanceada …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Usar un &lt;em&gt;cluster&lt;/em&gt; de &lt;strong&gt;docker swarm&lt;/strong&gt; no es transparente para nuestro uso; necesitamos cambiar de mentalidad y tener en cuenta algunos conceptos. Donde antes hablábamos de contenedores, aquí se habla de &lt;strong&gt;servicios&lt;/strong&gt;, que básicamente son un número variable de contenedores repartidos por los diferentes nodos del &lt;em&gt;cluster&lt;/em&gt; de forma balanceada.&lt;/p&gt;
&lt;p&gt;Para trabajar con &lt;strong&gt;docker swarm&lt;/strong&gt; necesitamos trabajar desde un nodo &lt;em&gt;manager&lt;/em&gt;, y no desde los &lt;em&gt;workers&lt;/em&gt;. Por ello, el resto de comandos se van a lanzar en &lt;strong&gt;shangrila&lt;/strong&gt;, a menos que se diga lo contrario. En este artículo vamos a utilizar el que montamos en &lt;a href="https://www.linuxsysadmin.ml/2017/10/montando-un-cluster-de-docker-con-docker-swarm.html"&gt;este otro artículo&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Un servicio básico&lt;/h2&gt;
&lt;p&gt;Si queremos crear un servicio, podemos utilizar los subcomandos de &lt;code&gt;docker service&lt;/code&gt;. Una vez creado el servicio podemos trabajar con él sin problemas. Veamos una creación de un servicio básico:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker service create --replicas &lt;span class="m"&gt;1&lt;/span&gt; --name helloworld alpine ping docker.com
rl39orkjrie1r1vv0as368s8x
Since --detach&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt; was not specified, tasks will be created in the background.
In a future release, --detach&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt; will become the default.
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Básicamente esto nos indica que queremos crear un servicio llamado &lt;em&gt;helloworld&lt;/em&gt;, ejecutando un contenedor &lt;em&gt;alpine&lt;/em&gt; y lanzando el comando &lt;code&gt;ping docker.com&lt;/code&gt;. El número de replicas inicial lo ponemos, por ejemplo, a 1; podemos escalarlo cuando queramos.&lt;/p&gt;
&lt;p&gt;Podemos ver el estado de nuestro servicio con &lt;code&gt;docker service ls&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
rl39orkjrie1        helloworld          replicated          &lt;span class="m"&gt;1&lt;/span&gt;/1                 alpine:latest
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Si necesitamos una información más detallada, podemos hacer un &lt;code&gt;docker service inspect&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker service inspect --pretty helloworld

ID:             rl39orkjrie1r1vv0as368s8x
Name:           helloworld
Service Mode:   Replicated
 Replicas:      &lt;span class="m"&gt;1&lt;/span&gt;
Placement:
UpdateConfig:
 Parallelism:   &lt;span class="m"&gt;1&lt;/span&gt;
 On failure:    pause
 Monitoring Period: 5s
 Max failure ratio: &lt;span class="m"&gt;0&lt;/span&gt;
 Update order:      stop-first
RollbackConfig:
 Parallelism:   &lt;span class="m"&gt;1&lt;/span&gt;
 On failure:    pause
 Monitoring Period: 5s
 Max failure ratio: &lt;span class="m"&gt;0&lt;/span&gt;
 Rollback order:    stop-first
ContainerSpec:
 Image:         alpine:latest@sha256:f006ecbb824d87947d0b51ab8488634bf69fe4094959d935c0c103f4820a417d
 Args:          ping docker.com
Resources:
Endpoint Mode:  vip
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Con &lt;code&gt;docker service ps&lt;/code&gt; podemos saber en que nodos se encuentra nuestro servicio desplegado:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker service ps helloworld
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
q5xeb2bftqlq        helloworld.1        alpine:latest       eldorado            Running             Running &lt;span class="m"&gt;10&lt;/span&gt; minutes ago
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Escalando el servicio&lt;/h2&gt;
&lt;p&gt;En el caso que necesitemos ajustar el número de nodos que ejecutan una copia, podemos escalar. Esto tampoco tiene ninguna complicación:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker service scale &lt;span class="nv"&gt;helloworld&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;
helloworld scaled to &lt;span class="m"&gt;4&lt;/span&gt;
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Hemos escalado nuestro servicio por encima del número de nodos del &lt;strong&gt;swarm&lt;/strong&gt;. En este caso, esto no es un problema; solamante veremos que hay nodos con varios contenedores.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
rl39orkjrie1        helloworld          replicated          &lt;span class="m"&gt;4&lt;/span&gt;/4                 alpine:latest
gerard@shangrila:~$ docker service ps helloworld
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
q5xeb2bftqlq        helloworld.1        alpine:latest       eldorado            Running             Running &lt;span class="m"&gt;12&lt;/span&gt; minutes ago
6ruk3h9lvzom        helloworld.2        alpine:latest       shangrila           Running             Running &lt;span class="m"&gt;2&lt;/span&gt; seconds ago
j2xuo9pvfoeu        helloworld.3        alpine:latest       arcadia             Running             Running &lt;span class="m"&gt;2&lt;/span&gt; seconds ago
t2emtl7m6r3b        helloworld.4        alpine:latest       arcadia             Running             Running &lt;span class="m"&gt;2&lt;/span&gt; seconds ago
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Si se cayera un nodo, el mismo &lt;strong&gt;swarm&lt;/strong&gt; se encarga de levantar otro contenedor en un nodo vivo para sustituir los fallos posibles de este &lt;em&gt;downtime&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;erard@shangrila:~$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
5p0wbl6rhvs5oo461xsmxhph4     eldorado            Down                Active
rtmzvsbndn4ox5mhzsgu2xi81 *   shangrila           Ready               Active              Leader
su20j7s0itgssfcm8x5whz8o8     arcadia             Ready               Active
gerard@shangrila:~$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
rl39orkjrie1        helloworld          replicated          &lt;span class="m"&gt;4&lt;/span&gt;/4                 alpine:latest
gerard@shangrila:~$ docker service ps helloworld
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
j26p1q1hgvtt        helloworld.1        alpine:latest       shangrila           Running             Running &lt;span class="m"&gt;5&lt;/span&gt; seconds ago
q5xeb2bftqlq         &lt;span class="se"&gt;\_&lt;/span&gt; helloworld.1    alpine:latest       eldorado            Shutdown            Running &lt;span class="m"&gt;24&lt;/span&gt; seconds ago
6ruk3h9lvzom        helloworld.2        alpine:latest       shangrila           Running             Running &lt;span class="m"&gt;3&lt;/span&gt; minutes ago
j2xuo9pvfoeu        helloworld.3        alpine:latest       arcadia             Running             Running &lt;span class="m"&gt;3&lt;/span&gt; minutes ago
t2emtl7m6r3b        helloworld.4        alpine:latest       arcadia             Running             Running &lt;span class="m"&gt;3&lt;/span&gt; minutes ago
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El contenedor &lt;em&gt;helloworld.1&lt;/em&gt; debería estar en &lt;strong&gt;eldorado&lt;/strong&gt;, pero como lo hemos apagado, se ha creado otro en su sustitución en &lt;strong&gt;shangrila&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Levantamos de nuevo &lt;strong&gt;eldorado&lt;/strong&gt;, y se vuelve al estado original, parando la instáncia de emergencia en &lt;strong&gt;shangrila&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
5p0wbl6rhvs5oo461xsmxhph4     eldorado            Ready               Active
rtmzvsbndn4ox5mhzsgu2xi81 *   shangrila           Ready               Active              Leader
su20j7s0itgssfcm8x5whz8o8     arcadia             Ready               Active
gerard@shangrila:~$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
rl39orkjrie1        helloworld          replicated          &lt;span class="m"&gt;4&lt;/span&gt;/4                 alpine:latest
gerard@shangrila:~$ docker service ps helloworld
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE             ERROR               PORTS
j26p1q1hgvtt        helloworld.1        alpine:latest       shangrila           Running             Running &lt;span class="m"&gt;3&lt;/span&gt; minutes ago
q5xeb2bftqlq         &lt;span class="se"&gt;\_&lt;/span&gt; helloworld.1    alpine:latest       eldorado            Shutdown            Shutdown &lt;span class="m"&gt;24&lt;/span&gt; seconds ago
6ruk3h9lvzom        helloworld.2        alpine:latest       shangrila           Running             Running &lt;span class="m"&gt;7&lt;/span&gt; minutes ago
j2xuo9pvfoeu        helloworld.3        alpine:latest       arcadia             Running             Running &lt;span class="m"&gt;7&lt;/span&gt; minutes ago
t2emtl7m6r3b        helloworld.4        alpine:latest       arcadia             Running             Running &lt;span class="m"&gt;7&lt;/span&gt; minutes ago
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora, ya podemos eliminar este servicio de test, puesto que no lo vamos a necesitar más.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker service rm helloworld
helloworld
gerard@shangrila:~$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
gerard@shangrila:~$ docker service ps helloworld
no such services: helloworld
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Publicando servicios&lt;/h2&gt;
&lt;p&gt;Si deseamos exponer nuestro servicio, necesitamos indicarlo con el &lt;em&gt;flag&lt;/em&gt; &lt;code&gt;--publish&lt;/code&gt;, exactamente igual que el &lt;em&gt;flag&lt;/em&gt; &lt;code&gt;-p&lt;/code&gt; en &lt;strong&gt;docker-engine&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Veamos un ejemplo. Tenemos una imagen &lt;em&gt;sirrtea/myhostname&lt;/em&gt; que responde por HTTP en el puerto 8080, indicando el nombre del &lt;em&gt;host&lt;/em&gt; (el contenedor) en el que se ejecuta.&lt;/p&gt;
&lt;p&gt;Creamos el servicio con 2 replicas, y publicando el puerto 8080 del contenedor en el 8888 de los &lt;em&gt;hosts&lt;/em&gt; del &lt;em&gt;cluster&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker service create --replicas &lt;span class="m"&gt;2&lt;/span&gt; --name myhostname --publish &lt;span class="m"&gt;8888&lt;/span&gt;:8080 sirrtea/myhostname
c0pvrkvf233odc1z9piitf1wa
Since --detach&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt; was not specified, tasks will be created in the background.
In a future release, --detach&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt; will become the default.
gerard@shangrila:~$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE                       PORTS
c0pvrkvf233o        myhostname          replicated          &lt;span class="m"&gt;2&lt;/span&gt;/2                 sirrtea/myhostname:latest   *:8888-&amp;gt;8080/tcp
gerard@shangrila:~$ docker service ps myhostname
ID                  NAME                IMAGE                       NODE                DESIRED STATE       CURRENT STATE           ERROR               PORTS
xnljno3f285w        myhostname.1        sirrtea/myhostname:latest   shangrila           Running             Running &lt;span class="m"&gt;3&lt;/span&gt; seconds ago
rivqb4uqqiwl        myhostname.2        sirrtea/myhostname:latest   eldorado            Running             Running &lt;span class="m"&gt;3&lt;/span&gt; seconds ago
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;De ahora en adelante, podemos acceder a &lt;strong&gt;cualquier nodo&lt;/strong&gt; en el puerto 8888 y obtendremos un balanceo de peticiones entre todos los contenedores que conforman el servicio.&lt;/p&gt;
&lt;p&gt;Es importante recalcar que, aunque el servicio solo tiene contenedores en &lt;strong&gt;shangrila&lt;/strong&gt; y en &lt;strong&gt;eldorado&lt;/strong&gt;, también vamos a obtener respuesta del balanceador en &lt;strong&gt;arcadia&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;En el caso de mi &lt;em&gt;cluster&lt;/em&gt;, no tengo resolución DNS, así que os resumo las IPs de mi &lt;em&gt;cluster&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;shangrila&lt;/strong&gt; &amp;rarr; 192.168.56.2&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;arcadia&lt;/strong&gt; &amp;rarr; 192.168.56.3&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;eldorado&lt;/strong&gt; &amp;rarr; 192.168.56.4&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ curl http://192.168.56.2:8888/
Hello world from &amp;lt;em&amp;gt;9294907313e7&amp;lt;/em&amp;gt;
gerard@shangrila:~$ curl http://192.168.56.2:8888/
Hello world from &amp;lt;em&amp;gt;5295009a7737&amp;lt;/em&amp;gt;
gerard@shangrila:~$ curl http://192.168.56.3:8888/
Hello world from &amp;lt;em&amp;gt;9294907313e7&amp;lt;/em&amp;gt;
gerard@shangrila:~$ curl http://192.168.56.3:8888/
Hello world from &amp;lt;em&amp;gt;5295009a7737&amp;lt;/em&amp;gt;
gerard@shangrila:~$ curl http://192.168.56.4:8888/
Hello world from &amp;lt;em&amp;gt;9294907313e7&amp;lt;/em&amp;gt;
gerard@shangrila:~$ curl http://192.168.56.4:8888/
Hello world from &amp;lt;em&amp;gt;5295009a7737&amp;lt;/em&amp;gt;
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En este caso concreto, el &lt;em&gt;hostname&lt;/em&gt; 9294907313e7 es el de la instancia de &lt;strong&gt;shangrila&lt;/strong&gt; y el &lt;em&gt;hostname&lt;/em&gt; 5295009a7737 corresponde a la intancia de &lt;strong&gt;eldorado&lt;/strong&gt;. Esto se puede comprobar fácilmente mirando en los nodos que albergan los contenedores.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker ps
CONTAINER ID        IMAGE                       COMMAND                  CREATED             STATUS              PORTS               NAMES
9294907313e7        sirrtea/myhostname:latest   &lt;span class="s2"&gt;&amp;quot;/usr/bin/gunicorn...&amp;quot;&lt;/span&gt;   &lt;span class="m"&gt;25&lt;/span&gt; minutes ago      Up &lt;span class="m"&gt;25&lt;/span&gt; minutes                           myhostname.1.xnljno3f285wkfl0xyfmb0xek
gerard@shangrila:~$ docker inspect myhostname.1.xnljno3f285wkfl0xyfmb0xek &lt;span class="p"&gt;|&lt;/span&gt; grep Hostname
        &lt;span class="s2"&gt;&amp;quot;HostnamePath&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;/var/lib/docker/containers/9294907313e78d46193dabb36eee2b6eb422208675812c77fdef96139cb0e62b/hostname&amp;quot;&lt;/span&gt;,
            &lt;span class="s2"&gt;&amp;quot;Hostname&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;9294907313e7&amp;quot;&lt;/span&gt;,
gerard@shangrila:~$

gerard@eldorado:~$ docker ps
CONTAINER ID        IMAGE                       COMMAND                  CREATED             STATUS              PORTS               NAMES
5295009a7737        sirrtea/myhostname:latest   &lt;span class="s2"&gt;&amp;quot;/usr/bin/gunicorn...&amp;quot;&lt;/span&gt;   &lt;span class="m"&gt;25&lt;/span&gt; minutes ago      Up &lt;span class="m"&gt;25&lt;/span&gt; minutes                           myhostname.2.rivqb4uqqiwlbla8716foavzy
gerard@eldorado:~$ docker inspect myhostname.2.rivqb4uqqiwlbla8716foavzy &lt;span class="p"&gt;|&lt;/span&gt; grep Hostname
        &lt;span class="s2"&gt;&amp;quot;HostnamePath&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;/var/lib/docker/containers/5295009a7737ef709260ea984b9c7720d6bd752a40d8305e86c2649c0ab2af10/hostname&amp;quot;&lt;/span&gt;,
            &lt;span class="s2"&gt;&amp;quot;Hostname&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;5295009a7737&amp;quot;&lt;/span&gt;,
gerard@eldorado:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La recomendación oficial para tener tener una única IP consiste en poner delante de todos los nodos un balanceador, especialmente uno que pueda descartar nodos parados como puede ser &lt;strong&gt;HAProxy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Con eso ganamos balanceo entre todos los nodos del &lt;em&gt;cluster&lt;/em&gt; que queden vivos y, en caso de alcanzar alguno, un balanceo entre todas las instancias de un servicio dado.&lt;/p&gt;</content><category term="docker"></category><category term="swarm"></category><category term="cluster"></category><category term="uso"></category><category term="basico"></category></entry><entry><title>Montando un cluster de docker con docker swarm</title><link href="https://www.linuxsysadmin.ml/2017/10/montando-un-cluster-de-docker-con-docker-swarm.html" rel="alternate"></link><published>2017-10-30T10:00:00+01:00</published><updated>2017-10-30T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-10-30:/2017/10/montando-un-cluster-de-docker-con-docker-swarm.html</id><summary type="html">&lt;p&gt;Usar &lt;strong&gt;docker&lt;/strong&gt; en nuestro dia a dia es muy interesante y tiene un montón de aplicaciones prácticas; sin embargo no es la mejor opción confiar en un único servidor en producción. Para tener alta disponibilidad y alto renidmiento podemos montar un cluster, como por ejemplo su implementación oficial, &lt;strong&gt;docker swarm …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Usar &lt;strong&gt;docker&lt;/strong&gt; en nuestro dia a dia es muy interesante y tiene un montón de aplicaciones prácticas; sin embargo no es la mejor opción confiar en un único servidor en producción. Para tener alta disponibilidad y alto renidmiento podemos montar un cluster, como por ejemplo su implementación oficial, &lt;strong&gt;docker swarm&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Preparando los servidores&lt;/h2&gt;
&lt;p&gt;Vamos a seguir el tutorial mismo de la página de docker, que nos invita a crear 3 servidores, siendo uno el &lt;em&gt;manager&lt;/em&gt; y el resto &lt;em&gt;nodos&lt;/em&gt;. Así pues, nuestro cluster se va a componer inicialmente de 3 servidores:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;shangrila&lt;/strong&gt; &amp;rarr; nuestro manager, con IP 192.168.56.2&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;arcadia&lt;/strong&gt; &amp;rarr; el nodo 1, con IP 192.168.56.3&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;eldorado&lt;/strong&gt; &amp;rarr; el nodo 2, con IP 192.168.56.4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Para ello vamos a preparar tres máquinas que son las que van a componer el cluster de &lt;strong&gt;Docker&lt;/strong&gt;. Las máquinas solo necesitan &lt;strong&gt;docker&lt;/strong&gt; y me he guiado por &lt;a href="https://www.linuxsysadmin.ml/2017/07/montando-un-servidor-docker-con-debian-stretch.html"&gt;este artículo&lt;/a&gt;. Adicionalmente, he instalado el servicio &lt;strong&gt;NTP&lt;/strong&gt; siguiendo &lt;a href="https://www.linuxsysadmin.ml/2017/10/un-servicio-casi-imprescindible-ntp.html"&gt;este otro artículo&lt;/a&gt;, aunque esto es opcional.&lt;/p&gt;
&lt;p&gt;Un punto importante a tener en cuenta es que las máquinas deben tener &lt;a href="https://docs.docker.com/engine/swarm/swarm-tutorial/#the-ip-address-of-the-manager-machine"&gt;conectividad con el manager&lt;/a&gt;, y deben poder accederse entre ellas por &lt;a href="https://docs.docker.com/engine/swarm/swarm-tutorial/#open-protocols-and-ports-between-the-hosts"&gt;algunos puertos&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Creando el manager&lt;/h2&gt;
&lt;p&gt;Entramos por SSH en el manager (en nuestro caso &lt;strong&gt;shangrila&lt;/strong&gt;) y lanzamos el comando &lt;code&gt;docker swarm init&lt;/code&gt;. Esto nos va a crear un cluster con solamente un &lt;em&gt;manager&lt;/em&gt; y sin &lt;em&gt;nodos&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker swarm init --advertise-addr &lt;span class="m"&gt;192&lt;/span&gt;.168.56.2
Swarm initialized: current node &lt;span class="o"&gt;(&lt;/span&gt;rtmzvsbndn4ox5mhzsgu2xi81&lt;span class="o"&gt;)&lt;/span&gt; is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-09ebvbcbqjttt3o6ssqfnkb1n4xuzf1e1jildujhkh7dpb6iaq-0i6smvy5g75h2mn82t8kv4ptd &lt;span class="m"&gt;192&lt;/span&gt;.168.56.2:2377

To add a manager to this swarm, run &lt;span class="s1"&gt;&amp;#39;docker swarm join-token manager&amp;#39;&lt;/span&gt; and follow the instructions.

gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;También hemos añadido el flag &lt;code&gt;--advertise-addr&lt;/code&gt; para indicar la IP del &lt;em&gt;manager&lt;/em&gt; que se va a usar para las comunicaciones con los diferentes nodos.&lt;/p&gt;
&lt;p&gt;Ahora ya tenemos un cluster de &lt;em&gt;docker swarm&lt;/em&gt; operativo, aunque al no tener &lt;em&gt;nodos&lt;/em&gt;, no podemos usar algunos de los comandos, que son multinodo.&lt;/p&gt;
&lt;p&gt;Podemos ver los nodos del cluster y el estado del mismo con unos pocos comandos:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker info
...
Swarm: active
 NodeID: rtmzvsbndn4ox5mhzsgu2xi81
 Is Manager: &lt;span class="nb"&gt;true&lt;/span&gt;
 ClusterID: krknl2v0llpyczyqg1ihi250d
 Managers: &lt;span class="m"&gt;1&lt;/span&gt;
 Nodes: &lt;span class="m"&gt;1&lt;/span&gt;
 Orchestration:
  Task History Retention Limit: &lt;span class="m"&gt;5&lt;/span&gt;
 Raft:
  Snapshot Interval: &lt;span class="m"&gt;10000&lt;/span&gt;
  Number of Old Snapshots to Retain: &lt;span class="m"&gt;0&lt;/span&gt;
  Heartbeat Tick: &lt;span class="m"&gt;1&lt;/span&gt;
  Election Tick: &lt;span class="m"&gt;3&lt;/span&gt;
 Dispatcher:
  Heartbeat Period: &lt;span class="m"&gt;5&lt;/span&gt; seconds
 CA Configuration:
  Expiry Duration: &lt;span class="m"&gt;3&lt;/span&gt; months
  Force Rotate: &lt;span class="m"&gt;0&lt;/span&gt;
 Root Rotation In Progress: &lt;span class="nb"&gt;false&lt;/span&gt;
 Node Address: &lt;span class="m"&gt;192&lt;/span&gt;.168.56.2
 Manager Addresses:
  &lt;span class="m"&gt;192&lt;/span&gt;.168.56.2:2377
...
gerard@shangrila:~$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
rtmzvsbndn4ox5mhzsgu2xi81 *   shangrila           Ready               Active              Leader
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Añadiendo nodos al cluster&lt;/h2&gt;
&lt;p&gt;Para añadir un &lt;em&gt;nodo&lt;/em&gt; en el cluster, solo tenemos que entrar en ese nodo y lanzar el comando que nos sugirió el &lt;code&gt;docker init&lt;/code&gt;. Si no nos acordamos, siempre podemos pedir que nos lo repita:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker swarm join-token worker
To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-09ebvbcbqjttt3o6ssqfnkb1n4xuzf1e1jildujhkh7dpb6iaq-0i6smvy5g75h2mn82t8kv4ptd &lt;span class="m"&gt;192&lt;/span&gt;.168.56.2:2377

gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Así tal cual, lanzamos el comando en cualquier &lt;em&gt;nodo&lt;/em&gt; que deseemos preparar ahora. No es necesario poner todos los &lt;em&gt;nodos&lt;/em&gt; en este momento, pudiendo añadir &lt;em&gt;nodos&lt;/em&gt; en un futuro, tal como nuestro cluster lo vaya necesitando.&lt;/p&gt;
&lt;p&gt;Lanzamos en &lt;strong&gt;arcadia&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@arcadia:~$ docker swarm join --token SWMTKN-1-09ebvbcbqjttt3o6ssqfnkb1n4xuzf1e1jildujhkh7dpb6iaq-0i6smvy5g75h2mn82t8kv4ptd &lt;span class="m"&gt;192&lt;/span&gt;.168.56.2:2377
This node joined a swarm as a worker.
gerard@arcadia:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y lanzamos en &lt;strong&gt;eldorado&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@eldorado:~$ docker swarm join --token SWMTKN-1-09ebvbcbqjttt3o6ssqfnkb1n4xuzf1e1jildujhkh7dpb6iaq-0i6smvy5g75h2mn82t8kv4ptd &lt;span class="m"&gt;192&lt;/span&gt;.168.56.2:2377
This node joined a swarm as a worker.
gerard@eldorado:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y podemos comprobar de nuevo el estado de nuestro cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@shangrila:~$ docker info
...
Swarm: active
 NodeID: rtmzvsbndn4ox5mhzsgu2xi81
 Is Manager: &lt;span class="nb"&gt;true&lt;/span&gt;
 ClusterID: krknl2v0llpyczyqg1ihi250d
 Managers: &lt;span class="m"&gt;1&lt;/span&gt;
 Nodes: &lt;span class="m"&gt;3&lt;/span&gt;
 Orchestration:
  Task History Retention Limit: &lt;span class="m"&gt;5&lt;/span&gt;
 Raft:
  Snapshot Interval: &lt;span class="m"&gt;10000&lt;/span&gt;
  Number of Old Snapshots to Retain: &lt;span class="m"&gt;0&lt;/span&gt;
  Heartbeat Tick: &lt;span class="m"&gt;1&lt;/span&gt;
  Election Tick: &lt;span class="m"&gt;3&lt;/span&gt;
 Dispatcher:
  Heartbeat Period: &lt;span class="m"&gt;5&lt;/span&gt; seconds
 CA Configuration:
  Expiry Duration: &lt;span class="m"&gt;3&lt;/span&gt; months
  Force Rotate: &lt;span class="m"&gt;0&lt;/span&gt;
 Root Rotation In Progress: &lt;span class="nb"&gt;false&lt;/span&gt;
 Node Address: &lt;span class="m"&gt;192&lt;/span&gt;.168.56.2
 Manager Addresses:
  &lt;span class="m"&gt;192&lt;/span&gt;.168.56.2:2377
...
gerard@shangrila:~$ docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS
5p0wbl6rhvs5oo461xsmxhph4     eldorado            Ready               Active
rtmzvsbndn4ox5mhzsgu2xi81 *   shangrila           Ready               Active              Leader
su20j7s0itgssfcm8x5whz8o8     arcadia             Ready               Active
gerard@shangrila:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;AVISO:&lt;/strong&gt; Tened en cuenta que los comandos de estado del cluster solo se pueden lanzar en un &lt;em&gt;manager&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Y con esto tenemos montado nuestro cluster.&lt;/p&gt;</content><category term="docker"></category><category term="swarm"></category><category term="cluster"></category></entry><entry><title>Un servicio casi imprescindible: NTP</title><link href="https://www.linuxsysadmin.ml/2017/10/un-servicio-casi-imprescindible-ntp.html" rel="alternate"></link><published>2017-10-02T10:00:00+02:00</published><updated>2017-10-02T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-10-02:/2017/10/un-servicio-casi-imprescindible-ntp.html</id><summary type="html">&lt;p&gt;Normalmente, me gustan los servidores con un número de servicios tirando a mezquino; menos servicios significan menos actualizaciones, menos superficie de ataque y menos recursos ocupados. Sin embargo, hay algunos que son imprescindibles, mientras que otros son altamente recomendables. Este es el caso del &lt;strong&gt;NTP&lt;/strong&gt;, que mantiene la hora actualizada …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Normalmente, me gustan los servidores con un número de servicios tirando a mezquino; menos servicios significan menos actualizaciones, menos superficie de ataque y menos recursos ocupados. Sin embargo, hay algunos que son imprescindibles, mientras que otros son altamente recomendables. Este es el caso del &lt;strong&gt;NTP&lt;/strong&gt;, que mantiene la hora actualizada.&lt;/p&gt;
&lt;p&gt;Esto es crucial para muchos servicios de &lt;em&gt;cluster&lt;/em&gt;, que necesitan una sincronización temporal estricta. Otros usos son para aplicaciones y sus &lt;em&gt;logs&lt;/em&gt;, en donde el momento exacto en el que pasan las cosas es crucial, y puede ser consultado en &lt;em&gt;logs&lt;/em&gt; de varias máquinas, que idealmente deberían coincidir. Finalmente, el otro uso que considero indispensable es para aquellas máquinas que se dedican a virtualizar contenedores u otras máquinas virtuales, ya que pagando el precio del proceso una única vez, permite a sus descendientes (por ejemplo, contenedores &lt;strong&gt;docker&lt;/strong&gt;) seguir actualizadas.&lt;/p&gt;
&lt;h2&gt;Instalación de NTP&lt;/h2&gt;
&lt;p&gt;El servicio &lt;strong&gt;NTP&lt;/strong&gt; se instala en un solo paquete, que tanto en la famíla &lt;em&gt;RedHat&lt;/em&gt; con en la família &lt;em&gt;Debian&lt;/em&gt;, se llama &lt;strong&gt;ntp&lt;/strong&gt;. Usad las herramientas que tengáis a mano en vuestra distribución.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# apt-get install ntp
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
Se instalarán los siguientes paquetes adicionales:
  libopts25 libperl5.24 perl perl-modules-5.24 rename
Paquetes sugeridos:
  ntp-doc perl-doc libterm-readline-gnu-perl &lt;span class="p"&gt;|&lt;/span&gt; libterm-readline-perl-perl make
Se instalarán los siguientes paquetes NUEVOS:
  libopts25 libperl5.24 ntp perl perl-modules-5.24 rename
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;6&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;7&lt;/span&gt;.103 kB de archivos.
Se utilizarán &lt;span class="m"&gt;42&lt;/span&gt;,1 MB de espacio de disco adicional después de esta operación.
¿Desea continuar? &lt;span class="o"&gt;[&lt;/span&gt;S/n&lt;span class="o"&gt;]&lt;/span&gt; s
...
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La configuración por defecto ya viene preconfigurada con algunos servidores base contra los que sincronizar. Podemos ver que estamos sincronizando y contra qué, con el comando &lt;strong&gt;ntpq&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# ntpq -p
     remote           refid      st t when poll reach   delay   offset  &lt;span class="nv"&gt;jitter&lt;/span&gt;
&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
 &lt;span class="m"&gt;0&lt;/span&gt;.debian.pool.n .POOL.          &lt;span class="m"&gt;16&lt;/span&gt; p    -   &lt;span class="m"&gt;64&lt;/span&gt;    &lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="m"&gt;0&lt;/span&gt;.000    &lt;span class="m"&gt;0&lt;/span&gt;.000   &lt;span class="m"&gt;0&lt;/span&gt;.000
 &lt;span class="m"&gt;1&lt;/span&gt;.debian.pool.n .POOL.          &lt;span class="m"&gt;16&lt;/span&gt; p    -   &lt;span class="m"&gt;64&lt;/span&gt;    &lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="m"&gt;0&lt;/span&gt;.000    &lt;span class="m"&gt;0&lt;/span&gt;.000   &lt;span class="m"&gt;0&lt;/span&gt;.000
 &lt;span class="m"&gt;2&lt;/span&gt;.debian.pool.n .POOL.          &lt;span class="m"&gt;16&lt;/span&gt; p    -   &lt;span class="m"&gt;64&lt;/span&gt;    &lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="m"&gt;0&lt;/span&gt;.000    &lt;span class="m"&gt;0&lt;/span&gt;.000   &lt;span class="m"&gt;0&lt;/span&gt;.000
 &lt;span class="m"&gt;3&lt;/span&gt;.debian.pool.n .POOL.          &lt;span class="m"&gt;16&lt;/span&gt; p    -   &lt;span class="m"&gt;64&lt;/span&gt;    &lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="m"&gt;0&lt;/span&gt;.000    &lt;span class="m"&gt;0&lt;/span&gt;.000   &lt;span class="m"&gt;0&lt;/span&gt;.000
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En este caso, podemos ver que, debido a la configuración de red, no llegamos a los servidores:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;El tiempo desde la última sincronización no existe (&lt;em&gt;when&lt;/em&gt; vacío)&lt;/li&gt;
&lt;li&gt;El &lt;em&gt;stratum&lt;/em&gt; de los servidores es infinito, con lo que son inaccesibles (&lt;em&gt;st&lt;/em&gt; vale 16 para marcar esta inaccesibilidad)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Así pues, vamos a cambiar la configuración del &lt;strong&gt;NTP&lt;/strong&gt; para acceder a un servidor &lt;strong&gt;NTP&lt;/strong&gt; en nuestra propia red local, y quitando los que habían.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cat /etc/ntp.conf &lt;span class="p"&gt;|&lt;/span&gt; egrep &lt;span class="s2"&gt;&amp;quot;pool.*debian|server 10&amp;quot;&lt;/span&gt;
server &lt;span class="m"&gt;10&lt;/span&gt;.0.0.1
&lt;span class="c1"&gt;# pool 0.debian.pool.ntp.org iburst&lt;/span&gt;
&lt;span class="c1"&gt;# pool 1.debian.pool.ntp.org iburst&lt;/span&gt;
&lt;span class="c1"&gt;# pool 2.debian.pool.ntp.org iburst&lt;/span&gt;
&lt;span class="c1"&gt;# pool 3.debian.pool.ntp.org iburst&lt;/span&gt;
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Como no, reiniciamos el servicio para que use la nueva configuración:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# systemctl restart ntp
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo queda observar que estamos sincronizando contra el servidor solicitado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# ntpq -p
     remote           refid      st t when poll reach   delay   offset  &lt;span class="nv"&gt;jitter&lt;/span&gt;
&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
 &lt;span class="m"&gt;10&lt;/span&gt;.0.0.1            &lt;span class="m"&gt;10&lt;/span&gt;.0.0.1    &lt;span class="m"&gt;4&lt;/span&gt; u   &lt;span class="m"&gt;28&lt;/span&gt;   &lt;span class="m"&gt;64&lt;/span&gt;    &lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="m"&gt;0&lt;/span&gt;.277  -12.387   &lt;span class="m"&gt;9&lt;/span&gt;.552
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Servidor NTP&lt;/h2&gt;
&lt;p&gt;Para configurar un servidor &lt;strong&gt;NTP&lt;/strong&gt; no se necesita nada adicional. El paquete que acabamos de instalar ya ha levantado un servidor &lt;strong&gt;NTP&lt;/strong&gt; preparado para que lo usen otros servidores que lleguen a él vía red. De hecho, en el ejemplo estamos sincronizando contra otro servidor (10.0.0.1) que es idéntico al del ejemplo, solo que está sincronizando de otro servidor. A nivel de seguridad, hay que tener en cuenta que el protocolo &lt;strong&gt;NTP&lt;/strong&gt; utiliza los puertos 123 TCP y UDP y solo se necesita permitir uno de estos dos a nivel de &lt;em&gt;firewall&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;El estrato 4 (en el ejemplo) significa que estamos sincronizando contra un servidor de estrato 4, que es uno que sincroniza de uno de estrato 3. Al final de la cadena, encontraremos un reloj GPS o atómico, que es un servidor de estrato 1. El protocolo &lt;strong&gt;NTP&lt;/strong&gt; permite tener estratos hasta 15, significando el número 16 que no habría ninguna conectividad con el servidor especificado.&lt;/p&gt;</content><category term="ntp"></category><category term="servidor"></category><category term="hora"></category></entry><entry><title>Un proceso inicial para docker: tini y dumb-init</title><link href="https://www.linuxsysadmin.ml/2017/09/un-proceso-inicial-para-docker-tini-y-dumb-init.html" rel="alternate"></link><published>2017-09-11T10:00:00+02:00</published><updated>2017-09-11T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-09-11:/2017/09/un-proceso-inicial-para-docker-tini-y-dumb-init.html</id><summary type="html">&lt;p&gt;Siempre nos han vendido que &lt;strong&gt;docker&lt;/strong&gt; ejecuta un solo proceso, y que este puede ser cualquiera. Sin embargo, este proceso se ejecuta con PID 1, que es un poco especial y que tiene unas responsabilidades adicionales. Si no queremos implementarlas, podemos usar alguna solución que ya lo haga para nosotros …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Siempre nos han vendido que &lt;strong&gt;docker&lt;/strong&gt; ejecuta un solo proceso, y que este puede ser cualquiera. Sin embargo, este proceso se ejecuta con PID 1, que es un poco especial y que tiene unas responsabilidades adicionales. Si no queremos implementarlas, podemos usar alguna solución que ya lo haga para nosotros.&lt;/p&gt;
&lt;p&gt;Entre estas responsabilidades, podemos citar 3 que se consideran básicas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tiene que adoptar y controlar todos aquellos procesos que quedan huérfanos debido a una mala gestión de su anterior padre&lt;/li&gt;
&lt;li&gt;No puede dejar que ningún proceso &lt;em&gt;zombie&lt;/em&gt; quede sin su correspondiente &lt;em&gt;wait&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Debe ser capaz de progresar las señales de terminación a sus procesos hijos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Muchos de los binarios que utilizamos habitualmente no incumplen estas responsabilidades, sea por una buena gestión, o porque directamente no levantan procesos hijos.&lt;/p&gt;
&lt;p&gt;El problema es cuando alguno de estos procesos sí que incumple. En estos casos &lt;strong&gt;docker&lt;/strong&gt; puede enviar señales de acabado, y viendo que no todos los procesos han acabado, tiene que entrar tras 10 segundos a arreglar el desaguisado. Aunque &lt;strong&gt;docker&lt;/strong&gt; hace un trabajo magnífico en este aspecto, el resultado es un contenedor que es caro de apagar, en cuanto a tiempo se refiere.&lt;/p&gt;
&lt;p&gt;Y es por eso que han habido varios intentos de crear un proceso &lt;strong&gt;init&lt;/strong&gt; que pueda levantar otro proceso único, pero cumpliendo con las responsabilidades que se le presuponen. Entre estos binarios, me gustaría mencionar dos: &lt;strong&gt;tini&lt;/strong&gt; y &lt;strong&gt;dumb-init&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;El problema&lt;/h2&gt;
&lt;p&gt;Vamos a hacer este ejemplo con un servicio afectado por el problema, para su fácil demostración. No se trata de un servicio raro o de uso minoritario, sino que estamos hablando de &lt;strong&gt;haproxy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Vamos a partir del más simple de los balanceadores basados en &lt;strong&gt;haproxy&lt;/strong&gt; y &lt;strong&gt;alpine linux&lt;/strong&gt;, con una configuración mínima (por no decir nula).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@docker:~/docker/docker-init$ cat context/Dockerfile
FROM alpine:3.6
RUN apk add --no-cache haproxy
COPY haproxy.cfg /etc/haproxy/
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;haproxy&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-f&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;/etc/haproxy/haproxy.cfg&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-db&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@docker:~/docker/docker-init$ cat context/haproxy.cfg
global
    chroot /var/lib/haproxy
    user haproxy
    group haproxy

defaults
    mode http

listen stats
    &lt;span class="nb"&gt;bind&lt;/span&gt; *:8080
    stats &lt;span class="nb"&gt;enable&lt;/span&gt;
    stats uri /

&lt;span class="c1"&gt;#listen web&lt;/span&gt;
&lt;span class="c1"&gt;#    bind *:80&lt;/span&gt;
&lt;span class="c1"&gt;#    balance roundrobin&lt;/span&gt;
&lt;span class="c1"&gt;#    server web1 web1:80 check&lt;/span&gt;
&lt;span class="c1"&gt;#    server web2 web2:80 check&lt;/span&gt;
gerard@docker:~/docker/docker-init$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La vamos a construir siguiendo los comandos habituales:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@docker:~/docker/docker-init$ docker build -t balancer context/
Sending build context to Docker daemon &lt;span class="m"&gt;3&lt;/span&gt;.072 kB
...
Successfully built 499dc4873adb
gerard@docker:~/docker/docker-init$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lo que vamos a ver es que la imagen no se detiene en un tiempo adecuado. Para ellos vamos a automatizar su levantamiento y su parada con &lt;strong&gt;docker-compose&lt;/strong&gt; y vamos a cronometrar lo segundo. Os adjunto el fichero &lt;em&gt;docker-compose.yml&lt;/em&gt;, aunque es relativamente simple.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@docker:~/docker/docker-init$ cat docker-compose.yml
version: &lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;
services:
  balancer:
    image: balancer
    container_name: balancer
    hostname: balancer
gerard@docker:~/docker/docker-init$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;dockerinit_default&amp;quot;&lt;/span&gt; with the default driver
Creating balancer
gerard@docker:~/docker/docker-init$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y podemos ver que este contenedor tan simple no acaba decentemente, teniendo que esperar 10 segundos para que &lt;strong&gt;docker&lt;/strong&gt; elimine el resto, cosa que es molesta y puede llevar a problemas futuros.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@docker:~/docker/docker-init$ &lt;span class="nb"&gt;time&lt;/span&gt; docker-compose down
Stopping balancer ... &lt;span class="k"&gt;done&lt;/span&gt;
Removing balancer ... &lt;span class="k"&gt;done&lt;/span&gt;
Removing network dockerinit_default

real    0m10,486s
user    0m0,348s
sys     0m0,028s
gerard@docker:~/docker/docker-init$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;La solución&lt;/h2&gt;
&lt;p&gt;Ambas soluciones propuestas (&lt;strong&gt;tini&lt;/strong&gt; y &lt;strong&gt;dumb-init&lt;/strong&gt;) funcionan de la misma forma: ejecutan el comando que se les pasa en los argumentos. De esta forma, el comando "peligroso" se ejecuta con PID diferente de 1, siendo el PID 1 el mismo &lt;em&gt;init&lt;/em&gt;. Podemos anteponer el &lt;em&gt;init&lt;/em&gt; sin muchas modificaciones; basta con instalar el &lt;em&gt;init&lt;/em&gt; y usar la directiva &lt;code&gt;ENTRYPOINT&lt;/code&gt; para anteponer el nuevo &lt;em&gt;init&lt;/em&gt;. Veamos ambos como ejemplo.&lt;/p&gt;
&lt;h3&gt;Usando tini&lt;/h3&gt;
&lt;p&gt;Podemos instalar el paquete &lt;strong&gt;tini&lt;/strong&gt; sin añadir una nueva línea en el &lt;em&gt;Dockerfile&lt;/em&gt;, aprovechando el &lt;code&gt;apk add&lt;/code&gt; del mismo &lt;strong&gt;haproxy&lt;/strong&gt;. Prefijamos nuestro &lt;code&gt;CMD&lt;/code&gt; con el binario &lt;strong&gt;tini&lt;/strong&gt; mediante el uso de &lt;code&gt;ENTRYPOINT&lt;/code&gt; y listo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@docker:~/docker/docker-init$ cat context/Dockerfile.2
FROM alpine:3.6
RUN apk add --no-cache haproxy tini
COPY haproxy.cfg /etc/haproxy/
ENTRYPOINT &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tini&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;haproxy&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-f&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;/etc/haproxy/haproxy.cfg&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-db&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@docker:~/docker/docker-init$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Construimos la nueva imagen, y tras modificar el &lt;em&gt;docker-compose.yml&lt;/em&gt;, la levantamos. Modificad el parámetro &lt;em&gt;image&lt;/em&gt; en el &lt;em&gt;docker-compose.yml&lt;/em&gt;, para reflejar el nuevo &lt;em&gt;tag&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@docker:~/docker/docker-init$ docker build -t balancer:v2 -f context/Dockerfile.2 context/
Sending build context to Docker daemon  &lt;span class="m"&gt;5&lt;/span&gt;.12 kB
...
Successfully built 179697bbd3ed
gerard@docker:~/docker/docker-init$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;dockerinit_default&amp;quot;&lt;/span&gt; with the default driver
Creating balancer
gerard@docker:~/docker/docker-init$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y midiendo el tiempo de parada, vemos que el problema ha desaparecido:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@docker:~/docker/docker-init$ &lt;span class="nb"&gt;time&lt;/span&gt; docker-compose down
Stopping balancer ... &lt;span class="k"&gt;done&lt;/span&gt;
Removing balancer ... &lt;span class="k"&gt;done&lt;/span&gt;
Removing network dockerinit_default

real    0m0,473s
user    0m0,284s
sys     0m0,020s
gerard@docker:~/docker/docker-init$
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Usando dumb-init&lt;/h3&gt;
&lt;p&gt;Este caso es análogo al anterior, sin más cambios que el nombre del paquete a instalar y el binario del &lt;code&gt;ENTRYPOINT&lt;/code&gt;. Es importante notar que a pesar de partir del primer ejemplo, el resultado es prácticamente idéntico al segundo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@docker:~/docker/docker-init$ cat context/Dockerfile.3
FROM alpine:3.6
RUN apk add --no-cache haproxy dumb-init
COPY haproxy.cfg /etc/haproxy/
ENTRYPOINT &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;dumb-init&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;haproxy&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-f&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;/etc/haproxy/haproxy.cfg&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-db&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@docker:~/docker/docker-init$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Construimos la nueva imagen, y tras modificar el &lt;em&gt;docker-compose.yml&lt;/em&gt;, la levantamos, justo como antes. Tened la precaución de usar nuevo &lt;em&gt;tag&lt;/em&gt; en el &lt;em&gt;docker-compose.yml&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@docker:~/docker/docker-init$ docker build -t balancer:v3 -f context/Dockerfile.3 context/
Sending build context to Docker daemon  &lt;span class="m"&gt;5&lt;/span&gt;.12 kB
...
Successfully built 928c992c5251
gerard@docker:~/docker/docker-init$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;dockerinit_default&amp;quot;&lt;/span&gt; with the default driver
Creating balancer
gerard@docker:~/docker/docker-init$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y midiendo el tiempo de parada, vemos que el problema también desaparece:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@docker:~/docker/docker-init$ &lt;span class="nb"&gt;time&lt;/span&gt; docker-compose down
Stopping balancer ... &lt;span class="k"&gt;done&lt;/span&gt;
Removing balancer ... &lt;span class="k"&gt;done&lt;/span&gt;
Removing network dockerinit_default

real    0m0,520s
user    0m0,252s
sys     0m0,060s
gerard@docker:~/docker/docker-init$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Conclusión&lt;/h2&gt;
&lt;p&gt;El hecho de tener procesos &lt;em&gt;zombie&lt;/em&gt; es más una molestia que un problema real, al menos mientras &lt;strong&gt;docker&lt;/strong&gt; pueda limpiar lo que quede al final. Sin embargo, las buenas maneras, y un proceso ágil de despliegue, nos sugieren encarecidamente que tratemos estos detalles de forma adecuada.&lt;/p&gt;
&lt;p&gt;En cuanto al peso adicional en las imágenes por poner nuestros procesos &lt;em&gt;init&lt;/em&gt;, podemos ver que es casi nula:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@docker:~/docker/docker-init$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED              SIZE
balancer            v3                  928c992c5251        About a minute ago   &lt;span class="m"&gt;5&lt;/span&gt;.674 MB
balancer            v2                  179697bbd3ed        &lt;span class="m"&gt;5&lt;/span&gt; minutes ago        &lt;span class="m"&gt;5&lt;/span&gt;.651 MB
balancer            latest              499dc4873adb        &lt;span class="m"&gt;9&lt;/span&gt; minutes ago        &lt;span class="m"&gt;5&lt;/span&gt;.631 MB
alpine              &lt;span class="m"&gt;3&lt;/span&gt;.6                 7328f6f8b418        &lt;span class="m"&gt;7&lt;/span&gt; days ago           &lt;span class="m"&gt;3&lt;/span&gt;.966 MB
gerard@docker:~/docker/docker-init$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Así pues, en caso de duda, ponerlo siempre nos puede ahorrar algunos dolores de cabeza, aunque por ahora los desconozcamos.&lt;/p&gt;</content><category term="docker"></category><category term="Dockerfile"></category><category term="tini"></category><category term="dumb-init"></category></entry><entry><title>Creando imágenes con estilo: la instrucción ONBUILD</title><link href="https://www.linuxsysadmin.ml/2017/08/creando-imagenes-con-estilo-la-instruccion-onbuild.html" rel="alternate"></link><published>2017-08-28T10:00:00+02:00</published><updated>2017-08-28T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-08-28:/2017/08/creando-imagenes-con-estilo-la-instruccion-onbuild.html</id><summary type="html">&lt;p&gt;En el día a día de mi trabajo, me encuentro con un conjunto muy variado de ficheros &lt;em&gt;Dockerfile&lt;/em&gt; que vienen a hacer lo mismo, pero de formas muy distintas. El fichero original se pasa de mano en mano, pervirtiéndose en cada paso y al final queda hecho un gran asco …&lt;/p&gt;</summary><content type="html">&lt;p&gt;En el día a día de mi trabajo, me encuentro con un conjunto muy variado de ficheros &lt;em&gt;Dockerfile&lt;/em&gt; que vienen a hacer lo mismo, pero de formas muy distintas. El fichero original se pasa de mano en mano, pervirtiéndose en cada paso y al final queda hecho un gran asco.&lt;/p&gt;
&lt;p&gt;Para evitar la reinvención de la rueda me propuse crear una imagen base, para que los desarrolladores no tuvieran que crear una imagen, que muchas veces está mal por falta de conocimiento de &lt;em&gt;Linux&lt;/em&gt;, y que reduzca al máximo su participación.&lt;/p&gt;
&lt;h2&gt;Un caso simple&lt;/h2&gt;
&lt;p&gt;Imaginemos una aplicación hecha con &lt;em&gt;NodeJS&lt;/em&gt;, que es el caso más frecuente en mi trabajo; nuestro flujo de trabajo exige el uso de &lt;em&gt;npm&lt;/em&gt; y del correspondiente &lt;em&gt;package.json&lt;/em&gt;. Una instalación básica es bastante simple: se trata de copiar la aplicación, ejecutar el &lt;code&gt;npm install&lt;/code&gt; de rigor y declarar que se va a ejecutar con &lt;code&gt;npm start&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Tomemos como ejemplo el básico de &lt;em&gt;express&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/nodetest/v1$ cat app/app.js 
const &lt;span class="nv"&gt;express&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; require&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;express&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
const &lt;span class="nv"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; express&lt;span class="o"&gt;()&lt;/span&gt;

app.get&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/&amp;#39;&lt;/span&gt;, &lt;span class="k"&gt;function&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;req, res&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  res.send&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Hello World!&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;})&lt;/span&gt;

app.listen&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3000&lt;/span&gt;, &lt;span class="k"&gt;function&lt;/span&gt; &lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  console.log&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Example app listening on port 3000!&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;})&lt;/span&gt;
gerard@aldebaran:~/docker/nodetest/v1$ cat app/package.json 
&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;app&amp;quot;&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;version&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;1.0.0&amp;quot;&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;description&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;main&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;app.js&amp;quot;&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;scripts&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;start&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;node app.js&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;test&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;echo \&amp;quot;Error: no test specified\&amp;quot; &amp;amp;&amp;amp; exit 1&amp;quot;&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;keywords&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[]&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;author&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;license&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;ISC&amp;quot;&lt;/span&gt;,
  &lt;span class="s2"&gt;&amp;quot;dependencies&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;express&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;4.15.3&amp;quot;&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@aldebaran:~/docker/nodetest/v1$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Siguiendo las instrucciones descritas más arriba, el &lt;em&gt;Dockerfile&lt;/em&gt; no guarda ninguna complicación:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/nodetest/v1$ cat Dockerfile 
FROM node:6-slim
COPY app/ /srv/app/
WORKDIR /srv/app
RUN npm install --production
USER node
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;npm&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;start&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@aldebaran:~/docker/nodetest/v1$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Este &lt;em&gt;Dockerfile&lt;/em&gt; nos genera una imagen adecuada, pero este fichero peca del mismo error que el resto: va a mutar un poco por cada mano por la que pase. Sin embargo, no podemos hacer una imagen base porque necesitamos la aplicación en la primera instrucción tras el &lt;strong&gt;FROM&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;La imagen base onbuild&lt;/h2&gt;
&lt;p&gt;Si miramos &lt;a href="https://docs.docker.com/engine/reference/builder/#onbuild"&gt;la documentación&lt;/a&gt;, podemos ver que podemos declarar algunas operaciones para que se lancen automáticamente tras todo &lt;strong&gt;FROM&lt;/strong&gt; que herede de nuestra imagen base. De esta forma, podemos declarar operaciones pendientes, al no disponer todavía de la aplicación final que va a tener que ejecutar nuestro contenedor.&lt;/p&gt;
&lt;p&gt;Vamos a retrasar la ejecución de todas aquellas instrucciones que dependan de la aplicación, que por cierto no está en este contexto, al no existir todavía:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/nodetest/onbuild$ cat Dockerfile 
FROM node:6-slim
ONBUILD COPY app/ /srv/app/
ONBUILD WORKDIR /srv/app
ONBUILD RUN npm install --production
ONBUILD USER node
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;npm&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;start&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@aldebaran:~/docker/nodetest/onbuild$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Con este &lt;em&gt;Dockerfile&lt;/em&gt; podemos generar una imagen base, que registre nuestras operaciones pendientes. Si construimos la imagen, veremos que no se ejecuta el &lt;code&gt;npm install&lt;/code&gt;, ni las otras instrucciones precedidas por &lt;strong&gt;ONBUILD&lt;/strong&gt;. Cualquier &lt;em&gt;Dockerfile&lt;/em&gt; que extienda esta imagen base, conseguirá varias cosas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Va a disponer de todos los añadidos por instrucciones lanzadas sin el &lt;strong&gt;ONBUILD&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Inmediatamente tras el &lt;strong&gt;FROM&lt;/strong&gt; se van a ejecutar las operaciones indicadas en el &lt;strong&gt;ONBUILD&lt;/strong&gt; (el &lt;strong&gt;COPY&lt;/strong&gt;, el &lt;strong&gt;WORKDIR&lt;/strong&gt;, el &lt;strong&gt;RUN&lt;/strong&gt; y el &lt;strong&gt;USER&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;El desarrollador no necesita declarar todas estas operaciones; solo va a necesitar aquellas que sean específicas de su proyecto.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Veamos el mismo ejemplo de antes; creamos un contexto con la misma aplicación y un &lt;em&gt;Dockerfile&lt;/em&gt;, aunque este último queda bastante simplificado (suponiendo que la imagen base ha sido etiquetada como &lt;em&gt;gerard/node:onbuild&lt;/em&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/nodetest/v2$ tree
.
├── app
│   ├── app.js
│   └── package.json
└── Dockerfile

&lt;span class="m"&gt;1&lt;/span&gt; directory, &lt;span class="m"&gt;3&lt;/span&gt; files
gerard@aldebaran:~/docker/nodetest/v2$ cat Dockerfile 
FROM gerard/node:onbuild
gerard@aldebaran:~/docker/nodetest/v2$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Si la construimos, vemos que justo tras acabar el paso del &lt;strong&gt;FROM&lt;/strong&gt;, van a saltar de forma automática los &lt;em&gt;triggers&lt;/em&gt;  declarados por la instrucción &lt;strong&gt;ONBUILD&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/nodetest/v2$ docker build -t gerard/app:v2 .
Sending build context to Docker daemon  &lt;span class="m"&gt;4&lt;/span&gt;.608kB
Step &lt;span class="m"&gt;1&lt;/span&gt;/1 : FROM gerard/node:onbuild
&lt;span class="c1"&gt;# Executing 4 build triggers...&lt;/span&gt;
Step &lt;span class="m"&gt;1&lt;/span&gt;/1 : COPY app/ /srv/app/
Step &lt;span class="m"&gt;1&lt;/span&gt;/1 : WORKDIR /srv/app
Step &lt;span class="m"&gt;1&lt;/span&gt;/1 : RUN npm install --production
 ---&amp;gt; Running in e3fe9e739e34
...  
Step &lt;span class="m"&gt;1&lt;/span&gt;/1 : USER node
 ---&amp;gt; Running in 3725eb574aff
 ---&amp;gt; d4661e9857e8
Removing intermediate container 7c9b3293ed2f
Removing intermediate container 7976c2b5aaaa
Removing intermediate container e3fe9e739e34
Removing intermediate container 3725eb574aff
Successfully built d4661e9857e8
Successfully tagged gerard/app:v2
gerard@aldebaran:~/docker/nodetest/v2$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto aseguramos que el desarrollador pasa por el aro, usando las instrucciones que realmente necesitamos para ejecutar la aplicación.&lt;/p&gt;</content><category term="docker"></category><category term="dockerfile"></category><category term="onbuild"></category></entry><entry><title>Problemas de escritura con openshift</title><link href="https://www.linuxsysadmin.ml/2017/08/problemas-de-escritura-con-openshift.html" rel="alternate"></link><published>2017-08-21T10:00:00+02:00</published><updated>2017-08-21T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-08-21:/2017/08/problemas-de-escritura-con-openshift.html</id><summary type="html">&lt;p&gt;En mi trabajo se ha decidido por el uso de virtualización por contenedores usando &lt;strong&gt;Openshift&lt;/strong&gt;. No es nada demasiado nuevo, puesto que ya usábamos &lt;strong&gt;Docker&lt;/strong&gt; de manera habitual, pero ha habido alguna &lt;em&gt;feature&lt;/em&gt; que nos ha hecho plantearnos el modo en el que hacemos las cosas, especialmente para las escrituras …&lt;/p&gt;</summary><content type="html">&lt;p&gt;En mi trabajo se ha decidido por el uso de virtualización por contenedores usando &lt;strong&gt;Openshift&lt;/strong&gt;. No es nada demasiado nuevo, puesto que ya usábamos &lt;strong&gt;Docker&lt;/strong&gt; de manera habitual, pero ha habido alguna &lt;em&gt;feature&lt;/em&gt; que nos ha hecho plantearnos el modo en el que hacemos las cosas, especialmente para las escrituras.&lt;/p&gt;
&lt;p&gt;Todo viene por una directiva de seguridad que prohibe estrictamente ejecutar un contenedor como &lt;em&gt;root&lt;/em&gt;, y de la misma manera, ejecuta el contenedor con un usuario aleatorio para incrementar la seguridad general.&lt;/p&gt;
&lt;p&gt;El problema viene para el pobre hombre que se dedica a generar imágenes, ya que la falta de determinismo, te asegura casi al 100% que no vas a poder escribir en las carpetas del contenedor, a menos que sepas lo que estás haciendo.&lt;/p&gt;
&lt;p&gt;Sin embargo, saber lo que hace &lt;strong&gt;Openshift&lt;/strong&gt; no es tarea complicada:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Se te asegura que nunca vas a usar el &lt;em&gt;uid&lt;/em&gt; 0, sino uno aleatorio&lt;/li&gt;
&lt;li&gt;El grupo del usuario de ejecución se mantiene siempre como &lt;em&gt;root&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Un problema derivado de esto es que no podemos ejecutar nada que requiera &lt;em&gt;root&lt;/em&gt;, como por ejemplo SSH (que no podríamos exponer en &lt;strong&gt;Openshift&lt;/strong&gt; de todas formas). Otro problema es que no tendremos permisos para crear carpetas en &lt;em&gt;runtime&lt;/em&gt; o incluso para cambiar el usuario de ejecución.&lt;/p&gt;
&lt;h2&gt;Simulando el comportamiento de Openshift&lt;/h2&gt;
&lt;p&gt;Vamos a poner un ejemplo tipo para entender lo que pasa:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/openshift_simulator$ cat Dockerfile
FROM python:2-slim
COPY server.py /
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/server.py&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@atlantis:~/projects/openshift_simulator$ cat server.py
&lt;span class="c1"&gt;#!/usr/bin/env python&lt;/span&gt;

from wsgiref.simple_server import make_server, demo_app

&lt;span class="nv"&gt;server&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; make_server&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;0.0.0.0&amp;#39;&lt;/span&gt;, &lt;span class="m"&gt;8080&lt;/span&gt;, demo_app&lt;span class="o"&gt;)&lt;/span&gt;
server.serve_forever&lt;span class="o"&gt;()&lt;/span&gt;
gerard@atlantis:~/projects/openshift_simulator$

Y lo construimos:

&lt;span class="sb"&gt;```&lt;/span&gt;bash
gerard@atlantis:~/projects/openshift_simulator$ docker build -t openshift_simulator .
Sending build context to Docker daemon  &lt;span class="m"&gt;3&lt;/span&gt;.072kB
...
Successfully tagged openshift_simulator:latest
gerard@atlantis:~/projects/openshift_simulator$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Normalmente lo ejecutaríamos de la siguiente manera:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/openshift_simulator$ docker run -ti --rm -p 8888:8080 --name test1 openshift_simulator
...
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos comprobar que los procesos, tanto nuevos como antiguos, corren con el usuario &lt;em&gt;root&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; test1 id
&lt;span class="nv"&gt;uid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;root&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;gid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;root&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;groups&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;root&lt;span class="o"&gt;)&lt;/span&gt;
gerard@atlantis:~$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; test1 ps -efa
UID        PID  PPID  C STIME TTY          TIME CMD
root         &lt;span class="m"&gt;1&lt;/span&gt;     &lt;span class="m"&gt;0&lt;/span&gt;  &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;14&lt;/span&gt;:18 pts/0    &lt;span class="m"&gt;00&lt;/span&gt;:00:00 python /server.py
root        &lt;span class="m"&gt;17&lt;/span&gt;     &lt;span class="m"&gt;0&lt;/span&gt;  &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;14&lt;/span&gt;:20 ?        &lt;span class="m"&gt;00&lt;/span&gt;:00:00 ps -efa
gerard@atlantis:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Sin embargo, en &lt;strong&gt;openshift&lt;/strong&gt; el usuario se elige de forma aleatoria, y se impone con el &lt;em&gt;flag&lt;/em&gt; de usuario &lt;em&gt;-u&lt;/em&gt;, como sigue:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~/projects/openshift_simulator$ docker run -ti --rm -p &lt;span class="m"&gt;8888&lt;/span&gt;:8080 --name test2 -u &lt;span class="m"&gt;123456&lt;/span&gt; openshift_simulator
...
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y podemos ver que los procesos amparados por este contenedor se ejecutarían con el &lt;em&gt;uid&lt;/em&gt; especificado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; test2 id
&lt;span class="nv"&gt;uid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;123456&lt;/span&gt; &lt;span class="nv"&gt;gid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;root&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;groups&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;root&lt;span class="o"&gt;)&lt;/span&gt;
gerard@atlantis:~$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; test2 ps -efa
UID        PID  PPID  C STIME TTY          TIME CMD
&lt;span class="m"&gt;123456&lt;/span&gt;       &lt;span class="m"&gt;1&lt;/span&gt;     &lt;span class="m"&gt;0&lt;/span&gt;  &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;14&lt;/span&gt;:22 pts/0    &lt;span class="m"&gt;00&lt;/span&gt;:00:00 python /server.py
&lt;span class="m"&gt;123456&lt;/span&gt;       &lt;span class="m"&gt;9&lt;/span&gt;     &lt;span class="m"&gt;0&lt;/span&gt;  &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;14&lt;/span&gt;:22 ?        &lt;span class="m"&gt;00&lt;/span&gt;:00:00 ps -efa
gerard@atlantis:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Implicaciones en escritura&lt;/h2&gt;
&lt;p&gt;Como no sabemos el usuario con el que vamos a ejecutar, es especialmente interesante saber donde vamos a escribir, ya que los permisos de lectura suelen ser suficientes para todo el mundo. Sin embargo, las carpetas de escritura suelen estar más restringidas.&lt;/p&gt;
&lt;p&gt;En este caso, estas carpetas tienen dos posibles salidas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Les damos barra libre con permisos 777, que no van a gustar a ningún miembro del equipo de seguridad&lt;/li&gt;
&lt;li&gt;Afinamos los permisos aprovechándonos de que nunca vamos a ser &lt;em&gt;root&lt;/em&gt;, pero vamos a ejecutar con su grupo&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;De esta forma, podemos ver la propiedad y los permisos de forma individual:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Usuario&lt;/strong&gt;: con pertenencia a &lt;em&gt;root&lt;/em&gt; nos aseguramos de que los permisos no aplican nunca, con lo que podemos ponerlos como queramos.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grupo&lt;/strong&gt;: Esta es la mejor forma de asegurar que la carpeta nos pertenece. Aquí si que tenemos que dar permisos de escritura.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Otros&lt;/strong&gt;: Nunca hay que dar permisos de escritura a este grupo; ningún auditor de seguridad lo va a permitir.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;De esta forma, la pertenencia habuitual para carpetas de lectura y escritura que suelo poner es &lt;code&gt;root:root&lt;/code&gt;, y los permisos acostumbran a ser 575, aunque no me libro de explicaciones cuando pido las excepciones de seguridad pertinentes.&lt;/p&gt;</content><category term="docker"></category><category term="openshift"></category><category term="permisos"></category></entry><entry><title>Evitando problemas de concurrencia múltiple con flock</title><link href="https://www.linuxsysadmin.ml/2017/08/evitando-problemas-de-concurrencia-multiple-con-flock.html" rel="alternate"></link><published>2017-08-07T10:00:00+02:00</published><updated>2017-08-07T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-08-07:/2017/08/evitando-problemas-de-concurrencia-multiple-con-flock.html</id><summary type="html">&lt;p&gt;Cuando trabajas con procesos en &lt;em&gt;background&lt;/em&gt;, es fácil que algunos de los procesos hagan algo que necesite exclusividad, no siendo seguro ejecutar varios de estos procesos a la vez. Por ejemplo, archivos que se descomprimen, se procesan y luego se borran; si usan la misma carpeta suele ser un problema …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Cuando trabajas con procesos en &lt;em&gt;background&lt;/em&gt;, es fácil que algunos de los procesos hagan algo que necesite exclusividad, no siendo seguro ejecutar varios de estos procesos a la vez. Por ejemplo, archivos que se descomprimen, se procesan y luego se borran; si usan la misma carpeta suele ser un problema.&lt;/p&gt;
&lt;p&gt;No vamos a entrar en como se lanzan estos procesos, pero vamos a dar énfasis en que no deben ejecutarse a la vez. Para ello, vamos a suponer que tenemos un proceso que nos interesa ejecutar en exclusividad. Voy a sustituir este proceso por un &lt;em&gt;script&lt;/em&gt;, para que no nos distraigamos del punto importante.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/flock_test$ cat process.sh 
&lt;span class="c1"&gt;#!/bin/bash&lt;/span&gt;

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;date +%H:%M:%S&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt; - Starting process in terminal &lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
sleep &lt;span class="m"&gt;5&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;date +%H:%M:%S&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt; - Process ended in terminal &lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
gerard@aldebaran:~/flock_test$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Muchos de los desarrolladores os propondrían miles de soluciones para evitar este caso, pero si buscamos en la &lt;em&gt;toolbox&lt;/em&gt; de Linux, podemos encontrar herramientas útiles. En mi caso concreto encontré el comando &lt;strong&gt;flock&lt;/strong&gt;, que actúa bloqueando un comando, en base a la existencia de un fichero de &lt;em&gt;lock&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Ejecutando casi concurrentemente&lt;/h2&gt;
&lt;p&gt;Para conseguirlo, voy a abrir dos terminales, uno para cada proceso. El &lt;em&gt;script&lt;/em&gt; va a recibir el numero de terminal por un parámetro, que voy a poner manualmente.&lt;/p&gt;
&lt;p&gt;Vamos al primer terminal, y ejecutamos nuestro &lt;em&gt;script&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/flock_test$ ./process.sh &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="m"&gt;12&lt;/span&gt;:43:38 - Starting process in terminal &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="m"&gt;12&lt;/span&gt;:43:43 - Process ended in terminal &lt;span class="m"&gt;1&lt;/span&gt;
gerard@aldebaran:~/flock_test$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Antes de que acabe, cambio al otro terminal y ejecuto lo mismo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/flock_test$ ./process.sh &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="m"&gt;12&lt;/span&gt;:43:40 - Starting process in terminal &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="m"&gt;12&lt;/span&gt;:43:45 - Process ended in terminal &lt;span class="m"&gt;2&lt;/span&gt;
gerard@aldebaran:~/flock_test$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Si juntamos las líneas de &lt;em&gt;log&lt;/em&gt; y las ordenamos, vemos claramente que los procesos estuvieron en algún momento ejecutándose a la vez.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="m"&gt;12&lt;/span&gt;:43:38 - Starting process in terminal &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="m"&gt;12&lt;/span&gt;:43:40 - Starting process in terminal &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="m"&gt;12&lt;/span&gt;:43:43 - Process ended in terminal &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="m"&gt;12&lt;/span&gt;:43:45 - Process ended in terminal &lt;span class="m"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En este caso, no parece peligroso que se ejecuten a la vez, pero hay que usar la imaginación y creernos que podrían dar problemas ejecutados a la vez.&lt;/p&gt;
&lt;h2&gt;Ejecución exclusiva con flock&lt;/h2&gt;
&lt;p&gt;Como puede interesarnos que no se ejecuten a la vez, podemos utilizar el comando &lt;strong&gt;flock&lt;/strong&gt; para conseguir que ambos procesos esperen ordenadamente la posibilidad de ejecutarse.&lt;/p&gt;
&lt;p&gt;El comando &lt;strong&gt;flock&lt;/strong&gt; esperaría la inexistencia de un &lt;em&gt;lock&lt;/em&gt; en el fichero indicado, momento en el que pondría dicho &lt;em&gt;lock&lt;/em&gt; para asegurar que ningún otro proceso pudiera ejecutarse. Lo siguiente sería ejecutar nuestro &lt;em&gt;script&lt;/em&gt;, y finalmente, eliminar el &lt;em&gt;lock&lt;/em&gt; puesto. Otro proceso concurrente quedaría a la espera de la liberación de &lt;em&gt;lock&lt;/em&gt; antes de poder proceder, de manera similar al anterior.&lt;/p&gt;
&lt;p&gt;El proceso va a ser el mismo: ejecutamos el &lt;em&gt;script&lt;/em&gt; en ambos terminales, prefijado esta vez por el comando &lt;strong&gt;flock&lt;/strong&gt; y el fichero sobre el que se va a poner el &lt;em&gt;lock&lt;/em&gt;. Pasamos a juntar las líneas de ambos terminales por brevedad:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="m"&gt;12&lt;/span&gt;:44:59 - Starting process in terminal &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="m"&gt;12&lt;/span&gt;:45:04 - Process ended in terminal &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="m"&gt;12&lt;/span&gt;:45:04 - Starting process in terminal &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="m"&gt;12&lt;/span&gt;:45:09 - Process ended in terminal &lt;span class="m"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto podemos ver que el proceso del terminal 2 ha tenido que esperar a que el comando &lt;strong&gt;flock&lt;/strong&gt; en el primer terminal acabara, antes de poder proceder a ejecutar su &lt;em&gt;script&lt;/em&gt;. Con eso se garantiza la exclusividad de ejecución y los problemas que podría haber derivados de esta situación.&lt;/p&gt;
&lt;p&gt;En un caso de &lt;em&gt;boom&lt;/em&gt; de procesos, podríamos ver un grupo de procesos esperando sin lanzar sus respectivos &lt;em&gt;scripts&lt;/em&gt;, mientras que uno solo de ello estaría ejecutando en exclusividad.&lt;/p&gt;</content><category term="flock"></category></entry><entry><title>Montando un servidor docker con Debian Stretch</title><link href="https://www.linuxsysadmin.ml/2017/07/montando-un-servidor-docker-con-debian-stretch.html" rel="alternate"></link><published>2017-07-24T10:00:00+02:00</published><updated>2017-07-24T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-07-24:/2017/07/montando-un-servidor-docker-con-debian-stretch.html</id><summary type="html">&lt;p&gt;Finalmente ha sucedido: ha llegado el esperado lanzamiento de &lt;strong&gt;Debian Stretch&lt;/strong&gt;. Como buen linuxero no me he podido resistir a hacer alguna instalación para probar, aunque solo sea como una máquina virtual. Su función, determinada por mi actual flujo de trabajo, va a ser como servidor de &lt;strong&gt;docker&lt;/strong&gt; con &lt;strong&gt;docker-compose …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Finalmente ha sucedido: ha llegado el esperado lanzamiento de &lt;strong&gt;Debian Stretch&lt;/strong&gt;. Como buen linuxero no me he podido resistir a hacer alguna instalación para probar, aunque solo sea como una máquina virtual. Su función, determinada por mi actual flujo de trabajo, va a ser como servidor de &lt;strong&gt;docker&lt;/strong&gt; con &lt;strong&gt;docker-compose&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;En este caso, lo necesito para mi uso personal, pero en el ámbito de mi trabajo. Una de las particularidades a las que me enfrento es un &lt;em&gt;proxy&lt;/em&gt; &lt;strong&gt;squid&lt;/strong&gt; no transparente, así que también voy a ponerlo como anotaciones en el artículo.&lt;/p&gt;
&lt;h2&gt;El servidor base&lt;/h2&gt;
&lt;p&gt;Se trata de instalar un sistema operativo básico con SSH, partiendo de la imagen &lt;em&gt;netinst&lt;/em&gt;, que considero suficiente para un servidor estándar, y me evita descargar una &lt;em&gt;iso&lt;/em&gt; más grande.&lt;/p&gt;
&lt;p&gt;No voy a explicar como se instala; simplemente he respondido las preguntas de la instalación tal como me las hacía. Solo hace falta tener en cuenta que se eligió el servidor &lt;strong&gt;SSH&lt;/strong&gt; durante la instalación (concretamente en el &lt;em&gt;tasksel&lt;/em&gt;) y que se indicó el &lt;em&gt;proxy&lt;/em&gt; cuando se me preguntó.&lt;/p&gt;
&lt;p&gt;Para evitar que la operación &lt;strong&gt;apt-get update&lt;/strong&gt; tarde más tiempo de lo debido, vamos a limpiar el fichero &lt;em&gt;/etc/apt/sources.list&lt;/em&gt;, eliminando las entradas que no nos interesen.&lt;/p&gt;
&lt;p&gt;Y ya para acabar, vamos a crear una carpeta &lt;em&gt;bin&lt;/em&gt; para nuestro usuario de trabajo, lo que hace especialmente fácil poner &lt;em&gt;scripts&lt;/em&gt; locales para el mismo usuario.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~$ mkdir bin
gerard@atlantis:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Instalar docker engine y docker-compose&lt;/h2&gt;
&lt;h3&gt;Docker engine&lt;/h3&gt;
&lt;p&gt;Para instalar &lt;strong&gt;docker engine&lt;/strong&gt; vamos a seguir &lt;a href="https://docs.docker.com/engine/installation/linux/docker-ce/debian/"&gt;la documentación&lt;/a&gt;. El primer paso es descargarse la clave oficial GPG de &lt;strong&gt;docker&lt;/strong&gt;, para que &lt;strong&gt;apt&lt;/strong&gt; confíe en la fuente de &lt;em&gt;software&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTA&lt;/strong&gt;: es probable que el comando &lt;strong&gt;wget&lt;/strong&gt; falle si estamos detrás de un &lt;em&gt;proxy&lt;/em&gt;; basta con exportar la variable de entorno &lt;strong&gt;https_proxy&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@atlantis:~# apt-get install apt-transport-https
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
...
root@atlantis:~# wget -qO- https://download.docker.com/linux/debian/gpg &lt;span class="p"&gt;|&lt;/span&gt; apt-key add -
OK
root@atlantis:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Añadimos la línea adecuada para usar el repositorio oficial de &lt;strong&gt;docker&lt;/strong&gt; y, tras hacer el correspondiente &lt;em&gt;update&lt;/em&gt;, instalamos el paquete &lt;strong&gt;docker-ce&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@atlantis:~# &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;deb https://download.docker.com/linux/debian stretch stable&amp;quot;&lt;/span&gt; &amp;gt; /etc/apt/sources.list.d/docker.list
root@atlantis:~# apt-get update
Obj:1 http://security.debian.org/debian-security stretch/updates InRelease
Des:2 https://download.docker.com/linux/debian stretch InRelease &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;,2 kB&lt;span class="o"&gt;]&lt;/span&gt;
Des:3 https://download.docker.com/linux/debian stretch/stable amd64 Packages &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.934 B&lt;span class="o"&gt;]&lt;/span&gt;
Ign:4 http://ftp.fr.debian.org/debian stretch InRelease
Des:5 http://ftp.fr.debian.org/debian stretch-updates InRelease &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;88&lt;/span&gt;,5 kB&lt;span class="o"&gt;]&lt;/span&gt;
Obj:6 http://ftp.fr.debian.org/debian stretch Release
Descargados &lt;span class="m"&gt;111&lt;/span&gt; kB en 5s &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;19&lt;/span&gt;,5 kB/s&lt;span class="o"&gt;)&lt;/span&gt;
Leyendo lista de paquetes... Hecho
root@atlantis:~# apt-get install docker-ce
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
...
root@atlantis:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para hacer mas fácil el acceso a &lt;strong&gt;docker&lt;/strong&gt; para el usuario de trabajo, vamos a añadirlo al mismo grupo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@atlantis:~# usermod -a -G docker gerard
root@atlantis:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Docker compose&lt;/h3&gt;
&lt;p&gt;Para instalar esta utilidad, vamos a seguir &lt;a href="https://docs.docker.com/compose/install/"&gt;su documentación&lt;/a&gt;. En esencia se limita a descargar el binario en algún lugar del &lt;em&gt;path&lt;/em&gt; y a darle permisos de ejecución.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@atlantis:~# wget -qO /usr/local/bin/docker-compose https://github.com/docker/compose/releases/download/1.14.0/docker-compose-&lt;span class="sb"&gt;`&lt;/span&gt;uname -s&lt;span class="sb"&gt;`&lt;/span&gt;-&lt;span class="sb"&gt;`&lt;/span&gt;uname -m&lt;span class="sb"&gt;`&lt;/span&gt;
root@atlantis:~# chmod &lt;span class="m"&gt;755&lt;/span&gt; /usr/local/bin/docker-compose
root@atlantis:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Sobre los servidores proxy&lt;/h2&gt;
&lt;p&gt;Trabajar detrás de un servidor &lt;em&gt;proxy&lt;/em&gt; es un problema cuando trabajamos con &lt;strong&gt;docker&lt;/strong&gt;, ya que el &lt;em&gt;proxy&lt;/em&gt; debe configurarse a nivel de servicio, luego debe especificarse en cada &lt;em&gt;docker build&lt;/em&gt; y finalmente en cada &lt;em&gt;docker run&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;La primera configuración &lt;em&gt;proxy&lt;/em&gt; necesaria es la de &lt;strong&gt;apt&lt;/strong&gt;. Por suerte para nosotros, cuando instalamos el sistema operativo y lo indicamos ya nos guardó ese parámetro en &lt;em&gt;/etc/apt/apt.conf&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~$ cat /etc/apt/apt.conf
Acquire::http::Proxy &lt;span class="s2"&gt;&amp;quot;http://192.168.0.2:3128&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
gerard@atlantis:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A veces, algunos comandos como el &lt;strong&gt;wget&lt;/strong&gt; necesitan definir el &lt;em&gt;proxy&lt;/em&gt; como una variable de sistema; por ejemplo, para poner la variable &lt;strong&gt;https_proxy&lt;/strong&gt; para esta sesión de terminal, podemos hacer algo como:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@atlantis:~# &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;https_proxy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;http://192.168.0.2:3128
root@atlantis:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El demonio de &lt;strong&gt;docker&lt;/strong&gt; utiliza el &lt;em&gt;proxy&lt;/em&gt; definido en las variables de sistema. En el caso de &lt;em&gt;systemd&lt;/em&gt; podemos añadir estas variables de forma fácil añadiendo una configuración &lt;em&gt;overlay&lt;/em&gt;. Esto hace necesario recargar las configuraciones para el demonio de &lt;strong&gt;systemd&lt;/strong&gt; y luego el mismo demonio de &lt;strong&gt;docker&lt;/strong&gt; para que utilice las nuevas variables.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@atlantis:~# mkdir /etc/systemd/system/docker.service.d
root@atlantis:~# cat /etc/systemd/system/docker.service.d/proxy.conf
&lt;span class="o"&gt;[&lt;/span&gt;Service&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;Environment&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;HTTP_PROXY=http://192.168.0.2:3128&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;Environment&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;HTTPS_PROXY=http://192.168.0.2:3128&amp;quot;&lt;/span&gt;
root@atlantis:~# systemctl daemon-reload
root@atlantis:~# systemctl restart docker
root@atlantis:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finalmente, y por comodidad podemos añadir estas variables de entorno de forma permanente para el usuario de trabajo en el fichero &lt;em&gt;~/.bashrc&lt;/em&gt;, de forma que en cada nueva sesión de SSH no tengamos que redefinirlas. Aprovechamos también para añadir algunos &lt;em&gt;alias&lt;/em&gt; útiles para reducir los comandos de construcción de imágenes y ejecución de contenedores, escondiendo las variables del &lt;em&gt;proxy&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@atlantis:~$ cat .bashrc
...
&lt;span class="nb"&gt;alias&lt;/span&gt; &lt;span class="nv"&gt;drun&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;docker run -e &amp;quot;http_proxy=http://192.168.0.2:3128&amp;quot; -e &amp;quot;https_proxy=http://192.168.0.2:3128&amp;quot;&amp;#39;&lt;/span&gt;
&lt;span class="nb"&gt;alias&lt;/span&gt; &lt;span class="nv"&gt;dbuild&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;docker build --build-arg=&amp;quot;http_proxy=http://192.168.0.2:3128&amp;quot; --build-arg=&amp;quot;https_proxy=http://192.168.0.2:3128&amp;quot;&amp;#39;&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;HTTP_PROXY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;http://192.168.0.2:3128
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;HTTPS_PROXY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;HTTP_PROXY&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;http_proxy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;HTTP_PROXY&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;https_proxy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;HTTP_PROXY&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;NO_PROXY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;127.0.0.1,localhost&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;no_proxy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;NO_PROXY&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;
gerard@atlantis:~$
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Siguientes pasos&lt;/h2&gt;
&lt;p&gt;Es probable que este servidor necesite algunas utilidades que no hayan venido con los paquetes base. Nada nos impide ponerlos nosotros a mano, con los correspondientes &lt;em&gt;apt-get install&lt;/em&gt;. A partir de aquí, solo nos queda disfrutar de nuestro nuevo servidor &lt;strong&gt;docker&lt;/strong&gt; mínimo.&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="stretch"></category><category term="docker"></category><category term="docker-compose"></category></entry><entry><title>Autenticación centralizada por claves SSH</title><link href="https://www.linuxsysadmin.ml/2017/07/autenticacion-centralizada-por-claves-ssh.html" rel="alternate"></link><published>2017-07-17T10:00:00+02:00</published><updated>2017-07-17T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-07-17:/2017/07/autenticacion-centralizada-por-claves-ssh.html</id><summary type="html">&lt;p&gt;Ya vimos en &lt;a href="https://www.linuxsysadmin.ml/2016/05/autenticacion-ssh-por-claves.html"&gt;un artículo anterior&lt;/a&gt; como autenticar las sesiones &lt;strong&gt;SSH&lt;/strong&gt; mediante claves locales en la máquina. Sin embargo, esto no es práctico cuando tenemos muchos servidores, y hay que replicar esas claves en todos ellos. Hoy vamos a ver como usar un &lt;em&gt;script&lt;/em&gt; que pueda sacar las claves dinámicamente …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ya vimos en &lt;a href="https://www.linuxsysadmin.ml/2016/05/autenticacion-ssh-por-claves.html"&gt;un artículo anterior&lt;/a&gt; como autenticar las sesiones &lt;strong&gt;SSH&lt;/strong&gt; mediante claves locales en la máquina. Sin embargo, esto no es práctico cuando tenemos muchos servidores, y hay que replicar esas claves en todos ellos. Hoy vamos a ver como usar un &lt;em&gt;script&lt;/em&gt; que pueda sacar las claves dinámicamente.&lt;/p&gt;
&lt;p&gt;Empezamos teniendo un servidor con el servicio &lt;strong&gt;SSH&lt;/strong&gt; levantado; vamos a crear un usuario &lt;em&gt;guest&lt;/em&gt; en él para que pueda abrir una sesión en él.&lt;/p&gt;
&lt;h2&gt;Estado inicial&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@sshserver:~# adduser guest
Adding user &lt;span class="sb"&gt;`&lt;/span&gt;guest&lt;span class="s1"&gt;&amp;#39; ...&lt;/span&gt;
&lt;span class="s1"&gt;Adding new group `guest&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1000&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; ...
Adding new user &lt;span class="sb"&gt;`&lt;/span&gt;guest&lt;span class="s1"&gt;&amp;#39; (1000) with group `guest&amp;#39;&lt;/span&gt; ...
Creating home directory &lt;span class="sb"&gt;`&lt;/span&gt;/home/guest&lt;span class="s1"&gt;&amp;#39; ...&lt;/span&gt;
&lt;span class="s1"&gt;Copying files from `/etc/skel&amp;#39;&lt;/span&gt; ...
Enter new UNIX password: 
Retype new UNIX password: 
passwd: password updated successfully
Changing the user information &lt;span class="k"&gt;for&lt;/span&gt; guest
Enter the new value, or press ENTER &lt;span class="k"&gt;for&lt;/span&gt; the default
    Full Name &lt;span class="o"&gt;[]&lt;/span&gt;: 
    Room Number &lt;span class="o"&gt;[]&lt;/span&gt;: 
    Work Phone &lt;span class="o"&gt;[]&lt;/span&gt;: 
    Home Phone &lt;span class="o"&gt;[]&lt;/span&gt;: 
    Other &lt;span class="o"&gt;[]&lt;/span&gt;: 
Is the information correct? &lt;span class="o"&gt;[&lt;/span&gt;Y/n&lt;span class="o"&gt;]&lt;/span&gt; y
root@sshserver:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Por supuesto, este usuario pueden entrar con su contraseña sin problemas.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/docker/ssh$ ssh guest@sshserver
guest@sshserver&amp;#39;s password: 

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
guest@sshserver:~$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;El script&lt;/h2&gt;
&lt;p&gt;La idea es que necesitamos un &lt;em&gt;script&lt;/em&gt; que reciba como primer parámetro el usuario del que queremos las claves, y este &lt;em&gt;script&lt;/em&gt; nos va a dar una salida con el mismo formato que pondríamos en el &lt;em&gt;authorized_keys&lt;/em&gt;. Eso significa que podemos devolver 0, 1 o mas líneas, con una clave pública por línea.&lt;/p&gt;
&lt;p&gt;No es importante de donde saque este &lt;em&gt;script&lt;/em&gt; la información; puede ser de un campo LDAP, de una base de datos, o de una llamada a un &lt;em&gt;webservice&lt;/em&gt;. Para evitar complicaciones innecesarias, para este artículo y a modo de ejemplo, vamos a poner los valores en el mismo &lt;em&gt;script&lt;/em&gt;. Echad un poco de imaginación si lo reproducís.&lt;/p&gt;
&lt;p&gt;Supongamos que queremos entrar con el usuario &lt;em&gt;guest&lt;/em&gt;, y disponemos en la máquina inicial la clave privada &lt;em&gt;id_rsa&lt;/em&gt;, habiendo generado una clave pública correspondiente. Creamos un &lt;em&gt;script&lt;/em&gt; que nos vuelque esta clave si el usuario solicitado es &lt;em&gt;guest&lt;/em&gt;, siempre con permisos de ejecución.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@sshserver:~# cat /usr/bin/authorized_keys_by_user.sh 
&lt;span class="c1"&gt;#!/bin/bash&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;guest&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9heqwqgv+O9aekeCpETDR/6BdTQWDOrSlNN/tnZeZZa8/qjf0JEF4r8jSA/MquPQog1tpOXM0XUEY9YWNphARAmZ/gV1IiNJZmqQJSb2pk2/nQLq9nCqWoHBgKHKINUKfgmsiopGz9IjnZw5BBZKrloE9ZU0oApduxnVUTl/G71OWH/SdCbef08zvwVvLxv3zAWEKSnRvnSn5Q/FkRNb4Qe09po8ePgMqpZWKUvEpAntOvokI7uid300mmZjiUL8EMbJo4oJ3ONOnDbH8FNKEmGI4q2UK5HbDIUm8SJcmyJXvoo6xabApkc2AcM7X2tXRd8wiYS0p7YjLVMcIJ/NR gerard@sirius&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
root@sshserver:~# chmod &lt;span class="m"&gt;755&lt;/span&gt; /usr/bin/authorized_keys_by_user.sh 
root@sshserver:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos comprobar que nos da la clave pública para el usuario &lt;em&gt;guest&lt;/em&gt; y ninguna para otros usuarios.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@sshserver:~# /usr/bin/authorized_keys_by_user.sh guest
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9heqwqgv+O9aekeCpETDR/6BdTQWDOrSlNN/tnZeZZa8/qjf0JEF4r8jSA/MquPQog1tpOXM0XUEY9YWNphARAmZ/gV1IiNJZmqQJSb2pk2/nQLq9nCqWoHBgKHKINUKfgmsiopGz9IjnZw5BBZKrloE9ZU0oApduxnVUTl/G71OWH/SdCbef08zvwVvLxv3zAWEKSnRvnSn5Q/FkRNb4Qe09po8ePgMqpZWKUvEpAntOvokI7uid300mmZjiUL8EMbJo4oJ3ONOnDbH8FNKEmGI4q2UK5HbDIUm8SJcmyJXvoo6xabApkc2AcM7X2tXRd8wiYS0p7YjLVMcIJ/NR gerard@sirius
root@sshserver:~# /usr/bin/authorized_keys_by_user.sh other
root@sshserver:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En este caso hemos puesto una sola clave, pero podrían haber sido varias, igual que cuando usamos el fichero &lt;em&gt;authorized_keys&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Configuración SSH&lt;/h2&gt;
&lt;p&gt;Por una limitación del servicio &lt;strong&gt;SSH&lt;/strong&gt;, es necesario que tanto el &lt;em&gt;script&lt;/em&gt; como todas las carpetas en el &lt;em&gt;path&lt;/em&gt;, pertenezcan al usuario &lt;em&gt;root&lt;/em&gt; y que solo este tenga permisos de escritura. Aunque nos saldría un mensaje de error en el &lt;em&gt;log&lt;/em&gt; del &lt;strong&gt;SSH&lt;/strong&gt;, cuesta poco de comprobarlo antes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@sshserver:~# &lt;span class="k"&gt;for&lt;/span&gt; folder in / /usr /usr/bin /usr/bin/authorized_keys_by_user.sh&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt; stat --printf &lt;span class="s2"&gt;&amp;quot;%U:%G\t%A %n\n&amp;quot;&lt;/span&gt; &lt;span class="nv"&gt;$folder&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;done&lt;/span&gt;
root:root   drwxr-xr-x /
root:root   drwxr-xr-x /usr
root:root   drwxr-xr-x /usr/bin
root:root   -rwxr-xr-x /usr/bin/authorized_keys_by_user.sh
root@sshserver:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El truco consiste en indicar dos directivas al demonio &lt;strong&gt;SSH&lt;/strong&gt;, para que sepa que debe ejecutar este &lt;em&gt;script&lt;/em&gt; para sacar el &lt;em&gt;authorized_keys&lt;/em&gt; de cada usuario.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@sshserver:~# tail -2 /etc/ssh/sshd_config 
AuthorizedKeysCommand /usr/bin/authorized_keys_by_user.sh
AuthorizedKeysCommandUser nobody
root@sshserver:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Recargamos la configuración del demonio &lt;strong&gt;SSH&lt;/strong&gt;, para que relea la configuración nueva que acabamos de poner.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@sshserver:~# service ssh reload
Reloading OpenBSD Secure Shell server&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;s configuration: sshd.
root@sshserver:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y solo nos queda comprobar que podemos entrar con el usuario &lt;em&gt;guest&lt;/em&gt; usando la clave privada. En este ejemplo no se indica porque la clave &lt;em&gt;.ssh/id_rsa&lt;/em&gt; es ofrecida por defecto, y es la parte privada de la clave que pusimos en el &lt;em&gt;script&lt;/em&gt; remoto.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/docker/ssh$ ssh guest@sshserver

The programs included with the Debian GNU/Linux system are free software&lt;span class="p"&gt;;&lt;/span&gt;
the exact distribution terms &lt;span class="k"&gt;for&lt;/span&gt; each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Thu Jul &lt;span class="m"&gt;21&lt;/span&gt; &lt;span class="m"&gt;08&lt;/span&gt;:39:21 &lt;span class="m"&gt;2016&lt;/span&gt; from &lt;span class="m"&gt;172&lt;/span&gt;.20.0.1
guest@sshserver:~$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A partir de aquí, podéis modificar el &lt;em&gt;script&lt;/em&gt; remoto para que saque la información de las claves de algún sitio centralizado (LDAP, base de datos, &lt;em&gt;webservice&lt;/em&gt;, ...).&lt;/p&gt;</content><category term="ssh"></category><category term="autenticación"></category><category term="password"></category><category term="passphrase"></category><category term="centralizado"></category></entry><entry><title>Desactivando nuestras APIs con un frontal nginx</title><link href="https://www.linuxsysadmin.ml/2017/07/desactivando-nuestras-apis-con-un-frontal-nginx.html" rel="alternate"></link><published>2017-07-10T10:00:00+02:00</published><updated>2017-07-10T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-07-10:/2017/07/desactivando-nuestras-apis-con-un-frontal-nginx.html</id><summary type="html">&lt;p&gt;El otro día recibí una petición algo atípica en mi trabajo: querían activar y desactivar en un único punto centralizado cada una de las varias APIs que tenemos. Se trata de poner un &lt;strong&gt;nginx&lt;/strong&gt; frontal que gestione los &lt;em&gt;virtualhosts&lt;/em&gt; existentes y haga &lt;em&gt;proxy_pass&lt;/em&gt; o no en función de un &lt;em&gt;flag …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;El otro día recibí una petición algo atípica en mi trabajo: querían activar y desactivar en un único punto centralizado cada una de las varias APIs que tenemos. Se trata de poner un &lt;strong&gt;nginx&lt;/strong&gt; frontal que gestione los &lt;em&gt;virtualhosts&lt;/em&gt; existentes y haga &lt;em&gt;proxy_pass&lt;/em&gt; o no en función de un &lt;em&gt;flag&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;La propuesta me pareció bastante interesante, así que decidí hacer una prueba de concepto que aquí queda reflejada. Se trata de ofrecer un frontal web (por ejemplo un &lt;strong&gt;jenkins&lt;/strong&gt;) mediante el cual se puedan poner los ficheros cuya presencia le indican a &lt;strong&gt;nginx&lt;/strong&gt; si ese &lt;em&gt;virtualhost&lt;/em&gt; concreto debe dar un error o no.&lt;/p&gt;
&lt;h2&gt;Simulando las APIs&lt;/h2&gt;
&lt;p&gt;Vamos a poner un par de contenedores &lt;strong&gt;docker&lt;/strong&gt; con &lt;strong&gt;nginx&lt;/strong&gt;, que sirvan una página personalizada y nos sirva para simular la API. No es especialmente complejo, así que solo se adjunta por completitud.&lt;/p&gt;
&lt;p&gt;el único punto interesante es que, para no repetirnos, vamos a pasar el contenido del fichero &lt;em&gt;index.html&lt;/em&gt; como una variable de entorno. Así no hay que construir varias imágenes.&lt;/p&gt;
&lt;p&gt;Empezamos con un &lt;em&gt;Dockerfile&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gw-poc$ cat api/Dockerfile 
FROM alpine:3.5
RUN apk add --no-cache nginx &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    ln -s /dev/stdout /var/log/nginx/access.log &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    ln -s /dev/stderr /var/log/nginx/error.log &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    mkdir /run/nginx &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    mkdir /srv/www &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /etc/nginx/conf.d/default.conf
COPY nginx.conf /etc/nginx/
COPY conf.d/* /etc/nginx/conf.d/
COPY start.sh /
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/start.sh&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@aldebaran:~/docker/gw-poc$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y lo acompañamos con sus ficheros auxiliares.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gw-poc$ cat api/nginx.conf 
worker_processes  &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
events &lt;span class="o"&gt;{&lt;/span&gt;
    worker_connections  &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
http &lt;span class="o"&gt;{&lt;/span&gt;
    include mime.types&lt;span class="p"&gt;;&lt;/span&gt;
    default_type application/octet-stream&lt;span class="p"&gt;;&lt;/span&gt;
    sendfile on&lt;span class="p"&gt;;&lt;/span&gt;
    keepalive_timeout &lt;span class="m"&gt;65&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    include conf.d/*&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@aldebaran:~/docker/gw-poc$ cat api/conf.d/api 
server &lt;span class="o"&gt;{&lt;/span&gt;
    server_name _&lt;span class="p"&gt;;&lt;/span&gt;
    listen &lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    root /srv/www&lt;span class="p"&gt;;&lt;/span&gt;
    index index.html&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@aldebaran:~/docker/gw-poc$ cat api/start.sh 
&lt;span class="c1"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;GREETING&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; &amp;gt; /srv/www/index.html
&lt;span class="nb"&gt;exec&lt;/span&gt; /usr/sbin/nginx -g &lt;span class="s2"&gt;&amp;quot;daemon off;&amp;quot;&lt;/span&gt;
gerard@aldebaran:~/docker/gw-poc$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto tenemos nuestra imagen lista para ser construida.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gw-poc$ dbuild -t api api/
Sending build context to Docker daemon &lt;span class="m"&gt;5&lt;/span&gt;.632 kB
...  
Successfully built c91adbf6534e
gerard@aldebaran:~/docker/gw-poc$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Creando un proxy como fachada&lt;/h2&gt;
&lt;p&gt;Esta es la piedra angular de la solución. Vamos a empezar con un &lt;strong&gt;nginx&lt;/strong&gt;, pero la novedad es que cada &lt;em&gt;virtualhost&lt;/em&gt; va a incluir una condición nueva: si existe un fichero con el mismo nombre que el dominio en la carpeta raíz (la misma para todos los dominios vale), devolveremos un error 503 en JSON sin pasar la petición a nuestro &lt;em&gt;backend&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gw-poc$ cat gw/Dockerfile 
FROM alpine:3.5
RUN apk add --no-cache nginx &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    ln -s /dev/stdout /var/log/nginx/access.log &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    ln -s /dev/stderr /var/log/nginx/error.log &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    mkdir /run/nginx &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    mkdir /srv/www &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /etc/nginx/conf.d/default.conf
COPY nginx.conf /etc/nginx/
COPY conf.d/* /etc/nginx/conf.d/
COPY start.sh /
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/start.sh&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@aldebaran:~/docker/gw-poc$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y también ponemos las configuraciones necesarias para dos APIs de &lt;em&gt;backend&lt;/em&gt;, que nos basta para ver si funciona.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gw-poc$ cat gw/nginx.conf 
worker_processes  &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
events &lt;span class="o"&gt;{&lt;/span&gt;
    worker_connections  &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
http &lt;span class="o"&gt;{&lt;/span&gt;
    include mime.types&lt;span class="p"&gt;;&lt;/span&gt;
    default_type application/octet-stream&lt;span class="p"&gt;;&lt;/span&gt;
    sendfile on&lt;span class="p"&gt;;&lt;/span&gt;
    keepalive_timeout &lt;span class="m"&gt;65&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    include conf.d/*&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@aldebaran:~/docker/gw-poc$ cat gw/conf.d/api1 
server &lt;span class="o"&gt;{&lt;/span&gt;
    server_name api1&lt;span class="p"&gt;;&lt;/span&gt;
    listen &lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    root /srv/www&lt;span class="p"&gt;;&lt;/span&gt;

    location / &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;-f &lt;span class="nv"&gt;$document_root&lt;/span&gt;/&lt;span class="nv"&gt;$host&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="m"&gt;503&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
        proxy_pass http://api1:80&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    error_page &lt;span class="m"&gt;503&lt;/span&gt; @maintenance&lt;span class="p"&gt;;&lt;/span&gt;

    location @maintenance &lt;span class="o"&gt;{&lt;/span&gt;
        default_type application/json&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="m"&gt;503&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{&amp;quot;message&amp;quot;:&amp;quot;Sorry you! This entity (api1) is in maintenance mode&amp;quot;}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@aldebaran:~/docker/gw-poc$ cat gw/conf.d/api2 
server &lt;span class="o"&gt;{&lt;/span&gt;
    server_name api2&lt;span class="p"&gt;;&lt;/span&gt;
    listen &lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    root /srv/www&lt;span class="p"&gt;;&lt;/span&gt;

    location / &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;-f &lt;span class="nv"&gt;$document_root&lt;/span&gt;/&lt;span class="nv"&gt;$host&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="m"&gt;503&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
        proxy_pass http://api2:80&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    error_page &lt;span class="m"&gt;503&lt;/span&gt; @maintenance&lt;span class="p"&gt;;&lt;/span&gt;

    location @maintenance &lt;span class="o"&gt;{&lt;/span&gt;
        default_type application/json&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="m"&gt;503&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{&amp;quot;message&amp;quot;:&amp;quot;Sorry you! This entity (api2) is in maintenance mode&amp;quot;}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@aldebaran:~/docker/gw-poc$ cat gw/start.sh 
&lt;span class="c1"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="nb"&gt;exec&lt;/span&gt; /usr/sbin/nginx -g &lt;span class="s2"&gt;&amp;quot;daemon off;&amp;quot;&lt;/span&gt;
gerard@aldebaran:~/docker/gw-poc$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y construimos la imagen.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gw-poc$ dbuild -t gw gw/
Sending build context to Docker daemon &lt;span class="m"&gt;6&lt;/span&gt;.656 kB
...
Successfully built ddf3f294c99c
gerard@aldebaran:~/docker/gw-poc$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Ponemos todo junto&lt;/h2&gt;
&lt;p&gt;El último paso es montar un entorno que nos permita lanzar pruebas y ver que funciona. Para ello, vamos a usar &lt;strong&gt;docker-compose&lt;/strong&gt; por la comodidad que supone.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gw-poc$ cat docker-compose.yml 
version: &lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;
services:
  gw:
    image: gw
    container_name: gw
    hostname: gw
    volumes:
      - ./volume:/srv/www
    ports:
      - &lt;span class="s2"&gt;&amp;quot;80:80&amp;quot;&lt;/span&gt;
    depends_on:
      - api1
      - api2
  api1:
    image: api
    container_name: api1
    hostname: api1
    environment:
      GREETING: &lt;span class="s2"&gt;&amp;quot;Hello from api1&amp;quot;&lt;/span&gt;
  api2:
    image: api
    container_name: api2
    hostname: api2
    environment:
      GREETING: &lt;span class="s2"&gt;&amp;quot;Bye from api2&amp;quot;&lt;/span&gt;
gerard@aldebaran:~/docker/gw-poc$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Levantamos el entorno entero con los comandos habituales:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gw-poc$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;gwpoc_default&amp;quot;&lt;/span&gt; with the default driver
Creating api1
Creating api2
Creating gw
gerard@aldebaran:~/docker/gw-poc$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Tras levantar el entorno, podemos hacer algunas peticiones, tanto a los contenedores que sirven la API, como al &lt;em&gt;gateway&lt;/em&gt; que las engloba.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gw-poc$ curl http://172.18.0.2/
Hello from api1
gerard@aldebaran:~/docker/gw-poc$ curl http://172.18.0.3/
Bye from api2
gerard@aldebaran:~/docker/gw-poc$ curl -H &lt;span class="s2"&gt;&amp;quot;Host: api1&amp;quot;&lt;/span&gt; http://localhost/
Hello from api1
gerard@aldebaran:~/docker/gw-poc$ curl -H &lt;span class="s2"&gt;&amp;quot;Host: api2&amp;quot;&lt;/span&gt; http://localhost/
Bye from api2
gerard@aldebaran:~/docker/gw-poc$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y ahora solo nos falta la magia: tiramos un fichero en &lt;em&gt;/srv/www/&lt;/em&gt;, cómodamente mapeados como un &lt;em&gt;host volume&lt;/em&gt; en la carpeta &lt;em&gt;volume/&lt;/em&gt;. Un fichero con el nombre del &lt;em&gt;virtualhost&lt;/em&gt; va a deshabilitar dicho &lt;em&gt;virtualhost&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gw-poc$ touch volume/api2
gerard@aldebaran:~/docker/gw-poc$ curl -H &lt;span class="s2"&gt;&amp;quot;Host: api1&amp;quot;&lt;/span&gt; http://localhost/
Hello from api1
gerard@aldebaran:~/docker/gw-poc$ curl -H &lt;span class="s2"&gt;&amp;quot;Host: api2&amp;quot;&lt;/span&gt; http://localhost/
&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;message&amp;quot;&lt;/span&gt;:&lt;span class="s2"&gt;&amp;quot;Sorry you! This entity (api2) is in maintenance mode&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
gerard@aldebaran:~/docker/gw-poc$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;De la misma manera, podemos rehabilitarlo quitando ese fichero de ahí.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gw-poc$ rm volume/api2 
gerard@aldebaran:~/docker/gw-poc$ curl -H &lt;span class="s2"&gt;&amp;quot;Host: api1&amp;quot;&lt;/span&gt; http://localhost/
Hello from api1
gerard@aldebaran:~/docker/gw-poc$ curl -H &lt;span class="s2"&gt;&amp;quot;Host: api2&amp;quot;&lt;/span&gt; http://localhost/
Bye from api2
gerard@aldebaran:~/docker/gw-poc$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y lo mismo aplica para el primer dominio, aunque no lo repito por brevedad.&lt;/p&gt;
&lt;h2&gt;Siguientes pasos&lt;/h2&gt;
&lt;p&gt;El hecho de habilitar y deshabilitar las APIs se necesitaba hacer por parte de gente que no tiene necesariamente conocimientos técnicos para acceder al entorno, o no queremos simplemente por seguridad. La solución cómoda es una bonita interfaz web que les permita hacerlo a golpe de click y con una gestión de permisos adecuada ya incorporada.&lt;/p&gt;
&lt;p&gt;Como no queremos inventar la rueda nuevamente, podemos usar algo que ya esté hecho, como por ejemplo un &lt;strong&gt;jenkins&lt;/strong&gt;. De hecho, nada nos impide que el &lt;strong&gt;jenkins&lt;/strong&gt; lance &lt;em&gt;playbooks&lt;/em&gt; de &lt;strong&gt;ansible&lt;/strong&gt;. Sin embargo este ya es otro proyecto y en caso de que os interese, &lt;a href="https://www.linuxsysadmin.ml/2016/09/lanzando-playbooks-de-ansible-desde-jenkins.html"&gt;ya he escrito sobre esto&lt;/a&gt;.&lt;/p&gt;</content><category term="api"></category><category term="gateway"></category><category term="proxy"></category><category term="nginx"></category></entry><entry><title>El concepto del servidor fachada con Docker</title><link href="https://www.linuxsysadmin.ml/2017/07/el-concepto-del-servidor-fachada-con-docker.html" rel="alternate"></link><published>2017-07-03T10:00:00+02:00</published><updated>2017-07-03T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-07-03:/2017/07/el-concepto-del-servidor-fachada-con-docker.html</id><summary type="html">&lt;p&gt;Muchos de nosotros tenemos un servidor en casa o en algún &lt;em&gt;hosting&lt;/em&gt;. Como no tenemos mucho tráfico y cada servidor tiene un coste, acabamos llenándolo con un conjunto de servicios bastante grande. Esto supone un problema para actualizar el sistema operativo, suponiendo que los servicios no se molesten entre sí …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Muchos de nosotros tenemos un servidor en casa o en algún &lt;em&gt;hosting&lt;/em&gt;. Como no tenemos mucho tráfico y cada servidor tiene un coste, acabamos llenándolo con un conjunto de servicios bastante grande. Esto supone un problema para actualizar el sistema operativo, suponiendo que los servicios no se molesten entre sí.&lt;/p&gt;
&lt;p&gt;En estos casos podemos valernos de &lt;strong&gt;docker&lt;/strong&gt; (o de cualquier otro sistema de contenedores) para aislar cada servicio en su propio contenedor y para facilitar su portabilidad hacia un nuevo servidor. Con un poco de habilidad con reglas de &lt;em&gt;networking&lt;/em&gt;, podemos hacer esta transición sin cortes y poco a poco.&lt;/p&gt;
&lt;p&gt;El truco es utilizar el concepto &lt;strong&gt;fachada&lt;/strong&gt;, es decir, nuestro servidor es solo la fachada de cada una de nuestros contenedores. Estos exponen su servicio como un puerto en la máquina &lt;em&gt;host&lt;/em&gt; y así parece que el &lt;em&gt;host&lt;/em&gt; es un único servidor. Este &lt;em&gt;host&lt;/em&gt; también nos puede servir para albergar los &lt;em&gt;host volumes&lt;/em&gt; y para hacer tareas de mantenimiento tales como &lt;em&gt;backups&lt;/em&gt; o &lt;em&gt;logrotate&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Un ejemplo práctico&lt;/h2&gt;
&lt;p&gt;Supongamos que queremos un servidor casero con 3 servicios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Un servidor &lt;strong&gt;mariadb&lt;/strong&gt; y su interfaz de administración web &lt;strong&gt;adminer&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Un servidor de &lt;strong&gt;mongodb&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Un servidor web &lt;strong&gt;nginx&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;En vez de instalarlo todo en nuestro servidor, vamos a aplicar la técnica antes descrita, de mapear en los puertos oficiales los puertos de los contenedores que ejecutan los servicios. Para simplificar el artículo, vamos a utilizar las imágenes oficiales en &lt;em&gt;DockerHub&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Para su fácil lanzamiento, vamos a usar &lt;strong&gt;docker-compose&lt;/strong&gt;, que nos simplifica bastante la línea de comandos, ocultando en el fichero &lt;em&gt;docker-compose.yml&lt;/em&gt; cosas como las variables de entorno, el mapeo de puertos o los volúmenes.&lt;/p&gt;
&lt;h3&gt;MariaDB y Adminer&lt;/h3&gt;
&lt;p&gt;Necesitaremos un &lt;em&gt;docker-compose.yml&lt;/em&gt; para levantar los contenedores de &lt;strong&gt;mariadb&lt;/strong&gt; y &lt;strong&gt;adminer&lt;/strong&gt;. En el caso de &lt;strong&gt;mariadb&lt;/strong&gt;, tiene una parte de datos persistentes, que vamos a dejar como un &lt;em&gt;host volume&lt;/em&gt; local.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/homeserver$ tree mariadb/
mariadb/
├── data
└── docker-compose.yml

&lt;span class="m"&gt;1&lt;/span&gt; directory, &lt;span class="m"&gt;1&lt;/span&gt; file
gerard@aldebaran:~/docker/homeserver$ cat mariadb/docker-compose.yml 
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  mariadb:
    image: mariadb
    container_name: mariadb
    hostname: mariadb
    volumes:
      - ./data:/var/lib/mysql
    environment:
      MYSQL_ROOT_PASSWORD: root1234
    ports:
      - &lt;span class="s2"&gt;&amp;quot;3306:3306&amp;quot;&lt;/span&gt;
  adminer:
    image: adminer
    container_name: adminer
    hostname: adminer
    ports:
      - &lt;span class="m"&gt;8080&lt;/span&gt;:8080
gerard@aldebaran:~/docker/homeserver$ docker-compose -f mariadb/docker-compose.yml up -d
Creating network &lt;span class="s2"&gt;&amp;quot;mariadb_default&amp;quot;&lt;/span&gt; with the default driver
Creating mariadb
Creating adminer
gerard@aldebaran:~/docker/homeserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;MongoDB&lt;/h3&gt;
&lt;p&gt;De forma análoga, vamos a usar un &lt;em&gt;docker-compose.yml&lt;/em&gt;, mapeando el puerto de &lt;strong&gt;mongodb&lt;/strong&gt; y su carpeta de datos en el &lt;em&gt;host&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/homeserver$ tree mongodb/
mongodb/
├── data
└── docker-compose.yml

&lt;span class="m"&gt;1&lt;/span&gt; directory, &lt;span class="m"&gt;1&lt;/span&gt; file
gerard@aldebaran:~/docker/homeserver$ cat mongodb/docker-compose.yml 
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  mongodb:
    image: mongo
    container_name: mongodb
    hostname: mongodb
    volumes:
      - ./data:/data/db
    ports:
      - &lt;span class="s2"&gt;&amp;quot;27017:27017&amp;quot;&lt;/span&gt;
gerard@aldebaran:~/docker/homeserver$ docker-compose -f mongodb/docker-compose.yml up -d
Creating network &lt;span class="s2"&gt;&amp;quot;mongodb_default&amp;quot;&lt;/span&gt; with the default driver
Creating mongodb
gerard@aldebaran:~/docker/homeserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Nginx&lt;/h3&gt;
&lt;p&gt;Y volvemos a repetir el proceso; un &lt;em&gt;docker-compose.yml&lt;/em&gt;, un puerto mapeado, y un &lt;em&gt;host volume&lt;/em&gt; para albergar el contenido web.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/homeserver$ tree nginx/
nginx/
├── www
│   └── index.html
└── docker-compose.yml

&lt;span class="m"&gt;1&lt;/span&gt; directory, &lt;span class="m"&gt;2&lt;/span&gt; files
gerard@aldebaran:~/docker/homeserver$ cat nginx/docker-compose.yml 
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  nginx:
    image: nginx
    container_name: nginx
    hostname: nginx
    volumes:
      - ./www:/usr/share/nginx/html:ro
    ports:
      - &lt;span class="s2"&gt;&amp;quot;80:80&amp;quot;&lt;/span&gt;
gerard@aldebaran:~/docker/homeserver$ docker-compose -f nginx/docker-compose.yml up -d
Creating network &lt;span class="s2"&gt;&amp;quot;nginx_default&amp;quot;&lt;/span&gt; with the default driver
Creating nginx
gerard@aldebaran:~/docker/homeserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;El resultado&lt;/h3&gt;
&lt;p&gt;Si miramos los puertos abiertos en nuestro servidor, podemos ver fácilmente que responde los 4 puertos que suministran los servicios antes citados, y nada nos impide seguir creando servicios para ofrecer más puertos en nuestro servidor. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/homeserver$ netstat -lnt
Active Internet connections &lt;span class="o"&gt;(&lt;/span&gt;only servers&lt;span class="o"&gt;)&lt;/span&gt;
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp6       &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; :::27017                :::*                    LISTEN     
tcp6       &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; :::3306                 :::*                    LISTEN     
tcp6       &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; :::80                   :::*                    LISTEN     
tcp6       &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; :::8080                 :::*                    LISTEN     
gerard@aldebaran:~/docker/homeserver$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En caso de querer actualizar el sistema operativo base, solo tenemos que crear un nuevo servidor y levantar los contenedores de servicio, uno por uno; si usamos algún elemento de red como un &lt;em&gt;firewall&lt;/em&gt;, podemos desviar tráfico sin que se note, hasta que estemos preparados para reemplazar el servidor viejo con el nuevo.&lt;/p&gt;
&lt;p&gt;Al tratarse de contenedores individuales, lo que pase en un contendedor no va a interferir en lo que pase en otro, ganando así el concepto de aislamiento, pudiendo convivir varias versiones de un mismo &lt;em&gt;software&lt;/em&gt; o diversos servicios que ofrezcan el mismo protocolo. solo hay que tener en cuenta que los puertos mapeados en el &lt;em&gt;host&lt;/em&gt; deben ser únicos.&lt;/p&gt;</content><category term="docker"></category><category term="fachada"></category><category term="docker-compose"></category></entry><entry><title>Un servidor de git local con gitolite</title><link href="https://www.linuxsysadmin.ml/2017/06/un-servidor-de-git-local-con-gitolite.html" rel="alternate"></link><published>2017-06-26T10:00:00+02:00</published><updated>2017-06-26T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-06-26:/2017/06/un-servidor-de-git-local-con-gitolite.html</id><summary type="html">&lt;p&gt;A todos nos encanta el sistema de control de versiones &lt;strong&gt;git&lt;/strong&gt;. Tanto a nivel local como a nivel público en &lt;em&gt;GitHub&lt;/em&gt; es una maravilla; lo que no me gusta tanto es el precio que suelen tener las soluciones privadas. Sin embargo, y con un poco de habilidad, podemos encontrar alternativas …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A todos nos encanta el sistema de control de versiones &lt;strong&gt;git&lt;/strong&gt;. Tanto a nivel local como a nivel público en &lt;em&gt;GitHub&lt;/em&gt; es una maravilla; lo que no me gusta tanto es el precio que suelen tener las soluciones privadas. Sin embargo, y con un poco de habilidad, podemos encontrar alternativas.&lt;/p&gt;
&lt;p&gt;Existen varias alternativas tipo web, como por ejemplo &lt;a href="https://github.com/gitlabhq/gitlabhq"&gt;GitLab&lt;/a&gt; (imagen para &lt;strong&gt;docker&lt;/strong&gt; en &lt;a href="https://hub.docker.com/r/gitlab/gitlab-ce/"&gt;DockerHub&lt;/a&gt;); sin embargo, como amante del terminal me decanto por &lt;a href="http://gitolite.com/gitolite/index.html"&gt;Gitolite&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Además de las virtudes propias de &lt;strong&gt;git&lt;/strong&gt;, &lt;strong&gt;gitolite&lt;/strong&gt; nos ofrece un sistema de control de permisos en los repositorios bajo su administración, usando un usuario remoto SSH único y diferenciando quien es el usuario mediante la clave SSH que use para establecer la conexión.&lt;/p&gt;
&lt;p&gt;Otro punto interesante es que el servidor (usuarios, repositorios y permisos) se administra mediante &lt;strong&gt;git&lt;/strong&gt;, existiendo el usuario &lt;em&gt;admin&lt;/em&gt; con permisos sobre el repositorio &lt;em&gt;gitolite-admin&lt;/em&gt;. Este tiene la responsabilidad de clonar el repositorio, añadir los cambios y empujarlos con un &lt;em&gt;git push&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Montando el servidor&lt;/h2&gt;
&lt;p&gt;Como viene siendo tradición, vamos a aislar nuestro servicio de &lt;strong&gt;gitolite&lt;/strong&gt; en un contenedor &lt;strong&gt;docker&lt;/strong&gt;. Para ello vamos a utilizar una base de &lt;em&gt;Alpine Linux&lt;/em&gt; que nos va a dar un conjunto de paquetes bastante actualizados, a un tamaño bastante pequeño.&lt;/p&gt;
&lt;p&gt;Vamos a crear una imagen y le vamos a poner un &lt;em&gt;tag&lt;/em&gt; para diferenciarla del resto, por ejemplo, &lt;em&gt;gitolite&lt;/em&gt;. Aquí os paso el contexto para su construcción:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gitolite$ cat Dockerfile 
FROM alpine:3.5
RUN apk add --no-cache openssh gitolite &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    passwd -u git
COPY start.sh /
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/start.sh&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@aldebaran:~/docker/gitolite$ cat start.sh 
&lt;span class="c1"&gt;#!/bin/sh&lt;/span&gt;

ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key -N &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$SSH_PUBKEY&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt; /tmp/admin.pub
su git -c &lt;span class="s2"&gt;&amp;quot;gitolite setup -pk /tmp/admin.pub&amp;quot;&lt;/span&gt;
rm /tmp/admin.pub

&lt;span class="nb"&gt;exec&lt;/span&gt; /usr/sbin/sshd -D -e
gerard@aldebaran:~/docker/gitolite$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El &lt;em&gt;script&lt;/em&gt; de inicialización &lt;em&gt;start.sh&lt;/em&gt; va a iniciar el demonio de SSH, no sin antes generar las claves de &lt;em&gt;host&lt;/em&gt; nuevas e inicializar &lt;strong&gt;gitolite&lt;/strong&gt;. Un pequeño detalle interesante es que &lt;strong&gt;gitolite&lt;/strong&gt; exige una clave pública SSH para que el usuario &lt;em&gt;admin&lt;/em&gt; pueda modificar el repositorio de administración; por comodidad la vamos a pasar mediante la variable de entorno &lt;em&gt;SSH_PUBKEY&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;La forma más fácil de levantar el servidor es con &lt;strong&gt;docker compose&lt;/strong&gt;, y aunque este varia según vuestros gustos personales, yo he usado algo así:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gitolite$ cat docker-compose.yml 
version: &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;
services:
  gitolite:
    image: gitolite
    container_name: gitolite
    hostname: gitolite
    environment:
      SSH_PUBKEY: &lt;span class="s2"&gt;&amp;quot;ssh-rsa ...&amp;quot;&lt;/span&gt;
    ports:
      - &lt;span class="s2"&gt;&amp;quot;22:22&amp;quot;&lt;/span&gt;
gerard@aldebaran:~/docker/gitolite$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Para evitar indicar el usuario, la dirección IP y la clave SSH a usar, podemos definir algunos &lt;em&gt;hosts&lt;/em&gt; en el fichero &lt;em&gt;~/.ssh/config&lt;/em&gt;, que también nos va a ser útil en el momento de las operaciones &lt;strong&gt;git&lt;/strong&gt; remotas, que no aceptan parámetros.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~$ cat .ssh/config 
...
Host gitolite-admin
    Hostname &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1
    User git
    IdentityFile ~/docker/gitolite/keys/admin

Host gitolite-gerard
    Hostname &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1
    User git
    IdentityFile ~/docker/gitolite/keys/gerard
gerard@aldebaran:~$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Administrando gitolite&lt;/h2&gt;
&lt;p&gt;Como ya se ha indicado, el usuario &lt;em&gt;admin&lt;/em&gt; debe clonar el repositorio &lt;em&gt;gitolite-admin&lt;/em&gt; para editar los cambios. En principio es el único usuario y tiene permisos sobre el repositorio mencionado. Podemos ver sus permisos intentando entrar al servidor por SSH (recordad que lo he mapeado al puerto 22 de mi máquina).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gitolite/workspace$ ssh -i ../keys/admin git@localhost
Warning: Permanently added &lt;span class="s1"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
PTY allocation request failed on channel &lt;span class="m"&gt;0&lt;/span&gt;
hello admin, this is git@gitolite running gitolite3 v3.4.0-4380-g8bd1571 on git &lt;span class="m"&gt;2&lt;/span&gt;.11.2

 R W    gitolite-admin
 R W    testing
Connection to localhost closed.
gerard@aldebaran:~/docker/gitolite/workspace$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Alternativamente podemos usar el &lt;em&gt;host&lt;/em&gt; declarado en la configuración SSH cliente (el truco está más arriba):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gitolite/workspace$ ssh gitolite-admin
Warning: Permanently added &lt;span class="s1"&gt;&amp;#39;127.0.0.1&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
PTY allocation request failed on channel &lt;span class="m"&gt;0&lt;/span&gt;
hello admin, this is git@gitolite running gitolite3 v3.4.0-4380-g8bd1571 on git &lt;span class="m"&gt;2&lt;/span&gt;.11.2

 R W    gitolite-admin
 R W    testing
Connection to &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1 closed.
gerard@aldebaran:~/docker/gitolite/workspace$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La sesión se cierra, ya que la función de este SSH es solamente hacer las operaciones remotas de &lt;em&gt;clone&lt;/em&gt;, &lt;em&gt;pull&lt;/em&gt; y &lt;em&gt;push&lt;/em&gt;. Cualquier otro usuario va a fallar si intenta entrar al servidor de la misma forma, ya que no hay nadie más autorizado.&lt;/p&gt;
&lt;p&gt;Para realizar modificaciones tenemos que clonar el repositorio de administración:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gitolite/workspace$ git clone gitolite-admin:gitolite-admin.git
Cloning into &lt;span class="s1"&gt;&amp;#39;gitolite-admin&amp;#39;&lt;/span&gt;...
Warning: Permanently added &lt;span class="s1"&gt;&amp;#39;127.0.0.1&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
remote: Counting objects: &lt;span class="m"&gt;6&lt;/span&gt;, &lt;span class="k"&gt;done&lt;/span&gt;.
remote: Compressing objects: &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;/4&lt;span class="o"&gt;)&lt;/span&gt;, &lt;span class="k"&gt;done&lt;/span&gt;.
remote: Total &lt;span class="m"&gt;6&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;delta &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;, reused &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;delta &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
Receiving objects: &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;/6&lt;span class="o"&gt;)&lt;/span&gt;, &lt;span class="k"&gt;done&lt;/span&gt;.
Checking connectivity... &lt;span class="k"&gt;done&lt;/span&gt;.
gerard@aldebaran:~/docker/gitolite/workspace$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Esto nos da el repositorio de administración, que de por sí, es bastante intuitivo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ tree
.
├── conf
│   └── gitolite.conf
└── keydir
    └── admin.pub

&lt;span class="m"&gt;2&lt;/span&gt; directories, &lt;span class="m"&gt;2&lt;/span&gt; files
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La carpeta &lt;em&gt;keydir&lt;/em&gt; es donde hay que poner las claves de los usuarios SSH para que puedan entrar en la máquina. Es importante poner el formato &lt;code&gt;&amp;lt;usuario&amp;gt;.pub&lt;/code&gt; donde &lt;code&gt;usuario&lt;/code&gt; es el usuario tal como lo conoce &lt;strong&gt;gitolite&lt;/strong&gt; y como hay que indicar en los permisos; da igual como se llama la clave privada en la máquina del usuario (por ejemplo, &lt;em&gt;id_rsa&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;El fichero &lt;em&gt;conf/gitolite.conf&lt;/em&gt; tiene la especificación de los repositorios y los permisos que tienen los usuarios sobre ellos.&lt;/p&gt;
&lt;h3&gt;Añadiendo usuarios&lt;/h3&gt;
&lt;p&gt;Para añadir o retirar usuarios, basta con añadir o quitar su clave de la carpeta &lt;em&gt;keydir&lt;/em&gt; en nuestro repositorio local, para posteriormente hacer el correspondiente &lt;em&gt;push&lt;/em&gt;. Por ejemplo, añado la clave para mi usuario:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ tree
.
├── conf
│   └── gitolite.conf
└── keydir
    ├── admin.pub
    └── gerard.pub

&lt;span class="m"&gt;2&lt;/span&gt; directories, &lt;span class="m"&gt;3&lt;/span&gt; files
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ git add keydir/gerard.pub 
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ git commit -m &lt;span class="s2"&gt;&amp;quot;Add user gerard&amp;quot;&lt;/span&gt;
...
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos ver que ahora puedo usar el usuario &lt;em&gt;gerard&lt;/em&gt; para hacer SSH, pero que los repositorios a los que tiene acceso no son los mismos; de hecho, viene uno llamado &lt;em&gt;testing&lt;/em&gt; por defecto. Recordad que &lt;strong&gt;gitolite&lt;/strong&gt; decide el usuario en función de la clave SSH usada, y esta la he puesto en la configuración SSH cliente en &lt;em&gt;~/.ssh/config&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ ssh gitolite-admin
Warning: Permanently added &lt;span class="s1"&gt;&amp;#39;127.0.0.1&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
PTY allocation request failed on channel &lt;span class="m"&gt;0&lt;/span&gt;
hello admin, this is git@gitolite running gitolite3 v3.4.0-4380-g8bd1571 on git &lt;span class="m"&gt;2&lt;/span&gt;.11.2

 R W    gitolite-admin
 R W    testing
Connection to &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1 closed.
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ ssh gitolite-gerard
Warning: Permanently added &lt;span class="s1"&gt;&amp;#39;127.0.0.1&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
PTY allocation request failed on channel &lt;span class="m"&gt;0&lt;/span&gt;
hello gerard, this is git@gitolite running gitolite3 v3.4.0-4380-g8bd1571 on git &lt;span class="m"&gt;2&lt;/span&gt;.11.2

 R W    testing
Connection to &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1 closed.
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El punto interesante de todo esto es que todos los usuarios utilizan un &lt;em&gt;shell&lt;/em&gt; restringido, pero este acepta un parámetro, que es el usuario. Este parámetro es forzado por SSH cuando alguna de las líneas del fichero &lt;em&gt;authorized_keys&lt;/em&gt; da positivo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ docker &lt;span class="nb"&gt;exec&lt;/span&gt; gitolite cat /var/lib/git/.ssh/authorized_keys
&lt;span class="c1"&gt;# gitolite start&lt;/span&gt;
&lt;span class="nv"&gt;command&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/lib/gitolite/gitolite-shell admin&amp;quot;&lt;/span&gt;,no-port-forwarding,no-X11-forwarding,no-agent-forwarding,no-pty ssh-rsa ...  
&lt;span class="nv"&gt;command&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/lib/gitolite/gitolite-shell gerard&amp;quot;&lt;/span&gt;,no-port-forwarding,no-X11-forwarding,no-agent-forwarding,no-pty ssh-rsa ...  
&lt;span class="c1"&gt;# gitolite end&lt;/span&gt;
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Administrando repositorios y permisos&lt;/h3&gt;
&lt;p&gt;El fichero clave para esto es &lt;em&gt;conf/gitolite.conf&lt;/em&gt;. Si vemos lo que tiene, comprenderemos inmediatamente lo que hay que hacer.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ cat conf/gitolite.conf 
repo gitolite-admin
    RW+     &lt;span class="o"&gt;=&lt;/span&gt;   admin

repo testing
    RW+     &lt;span class="o"&gt;=&lt;/span&gt;   @all
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Se trata de repositorios (o grupos) con una lista tabulada de permisos y usuarios o grupos a los que afectan. Más información en &lt;a href="http://gitolite.com/gitolite/conf/"&gt;la documentación&lt;/a&gt;. El grupo &lt;em&gt;all&lt;/em&gt; es especial y viene predefinido.&lt;/p&gt;
&lt;p&gt;Vamos a poner algunos repositorios, grupos y permisos:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ cat conf/gitolite.conf 
@devs &lt;span class="o"&gt;=&lt;/span&gt; dev1 dev2 dev3
@ops &lt;span class="o"&gt;=&lt;/span&gt; ops1
@staff &lt;span class="o"&gt;=&lt;/span&gt; @devs @ops

@blog &lt;span class="o"&gt;=&lt;/span&gt;  blog-public blog-admin
@shop &lt;span class="o"&gt;=&lt;/span&gt; shop-public shop-admin shop-api

repo gitolite-admin
    RW+ &lt;span class="o"&gt;=&lt;/span&gt; admin

repo @blog
    RW+ &lt;span class="o"&gt;=&lt;/span&gt; gerard
    &lt;span class="nv"&gt;RW&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; @devs
    &lt;span class="nv"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; @ops

repo @shop
    RW+ &lt;span class="o"&gt;=&lt;/span&gt; @staff
    &lt;span class="nv"&gt;R&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; gerard
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ git add conf/gitolite.conf 
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ git commit -m &lt;span class="s2"&gt;&amp;quot;Added some projects and permissions&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;master f8fe801&lt;span class="o"&gt;]&lt;/span&gt; Added some projects and permissions
 &lt;span class="m"&gt;1&lt;/span&gt; file changed, &lt;span class="m"&gt;16&lt;/span&gt; insertions&lt;span class="o"&gt;(&lt;/span&gt;+&lt;span class="o"&gt;)&lt;/span&gt;, &lt;span class="m"&gt;3&lt;/span&gt; deletions&lt;span class="o"&gt;(&lt;/span&gt;-&lt;span class="o"&gt;)&lt;/span&gt;
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ git push
...  
Warning: Permanently added &lt;span class="s1"&gt;&amp;#39;127.0.0.1&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
Counting objects: &lt;span class="m"&gt;4&lt;/span&gt;, &lt;span class="k"&gt;done&lt;/span&gt;.
Delta compression using up to &lt;span class="m"&gt;4&lt;/span&gt; threads.
Compressing objects: &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;/3&lt;span class="o"&gt;)&lt;/span&gt;, &lt;span class="k"&gt;done&lt;/span&gt;.
Writing objects: &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;/4&lt;span class="o"&gt;)&lt;/span&gt;, &lt;span class="m"&gt;474&lt;/span&gt; bytes &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; bytes/s, &lt;span class="k"&gt;done&lt;/span&gt;.
Total &lt;span class="m"&gt;4&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;delta &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;, reused &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;delta &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
remote: Initialized empty Git repository in /var/lib/git/repositories/blog-admin.git/
remote: Initialized empty Git repository in /var/lib/git/repositories/blog-public.git/
remote: Initialized empty Git repository in /var/lib/git/repositories/shop-admin.git/
remote: Initialized empty Git repository in /var/lib/git/repositories/shop-api.git/
remote: Initialized empty Git repository in /var/lib/git/repositories/shop-public.git/
To gitolite-admin:gitolite-admin.git
   &lt;span class="m"&gt;5114865&lt;/span&gt;..f8fe801  master -&amp;gt; master
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y solo nos queda ver los permisos que tenemos ahora con los diferentes usuarios:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ ssh gitolite-admin
Warning: Permanently added &lt;span class="s1"&gt;&amp;#39;127.0.0.1&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
PTY allocation request failed on channel &lt;span class="m"&gt;0&lt;/span&gt;
hello admin, this is git@gitolite running gitolite3 v3.4.0-4380-g8bd1571 on git &lt;span class="m"&gt;2&lt;/span&gt;.11.2

 R W    gitolite-admin
Connection to &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1 closed.
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ ssh gitolite-gerard
Warning: Permanently added &lt;span class="s1"&gt;&amp;#39;127.0.0.1&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
PTY allocation request failed on channel &lt;span class="m"&gt;0&lt;/span&gt;
hello gerard, this is git@gitolite running gitolite3 v3.4.0-4380-g8bd1571 on git &lt;span class="m"&gt;2&lt;/span&gt;.11.2

 R W    blog-admin
 R W    blog-public
 R      shop-admin
 R      shop-api
 R      shop-public
Connection to &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1 closed.
gerard@aldebaran:~/docker/gitolite/workspace/gitolite-admin$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El repositorio &lt;em&gt;testing&lt;/em&gt; no aparece en la configuración, pero sus datos siguen en el servidor. En caso de querer eliminarlo definitivamente, necesitamos eliminar su carpeta entrando en el servidor.&lt;/p&gt;</content><category term="git"></category><category term="gitolite"></category></entry><entry><title>Generación fácil de certificados con easyrsa</title><link href="https://www.linuxsysadmin.ml/2017/05/generacion-facil-de-certificados-con-easyrsa.html" rel="alternate"></link><published>2017-05-29T10:00:00+02:00</published><updated>2017-05-29T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-05-29:/2017/05/generacion-facil-de-certificados-con-easyrsa.html</id><summary type="html">&lt;p&gt;Ya vimos en &lt;a href="https://www.linuxsysadmin.ml/2016/02/restringiendo-accesos-mediante-certificados-de-cliente.html"&gt;otro artículo&lt;/a&gt; como restringir los accesos a una web usando certificados SSL. Sin embargo, la generación de los mismos era un poco confusa. Sin embargo, existe una herramienta llamada &lt;strong&gt;easyrsa&lt;/strong&gt; que nos permite generar peticiones de forma fácil, firmarlas con nuestra CA y obtener el producto final …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ya vimos en &lt;a href="https://www.linuxsysadmin.ml/2016/02/restringiendo-accesos-mediante-certificados-de-cliente.html"&gt;otro artículo&lt;/a&gt; como restringir los accesos a una web usando certificados SSL. Sin embargo, la generación de los mismos era un poco confusa. Sin embargo, existe una herramienta llamada &lt;strong&gt;easyrsa&lt;/strong&gt; que nos permite generar peticiones de forma fácil, firmarlas con nuestra CA y obtener el producto final.&lt;/p&gt;
&lt;p&gt;Vamos a intentar seguir los pasos del citado artículo, solo en la parte de generación de los certificados. Si no se necesitara, se puede obviar la parte del certificado cliente, o porque no, generar certificados para decenas o cientos de usuarios de forma fácil.&lt;/p&gt;
&lt;p&gt;Vamos a partir de una distribución &lt;a href="https://alpinelinux.org/"&gt;Alpine Linux&lt;/a&gt;. Para los que no lo sospechen ya es un contenedor &lt;strong&gt;Docker&lt;/strong&gt; por la facilidad de crearlo y de destruirlo al acabar el artículo. Además, esta distribución nos ofrece la herramienta en la versión 3, que me ha parecido más intuitiva que la versión anterior. Asumo también que se dispone del paquete &lt;strong&gt;easy-rsa&lt;/strong&gt;, que se puede instalar con &lt;code&gt;apk add easy-rsa&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Preparando la CA&lt;/h2&gt;
&lt;p&gt;lo primero para hacer una CA es copiar la estructura base a cualquier sitio. La idea es que tenemos una copia para cada CA que tengamos en el servidor, y no quiero trabajar en las carpetas de sistema para no destruir la plantilla.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rsa:~# cp -R /usr/share/easy-rsa/* .
rsa:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Otro paso necesario es inicializar las estructura de PKI, que básicamente es crear un esqueleto de carpetas para contener nuestros ficheros.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rsa:~# easyrsa init-pki

init-pki complete&lt;span class="p"&gt;;&lt;/span&gt; you may now create a CA or requests.
Your newly created PKI dir is: /root/pki

rsa:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finalmente creamos los certificados y claves necesarios para la CA con un simple comando único.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rsa:~# easyrsa build-ca
Generating a &lt;span class="m"&gt;2048&lt;/span&gt; bit RSA private key
........................+++
...............................................+++
writing new private key to &lt;span class="s1"&gt;&amp;#39;/root/pki/private/ca.key.XXXXPPMHOb&amp;#39;&lt;/span&gt;
Enter PEM pass phrase:
Verifying - Enter PEM pass phrase:
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter &lt;span class="s1"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;, the field will be left blank.
-----
Common Name &lt;span class="o"&gt;(&lt;/span&gt;eg: your user, host, or server name&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;Easy-RSA CA&lt;span class="o"&gt;]&lt;/span&gt;:

CA creation &lt;span class="nb"&gt;complete&lt;/span&gt; and you may now import and sign cert requests.
Your new CA certificate file &lt;span class="k"&gt;for&lt;/span&gt; publishing is at:
/root/pki/ca.crt

rsa:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El resultado que deberemos exportar a nuestro servidor es el certificado de la CA, cuya localización es &lt;em&gt;/root/pki/ca.crt&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Generando el certificado de nuestro servidor&lt;/h2&gt;
&lt;p&gt;Este paso se debe repetir tantas veces como servidores queramos que utilicen un certificado SSL. De momento, nos basta con uno. Además, lo vamos a crear sin contraseña porque no queremos tener que introducirla cada vez que se reinicie el servidor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rsa:~# easyrsa gen-req private nopass
Generating a &lt;span class="m"&gt;2048&lt;/span&gt; bit RSA private key
......................................................+++
.+++
writing new private key to &lt;span class="s1"&gt;&amp;#39;/root/pki/private/private.key.XXXXhfGNmO&amp;#39;&lt;/span&gt;
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter &lt;span class="s1"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;, the field will be left blank.
-----
Common Name &lt;span class="o"&gt;(&lt;/span&gt;eg: your user, host, or server name&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;private&lt;span class="o"&gt;]&lt;/span&gt;:

Keypair and certificate request completed. Your files are:
req: /root/pki/reqs/private.req
key: /root/pki/private/private.key

rsa:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;IMPORTANTE&lt;/strong&gt;: el &lt;em&gt;Common Name&lt;/em&gt; es el parámetro mas importante; debe coincidir con el dominio para que se dé por bueno.&lt;/p&gt;
&lt;p&gt;Lo firmamos con nuestra CA y ya habremos acabado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rsa:~# easyrsa sign-req server private


You are about to sign the following certificate.
Please check over the details shown below &lt;span class="k"&gt;for&lt;/span&gt; accuracy. Note that this request
has not been cryptographically verified. Please be sure it came from a trusted
&lt;span class="nb"&gt;source&lt;/span&gt; or that you have verified the request checksum with the sender.

Request subject, to be signed as a server certificate &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="m"&gt;3650&lt;/span&gt; days:

&lt;span class="nv"&gt;subject&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;
    &lt;span class="nv"&gt;commonName&lt;/span&gt;                &lt;span class="o"&gt;=&lt;/span&gt; private


Type the word &lt;span class="s1"&gt;&amp;#39;yes&amp;#39;&lt;/span&gt; to &lt;span class="k"&gt;continue&lt;/span&gt;, or any other input to abort.
  Confirm request details: yes
Using configuration from /root/openssl-1.0.cnf
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; /root/pki/private/ca.key:
Check that the request matches the signature
Signature ok
The Subject&lt;span class="s1"&gt;&amp;#39;s Distinguished Name is as follows&lt;/span&gt;
&lt;span class="s1"&gt;commonName            :ASN.1 12:&amp;#39;&lt;/span&gt;private&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;
Certificate is to be certified &lt;span class="k"&gt;until&lt;/span&gt; Nov  &lt;span class="m"&gt;9&lt;/span&gt; &lt;span class="m"&gt;11&lt;/span&gt;:24:39 &lt;span class="m"&gt;2026&lt;/span&gt; GMT &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3650&lt;/span&gt; days&lt;span class="o"&gt;)&lt;/span&gt;

Write out database with &lt;span class="m"&gt;1&lt;/span&gt; new entries
Data Base Updated

Certificate created at: /root/pki/issued/private.crt

rsa:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a necesitar los ficheros &lt;em&gt;/root/pki/private/private.key&lt;/em&gt; y &lt;em&gt;/root/pki/private/private.crt&lt;/em&gt; para ponerlos en nuestro servidor, juntamente con el certificado de la CA.&lt;/p&gt;
&lt;h2&gt;Generando certificados cliente (opcional)&lt;/h2&gt;
&lt;p&gt;Se trata de la misma filosofía; generamos una &lt;em&gt;request&lt;/em&gt;, la firmamos y finalmente la vamos a empaquetar en un fichero &lt;em&gt;.p12&lt;/em&gt; para su fácil y segura distribución.&lt;/p&gt;
&lt;p&gt;Repetimos la generación de la &lt;em&gt;request&lt;/em&gt; de la misma manera:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rsa:~# easyrsa gen-req gerard
Generating a &lt;span class="m"&gt;2048&lt;/span&gt; bit RSA private key
............................................+++
...........+++
writing new private key to &lt;span class="s1"&gt;&amp;#39;/root/pki/private/gerard.key.XXXXfHJDkE&amp;#39;&lt;/span&gt;
Enter PEM pass phrase:
Verifying - Enter PEM pass phrase:
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter &lt;span class="s1"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;, the field will be left blank.
-----
Common Name &lt;span class="o"&gt;(&lt;/span&gt;eg: your user, host, or server name&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;gerard&lt;span class="o"&gt;]&lt;/span&gt;:

Keypair and certificate request completed. Your files are:
req: /root/pki/reqs/gerard.req
key: /root/pki/private/gerard.key

rsa:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Firmamos la petición. Es especialmente importante el parámetro &lt;em&gt;client&lt;/em&gt;, ya que sino, no va a funcionar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rsa:~# easyrsa sign-req client gerard


You are about to sign the following certificate.
Please check over the details shown below &lt;span class="k"&gt;for&lt;/span&gt; accuracy. Note that this request
has not been cryptographically verified. Please be sure it came from a trusted
&lt;span class="nb"&gt;source&lt;/span&gt; or that you have verified the request checksum with the sender.

Request subject, to be signed as a client certificate &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="m"&gt;3650&lt;/span&gt; days:

&lt;span class="nv"&gt;subject&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;
    &lt;span class="nv"&gt;commonName&lt;/span&gt;                &lt;span class="o"&gt;=&lt;/span&gt; gerard


Type the word &lt;span class="s1"&gt;&amp;#39;yes&amp;#39;&lt;/span&gt; to &lt;span class="k"&gt;continue&lt;/span&gt;, or any other input to abort.
  Confirm request details: yes
Using configuration from /root/openssl-1.0.cnf
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; /root/pki/private/ca.key:
Check that the request matches the signature
Signature ok
The Subject&lt;span class="s1"&gt;&amp;#39;s Distinguished Name is as follows&lt;/span&gt;
&lt;span class="s1"&gt;commonName            :ASN.1 12:&amp;#39;&lt;/span&gt;gerard&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;
Certificate is to be certified &lt;span class="k"&gt;until&lt;/span&gt; Nov  &lt;span class="m"&gt;9&lt;/span&gt; &lt;span class="m"&gt;11&lt;/span&gt;:42:10 &lt;span class="m"&gt;2026&lt;/span&gt; GMT &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3650&lt;/span&gt; days&lt;span class="o"&gt;)&lt;/span&gt;

Write out database with &lt;span class="m"&gt;1&lt;/span&gt; new entries
Data Base Updated

Certificate created at: /root/pki/issued/gerard.crt

rsa:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finalmente tenemos los ficheros &lt;em&gt;gerard.key&lt;/em&gt; y &lt;em&gt;gerard.crt&lt;/em&gt;, que no son lo que solemos importar en nuestro navegador. Para ellos lo empaquetamos en un fichero &lt;em&gt;gerard.12&lt;/em&gt; que está protegido por contraseña y es el que deberá importar el usuario de nuestra web.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rsa:~# openssl pkcs12 -export -in pki/issued/gerard.crt -inkey pki/private/gerard.key -out gerard.p12
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; pki/private/gerard.key:
Enter Export Password:
Verifying - Enter Export Password:
rsa:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Un ejemplo de servidor nginx funcional&lt;/h2&gt;
&lt;p&gt;Tenemos 3 ficheros para nuestro servidor, que son &lt;em&gt;ca.crt&lt;/em&gt;, &lt;em&gt;server.crt&lt;/em&gt; y &lt;em&gt;server.key&lt;/em&gt;, con los que podemos montar un dominio estándar, como en el artículo citado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cat /etc/nginx/sites-enabled/private.linuxsysadmin.tk
server &lt;span class="o"&gt;{&lt;/span&gt;
    listen                      &lt;span class="m"&gt;443&lt;/span&gt; ssl&lt;span class="p"&gt;;&lt;/span&gt;
    server_name                 private&lt;span class="p"&gt;;&lt;/span&gt;
    root                        /srv/www&lt;span class="p"&gt;;&lt;/span&gt;

    ssl_certificate             /etc/ssl/certs/server.crt&lt;span class="p"&gt;;&lt;/span&gt;
    ssl_certificate_key         /etc/ssl/private/server.key&lt;span class="p"&gt;;&lt;/span&gt;
    ssl_client_certificate      /etc/ssl/certs/ca.crt&lt;span class="p"&gt;;&lt;/span&gt;
    ssl_verify_client           on&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y suponiendo que el cliente ha importado su clave con éxito, ya tenemos el dominio montado.&lt;/p&gt;</content><category term="easyrsa"></category><category term="openssl"></category><category term="ssl"></category><category term="2 way ssl"></category><category term="certificado"></category></entry><entry><title>Un entorno web estático con nginx, rsync y docker</title><link href="https://www.linuxsysadmin.ml/2017/03/un-entorno-web-estatico-con-nginx-rsync-y-docker.html" rel="alternate"></link><published>2017-03-27T10:00:00+02:00</published><updated>2017-03-27T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-03-27:/2017/03/un-entorno-web-estatico-con-nginx-rsync-y-docker.html</id><summary type="html">&lt;p&gt;Hemos hablado de generar nuestro contenido HTML estático con otras herramientas, y finalmente ha llegado la hora de servirlo. Normalmente, los ficheros que cambian tal y como vamos generando páginas son pocos y nos interesa copiarlo de forma remota, pero no podemos hacerlo con &lt;strong&gt;docker&lt;/strong&gt; porque hacen falta dos servicios …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Hemos hablado de generar nuestro contenido HTML estático con otras herramientas, y finalmente ha llegado la hora de servirlo. Normalmente, los ficheros que cambian tal y como vamos generando páginas son pocos y nos interesa copiarlo de forma remota, pero no podemos hacerlo con &lt;strong&gt;docker&lt;/strong&gt; porque hacen falta dos servicios.&lt;/p&gt;
&lt;p&gt;Si has asentido con la cabeza, mal. Es verdad que se necesitan dos servicios, pero hay maneras de ejecutar dos procesos en un mismo contenedor, por ejemplo &lt;a href="https://www.linuxsysadmin.ml/2017/03/multiples-servicios-en-un-mismo-contenedor-docker.html"&gt;con un gestor de procesos&lt;/a&gt;. Sin embargo, esa no es la filosofía de &lt;strong&gt;docker&lt;/strong&gt;. Un contenedor solo debería ejecutar un proceso, simplificando su contenido y siendo necesarios varios contenedores para hacer nuestro sistema modular.&lt;/p&gt;
&lt;p&gt;Así que solo nos queda pensar las partes que tiene nuestro pequeño entorno, para luego levantar contenedores que ejecuten todos los servicios necesarios, posiblemente con &lt;strong&gt;docker-compose&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Un servidor web&lt;/li&gt;
&lt;li&gt;Un servicio de transferencia de archivos&lt;/li&gt;
&lt;li&gt;Algún sitio compartido para dejar los ficheros&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Puesto que ambos servicios van a necesitar acceder al mismo sitio, necesitamos volúmenes. Sin embargo, no queremos mezclar los datos con ninguno de los otros contenedores, ya que en caso de actualizarlos, perderíamos el volumen. Eso nos deja dos opciones: un &lt;em&gt;host volume&lt;/em&gt; o un &lt;em&gt;data container&lt;/em&gt;, que usaremos por portabilidad.&lt;/p&gt;
&lt;h2&gt;El servidor web&lt;/h2&gt;
&lt;p&gt;Vamos a utilizar &lt;strong&gt;nginx&lt;/strong&gt; por su eficiencia y velocidad. Consume poco, ocupa poco, y es simple de configurar. Como ligereza adicional, vamos a partir de una imagen de &lt;em&gt;Alpine Linux&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;La imagen no tiene misterio: un &lt;em&gt;Dockerfile&lt;/em&gt;, un fichero de arranque ejecutable y dos ficheros de configuración.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/syncweb/web$ cat Dockerfile 
FROM alpine:3.5
RUN apk add --no-cache nginx tini &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    ln -s /dev/stdout /var/log/nginx/access.log &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    ln -s /dev/stderr /var/log/nginx/error.log &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    mkdir /run/nginx &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    mkdir /srv/www &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm /etc/nginx/conf.d/default.conf
COPY nginx.conf /etc/nginx/
COPY conf.d/* /etc/nginx/conf.d/
COPY start.sh /
ENTRYPOINT &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/sbin/tini&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/start.sh&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@aldebaran:~/docker/syncweb/web$ cat start.sh 
&lt;span class="c1"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="nb"&gt;exec&lt;/span&gt; /usr/sbin/nginx -g &lt;span class="s2"&gt;&amp;quot;daemon off;&amp;quot;&lt;/span&gt;
gerard@aldebaran:~/docker/syncweb/web$ cat nginx.conf 
worker_processes &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
events &lt;span class="o"&gt;{&lt;/span&gt;
    worker_connections &lt;span class="m"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
http &lt;span class="o"&gt;{&lt;/span&gt;
    include mime.types&lt;span class="p"&gt;;&lt;/span&gt;
    default_type application/octet-stream&lt;span class="p"&gt;;&lt;/span&gt;
    sendfile on&lt;span class="p"&gt;;&lt;/span&gt;
    keepalive_timeout &lt;span class="m"&gt;65&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    include conf.d/*&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@aldebaran:~/docker/syncweb/web$ cat conf.d/web 
server &lt;span class="o"&gt;{&lt;/span&gt;
    server_name _&lt;span class="p"&gt;;&lt;/span&gt;
    listen &lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    root /srv/www&lt;span class="p"&gt;;&lt;/span&gt;
    index index.html&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@aldebaran:~/docker/syncweb/web$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a asumir que la carpeta &lt;code&gt;/srv/www/&lt;/code&gt; existe, puesto que la montaremos como un volumen.&lt;/p&gt;
&lt;h2&gt;Transfiriendo ficheros&lt;/h2&gt;
&lt;p&gt;Dada la naturaleza incremental de nuestro contenido HTML, nos viene muy bien utilizar &lt;strong&gt;rsync&lt;/strong&gt;, que funciona sobre &lt;strong&gt;ssh&lt;/strong&gt; y nos aporta encriptación, compresión y copia diferencial. Vamos a restringir el uso del &lt;strong&gt;ssh&lt;/strong&gt; mediante &lt;strong&gt;rssh&lt;/strong&gt;, permitiendo solamente usar &lt;strong&gt;rsync&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Nuevamente vamos a partir de una imagen &lt;em&gt;Alpine Linux&lt;/em&gt;, que es pequeña, segura y en este caso nos sirve de maravilla. Creamos la imagen con un simple &lt;em&gt;Dockerfile&lt;/em&gt; y un fichero de arranque, con permiso de ejecución.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/syncweb/rsync$ cat Dockerfile 
FROM alpine:3.5
RUN apk add --no-cache openssh rsync rssh tini &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    sed &lt;span class="s1"&gt;&amp;#39;s/^#allowrsync/allowrsync/g&amp;#39;&lt;/span&gt; /etc/rssh.conf.default &amp;gt; /etc/rssh.conf &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    adduser web -s /usr/bin/rssh -D -H &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;web:web&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; chpasswd
COPY start.sh /
ENTRYPOINT &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/sbin/tini&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/start.sh&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@aldebaran:~/docker/syncweb/rsync$ cat start.sh 
&lt;span class="c1"&gt;#!/bin/sh&lt;/span&gt;

chown -R web:web /srv/www
ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key -N &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="nb"&gt;exec&lt;/span&gt; /usr/sbin/sshd -D -e
gerard@aldebaran:~/docker/syncweb/rsync$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lo más interesante de decir es que la carpeta &lt;code&gt;/srv/www/&lt;/code&gt; va a ser montada como volumen. Como no podemos asegurar que los permisos sean correctos, los forzamos al usuario &lt;em&gt;web&lt;/em&gt;, que es el que va a poder entrar por &lt;strong&gt;rsync&lt;/strong&gt;. Otro punto a tener en cuenta es que se ha movido la creación de la clave de &lt;em&gt;host&lt;/em&gt; al fichero &lt;em&gt;start.sh&lt;/em&gt; para evitar duplicarla a base de crear contenedores a partir de la clave de la imagen.&lt;/p&gt;
&lt;h2&gt;El contenedor de datos&lt;/h2&gt;
&lt;p&gt;Se trata de un contenedor que va a acabar tras levantar. Su única función es albergar un volumen de datos para exportarlo a otros contenedores.&lt;/p&gt;
&lt;p&gt;Lo crearemos usando una imagen mínima, que solo tiene el comando &lt;em&gt;true&lt;/em&gt;, así podemos ejecutar un comando que no hace nada y no cargamos nada adicional. Le ponemos un contenido básico, que nos permite ver que todo funciona y que luego será sustituido mediante &lt;strong&gt;rsync&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/syncweb/data$ cat Dockerfile 
FROM tianon/true
COPY index.html /srv/www/
gerard@aldebaran:~/docker/syncweb/data$ cat index.html 
&amp;lt;h1&amp;gt;Hello world&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;This is a placeholder file&amp;lt;/p&amp;gt;
gerard@aldebaran:~/docker/syncweb/data$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Este contenedor solamente ejecuta &lt;em&gt;true&lt;/em&gt; y acaba. Sin embargo, su volumen sigue siendo accesible por aquellos contenedores que lo exporten.&lt;/p&gt;
&lt;h2&gt;Todo junto&lt;/h2&gt;
&lt;p&gt;Antes que nada creamos las imágenes, según los comandos habituales:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/syncweb$ docker build -t data data/
...  
gerard@aldebaran:~/docker/syncweb$ docker build -t web web/
...  
gerard@aldebaran:~/docker/syncweb$ docker build -t rsync rsync/
...  
gerard@aldebaran:~/docker/syncweb$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Comprobamos las imágenes que hemos creado, viendo que se han hecho bien y que ocupan una cantidad de espacio razonable.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/syncweb$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED              SIZE
rsync               latest              cb33669eb5f6        &lt;span class="m"&gt;54&lt;/span&gt; seconds ago       &lt;span class="m"&gt;8&lt;/span&gt;.77 MB
web                 latest              1ae71f497ae0        About a minute ago   &lt;span class="m"&gt;5&lt;/span&gt;.75 MB
data                latest              cc6555822fd9        About a minute ago   &lt;span class="m"&gt;180&lt;/span&gt; B
gerard@aldebaran:~/docker/syncweb$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y ya solo nos queda levantar el entorno con &lt;strong&gt;docker-compose&lt;/strong&gt;, por ejemplo con un &lt;em&gt;docker-compose.yml&lt;/em&gt; como el que sigue:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/syncweb$ cat docker-compose.yml 
version: &lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;
services:
  data:
    image: data
    hostname: data
    container_name: data
    volumes:
      - /srv/www
  rsync:
    image: rsync
    hostname: rsync
    container_name: rsync
    ports:
      - &lt;span class="s2"&gt;&amp;quot;22:22&amp;quot;&lt;/span&gt;
    volumes_from:
      - data
  web:
    image: web
    hostname: web
    container_name: web
    ports:
      - &lt;span class="s2"&gt;&amp;quot;8000:80&amp;quot;&lt;/span&gt;
    volumes_from:
      - data
gerard@aldebaran:~/docker/syncweb$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;syncweb_default&amp;quot;&lt;/span&gt; with the default driver
Creating data
Creating rsync
Creating web
gerard@aldebaran:~/docker/syncweb$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;De esta forma, todos importan el contenedor de datos, y podemos copiar ficheros por &lt;strong&gt;rsync&lt;/strong&gt; al puerto 22, y ver la web en el puerto 8000. Cambiad estos valores según vuestras necesidades.&lt;/p&gt;
&lt;h2&gt;Comprobaciones&lt;/h2&gt;
&lt;p&gt;Vemos que si hacemos una petición normal, nos devuelve el contenido inicial.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/syncweb$ curl http://localhost:8000/
&amp;lt;h1&amp;gt;Hello world&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;This is a placeholder file&amp;lt;/p&amp;gt;
gerard@aldebaran:~/docker/syncweb$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Eso significa que el servidor web funciona y está sirviendo el contenido del contenedor de datos. Ahora vamos a probar que el contenedor &lt;strong&gt;rsync&lt;/strong&gt; puede actualizar este contenido. Para comodidad lo he puesto en un &lt;em&gt;script&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/syncweb$ cat sync.sh 
&lt;span class="c1"&gt;#!/bin/bash&lt;/span&gt;

rsync -rvzc --delete content/ web@localhost:/srv/www/
gerard@aldebaran:~/docker/syncweb$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Asumimos que tenemos la carpeta &lt;code&gt;content/&lt;/code&gt; con otro fichero &lt;em&gt;index.html&lt;/em&gt;. Lo he creado a mano para la prueba, pero esto podría aparecer generado por un &lt;a href="https://www.linuxsysadmin.ml/2017/03/generadores-de-contenido-web-estaticos.html"&gt;generador de contenido estático&lt;/a&gt;. Lanzamos el &lt;em&gt;script&lt;/em&gt; por primera vez:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/syncweb$ ./sync.sh 
Warning: Permanently added &lt;span class="s1"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
web@localhost&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;s password: 
Could not chdir to home directory /home/web: No such file or directory
sending incremental file list
index.html

sent &lt;span class="m"&gt;182&lt;/span&gt; bytes  received &lt;span class="m"&gt;41&lt;/span&gt; bytes  &lt;span class="m"&gt;63&lt;/span&gt;.71 bytes/sec
total size is &lt;span class="m"&gt;60&lt;/span&gt;  speedup is &lt;span class="m"&gt;0&lt;/span&gt;.27
gerard@aldebaran:~/docker/syncweb$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El fichero &lt;em&gt;index.html&lt;/em&gt; ha cambiado, así que lo vuelve a enviar, y por supuesto, es lo que el servidor web va a servir, cosa que demuestra que también hace uso del contenedor de datos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/syncweb$ curl http://localhost:8000/
&amp;lt;h1&amp;gt;My autogenerated blog&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;This is the home page&amp;lt;/p&amp;gt;
gerard@aldebaran:~/docker/syncweb$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para comprobar el carácter incremental de &lt;strong&gt;rsync&lt;/strong&gt;, vamos a añadir un nuevo fichero &lt;em&gt;.html&lt;/em&gt; y a ejecutar de nuevo. En caso de un generador estático, veremos que solo se enviarían las nuevas páginas y aquellos índices que hayan cambiado, resultando en una ganancia alta, a nivel de tamaño enviado y tiempo invertido.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/syncweb$ ./sync.sh 
Warning: Permanently added &lt;span class="s1"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
web@localhost&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;s password: 
Could not chdir to home directory /home/web: No such file or directory
sending incremental file list
newpage.html

sent &lt;span class="m"&gt;179&lt;/span&gt; bytes  received &lt;span class="m"&gt;35&lt;/span&gt; bytes  &lt;span class="m"&gt;47&lt;/span&gt;.56 bytes/sec
total size is &lt;span class="m"&gt;71&lt;/span&gt;  speedup is &lt;span class="m"&gt;0&lt;/span&gt;.33
gerard@aldebaran:~/docker/syncweb$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo ha enviado el fichero nuevo, puesto que es un cambio con respecto a lo que ya tiene. De hecho, de no haber cambios, no se enviaría nada.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/syncweb$ ./sync.sh 
Warning: Permanently added &lt;span class="s1"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;RSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
web@localhost&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;s password: 
Could not chdir to home directory /home/web: No such file or directory
sending incremental file list

sent &lt;span class="m"&gt;128&lt;/span&gt; bytes  received &lt;span class="m"&gt;12&lt;/span&gt; bytes  &lt;span class="m"&gt;56&lt;/span&gt;.00 bytes/sec
total size is &lt;span class="m"&gt;71&lt;/span&gt;  speedup is &lt;span class="m"&gt;0&lt;/span&gt;.51
gerard@aldebaran:~/docker/syncweb$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto tenemos nuestro pequeño entorno funcional.&lt;/p&gt;</content><category term="docker"></category><category term="docker-compose"></category><category term="nginx"></category><category term="volumenes"></category><category term="rsync"></category><category term="rssh"></category></entry><entry><title>Múltiples servicios en un mismo contenedor Docker</title><link href="https://www.linuxsysadmin.ml/2017/03/multiples-servicios-en-un-mismo-contenedor-docker.html" rel="alternate"></link><published>2017-03-06T10:00:00+01:00</published><updated>2017-03-06T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-03-06:/2017/03/multiples-servicios-en-un-mismo-contenedor-docker.html</id><summary type="html">&lt;p&gt;Como ya sabemos, un contenedor &lt;strong&gt;docker&lt;/strong&gt; solo puede ejecutar un proceso, y su finalización implica la parada del contenedor. Sin embargo, a veces nos puede interesar cargar los contenedores con algún servicio más, para hacerlos autosuficientes. Para ello, nos podemos ayudar de un &lt;em&gt;gestor de procesos&lt;/em&gt;, como por ejemplo, &lt;strong&gt;runit …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Como ya sabemos, un contenedor &lt;strong&gt;docker&lt;/strong&gt; solo puede ejecutar un proceso, y su finalización implica la parada del contenedor. Sin embargo, a veces nos puede interesar cargar los contenedores con algún servicio más, para hacerlos autosuficientes. Para ello, nos podemos ayudar de un &lt;em&gt;gestor de procesos&lt;/em&gt;, como por ejemplo, &lt;strong&gt;runit&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Antes de nada, &lt;a href="https://blog.docker.com/2014/06/why-you-dont-need-to-run-sshd-in-docker/"&gt;una referencia en contra&lt;/a&gt;; esto complica nuestro contenedor de una forma no recomendad por el propia autor de &lt;strong&gt;docker&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you need multiple processes, you need to add one at the top-level to take care of the others. In other words, you’re turning a lean and simple container into something much more complicated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Avisados quedáis; a partir de aquí, vamos a ver como hacerlo con un ejemplo bastante extendido: un servidor &lt;a href="https://es.wikipedia.org/wiki/LAMP"&gt;LAMP&lt;/a&gt;. La idea es que usaremos un &lt;em&gt;gestor de procesos&lt;/em&gt; llamado &lt;strong&gt;runit&lt;/strong&gt; (aunque hay otros candidatos), que me gusta por su simplicidad.&lt;/p&gt;
&lt;h2&gt;Ejemplo: El contenedor LAMP&lt;/h2&gt;
&lt;p&gt;Nuestro contenedor va a ser un servidor muy clásico, con un &lt;strong&gt;apache&lt;/strong&gt;, un &lt;strong&gt;mysql&lt;/strong&gt;, &lt;strong&gt;php5&lt;/strong&gt; y los paquetes que esta configuración requiera, como por ejemplo, el driver de la base de datos &lt;strong&gt;php5-mysql&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AVISO&lt;/strong&gt;: No voy a añadir las instrucciones de &lt;em&gt;docker build&lt;/em&gt; ni de &lt;em&gt;docker run&lt;/em&gt; porque no aportan nada nuevo. Mejor centrémonos en las partes importantes.&lt;/p&gt;
&lt;p&gt;Vamos a utilizar la siguiente topología en la carpeta del &lt;em&gt;Dockerfile&lt;/em&gt;. La idea es que vamos a alojar el código &lt;strong&gt;php&lt;/strong&gt; en la carpeta &lt;em&gt;www&lt;/em&gt;, las configuraciones del &lt;strong&gt;apache&lt;/strong&gt; en la carpeta &lt;em&gt;apache2&lt;/em&gt; y las configuraciones de &lt;strong&gt;runit&lt;/strong&gt; en &lt;em&gt;services&lt;/em&gt;. No hay configuración específica para el &lt;strong&gt;mysql&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@antares:~/docker/lamp$ tree
.
├── apache2
│   ├── custom.conf
│   └── site.conf
├── services
│   ├── apache2
│   │   └── run
│   └── mysql
│       └── run
├── www
│   └── adminer.php
└── Dockerfile

&lt;span class="m"&gt;5&lt;/span&gt; directories, &lt;span class="m"&gt;6&lt;/span&gt; files
gerard@antares:~/docker/lamp$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y como no, el &lt;em&gt;Dockerfile&lt;/em&gt; usado:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@antares:~/docker/lamp$ cat Dockerfile 
FROM debian:jessie
ENV DEBIAN_FRONTEND noninteractive

&lt;span class="c1"&gt;# Paquetes necesarios&lt;/span&gt;
RUN apt-get update &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    apt-get install -y runit php5 php5-mysql mysql-server &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    rm -rf /var/lib/apt/lists/*

&lt;span class="c1"&gt;# Configuracion de runit&lt;/span&gt;
COPY services /etc/service

&lt;span class="c1"&gt;# Configuracion de apache&lt;/span&gt;
RUN unlink /etc/apache2/sites-enabled/000-default.conf
COPY apache2/custom.conf /etc/apache2/conf-enabled

&lt;span class="c1"&gt;# Configuracion y contenido del sitio&lt;/span&gt;
COPY apache2/site.conf /etc/apache2/sites-enabled
COPY www /srv/www

CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/bin/runsvdir&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;/etc/service&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@antares:~/docker/lamp$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Runit&lt;/h3&gt;
&lt;p&gt;El proceso principal, que se va a dedicar a controlar los otros procesos es &lt;strong&gt;runit&lt;/strong&gt;, concretamente mediante el binario &lt;em&gt;runsvdir&lt;/em&gt;. De acuerdo a nuestro &lt;em&gt;Dockerfile&lt;/em&gt;, va a gestionar un proceso por carpeta en &lt;em&gt;/etc/service&lt;/em&gt;. Dentro de esta carpeta va a tener información de ejecución e información de estado.&lt;/p&gt;
&lt;p&gt;Es importante poner un &lt;em&gt;script&lt;/em&gt; llamado &lt;em&gt;run&lt;/em&gt; con permisos de ejecución, que es el &lt;em&gt;script&lt;/em&gt; que &lt;strong&gt;runit&lt;/strong&gt; va a ejecutar y monitorizar, reiniciándolo en caso de caerse. No hay sorpresas en estos &lt;em&gt;scripts&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@antares:~/docker/lamp$ cat services/apache2/run 
&lt;span class="c1"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="nb"&gt;exec&lt;/span&gt; /usr/sbin/apache2ctl -D FOREGROUND
gerard@antares:~/docker/lamp$ cat services/mysql/run 
&lt;span class="c1"&gt;#!/bin/sh&lt;/span&gt;

&lt;span class="nb"&gt;exec&lt;/span&gt; /usr/bin/mysqld_safe
gerard@antares:~/docker/lamp$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Apache&lt;/h3&gt;
&lt;p&gt;El &lt;strong&gt;apache&lt;/strong&gt; necesita la configuración del &lt;em&gt;virtualhost&lt;/em&gt; que le indica lo que debe servir, y donde está; esto debe ir en &lt;em&gt;/etc/apache2/sites-enabled&lt;/em&gt;, de acuerdo con el &lt;em&gt;layout&lt;/em&gt; de directorios que utiliza la distribución usada (&lt;strong&gt;Debian&lt;/strong&gt; en este caso). Hemos eliminado el &lt;em&gt;virtualhost&lt;/em&gt; que viene por defecto.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@antares:~/docker/lamp$ cat apache2/site.conf 
&amp;lt;VirtualHost *:80&amp;gt;
    DocumentRoot /srv/www
    LogLevel info
    ErrorLog &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;APACHE_LOG_DIR&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/error.log
    CustomLog &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;APACHE_LOG_DIR&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/access.log combined
&amp;lt;/VirtualHost&amp;gt;
gerard@antares:~/docker/lamp$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El otro punto conflictivo es que el fichero de configuración base no lleva permisos para servir nada en &lt;em&gt;/srv/www&lt;/em&gt;, sino en &lt;em&gt;/var/www&lt;/em&gt;. Personalmente creo que el contenido debería estar en &lt;em&gt;/srv/www&lt;/em&gt;, así que tengo que añadir estos permisos, que se puede hacer cómodamente con un fichero adicional en la carpeta &lt;em&gt;/etc/apache2/conf-enabled&lt;/em&gt;, que se incluye desde el fichero &lt;em&gt;/etc/apache2/apache2.conf&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@antares:~/docker/lamp$ cat apache2/custom.conf 
&amp;lt;Directory /srv/&amp;gt;
    Options Indexes FollowSymLinks
    AllowOverride None
    Require all granted
&amp;lt;/Directory&amp;gt;
gerard@antares:~/docker/lamp$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;MySQL&lt;/h3&gt;
&lt;p&gt;No hay nada especial para &lt;strong&gt;mysql&lt;/strong&gt;. Simplemente dejamos que la variable de entorno &lt;em&gt;DEBIAN_FRONTEND&lt;/em&gt; indique a &lt;strong&gt;apt-get&lt;/strong&gt; que no pregunte una contraseña para el usuario &lt;em&gt;root&lt;/em&gt;, quedando este usuario sin contraseña. Esto debe ser revisado con esmero.&lt;/p&gt;
&lt;h3&gt;Contenido web&lt;/h3&gt;
&lt;p&gt;Para mantener limpia la carpeta de &lt;em&gt;build&lt;/em&gt;, he decidido poner el contenido web en una carpeta &lt;em&gt;www&lt;/em&gt; aparte. Simplemente se trata del &lt;em&gt;document root&lt;/em&gt; de nuestro &lt;em&gt;virtualhost&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;En este caso se ha usado &lt;a href="https://www.adminer.org/"&gt;Adminer&lt;/a&gt; que es una aplicación prefabricada de un solo fichero para administrar nuestra base de datos. La parte interesante es que se trata de algo hecho en &lt;strong&gt;php&lt;/strong&gt; que se conecta a nuestro &lt;strong&gt;mysql&lt;/strong&gt;, y nos demuestra que todo funciona. Reemplazad el contenido de esta carpeta por vuestro código final.&lt;/p&gt;
&lt;h3&gt;Más servicios&lt;/h3&gt;
&lt;p&gt;Se necesita subir código mediante un servidor &lt;strong&gt;FTP&lt;/strong&gt; o &lt;strong&gt;SFTP&lt;/strong&gt;, algún &lt;strong&gt;cron job&lt;/strong&gt;, &lt;strong&gt;logrotate&lt;/strong&gt; o lo que sea? Pues usad la misma filosofía: instalad, configurad e instruid a &lt;strong&gt;runit&lt;/strong&gt; para que levante el proceso. Sin límites.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RESULTADO&lt;/strong&gt;: Tras construir la imagen y ejecutarla, ya tenemos nuestro servidor funcionando en un solo contenedor.&lt;/p&gt;</content><category term="docker"></category><category term="runit"></category><category term="LAMP"></category><category term="apache"></category><category term="mysql"></category><category term="php"></category></entry><entry><title>Alta disponibilidad con Keepalived</title><link href="https://www.linuxsysadmin.ml/2017/02/alta-disponibilidad-con-keepalived.html" rel="alternate"></link><published>2017-02-27T10:00:00+01:00</published><updated>2017-02-27T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-02-27:/2017/02/alta-disponibilidad-con-keepalived.html</id><summary type="html">&lt;p&gt;Cuando tenemos un servicio balanceado, los &lt;em&gt;backends&lt;/em&gt; no tienen relación entre sí y podemos poner tantos como queramos, sin miedo a que alguno se caiga. Sin embargo, para los servicios tipo "ventanilla única" interesa tener varios dispuestos a dar un servicio &lt;em&gt;failover&lt;/em&gt;; si uno se cae, otro asume la carga …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Cuando tenemos un servicio balanceado, los &lt;em&gt;backends&lt;/em&gt; no tienen relación entre sí y podemos poner tantos como queramos, sin miedo a que alguno se caiga. Sin embargo, para los servicios tipo "ventanilla única" interesa tener varios dispuestos a dar un servicio &lt;em&gt;failover&lt;/em&gt;; si uno se cae, otro asume la carga.&lt;/p&gt;
&lt;p&gt;La idea general es que existe una dirección IP flotante y existe un servicio que se dedica a decidir quién la tiene asignada. Uno de estos servicios es &lt;strong&gt;keepalived&lt;/strong&gt;, que es sencillo y fácil de montar.&lt;/p&gt;
&lt;h2&gt;El entorno de trabajo&lt;/h2&gt;
&lt;p&gt;Para hacer este ejemplo, he dispuesto dos máquinas, una como prioritaria, y una de &lt;em&gt;failover&lt;/em&gt;, que va a asumir el servicio siempre que la otra no pueda hacerlo.&lt;/p&gt;
&lt;p&gt;Para mi comodidad, he dispuesto contenedores &lt;strong&gt;docker&lt;/strong&gt;, como se describe en &lt;a href="https://www.linuxsysadmin.ml/2016/06/controlando-contenedores-docker-con-ansible.html"&gt;otro artículo&lt;/a&gt; y voy a hacer la instalación por &lt;strong&gt;ansible&lt;/strong&gt;. Aunque el entorno no es muy complejo, por comodidad lo he levantado con &lt;strong&gt;docker-compose&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@gatria:~/docker/keepalived$ cat docker-compose.yml 
version: &lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;
services:
  gemini:
    image: master
    hostname: gemini
    container_name: gemini
    volumes:
      - ./playbooks:/root/playbooks:ro
      - ./inventory/hosts:/root/inventory/hosts:ro
    ports:
      - &lt;span class="s2"&gt;&amp;quot;22:22&amp;quot;&lt;/span&gt;
  castor:
    image: slave
    hostname: castor
    container_name: castor
    privileged: &lt;span class="nb"&gt;true&lt;/span&gt;
  pollux:
    image: slave
    hostname: pollux
    container_name: pollux
    privileged: &lt;span class="nb"&gt;true&lt;/span&gt;
gerard@gatria:~/docker/keepalived$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La diferencia radica en que he puesto un servidor &lt;strong&gt;SSH&lt;/strong&gt; en la máquina &lt;em&gt;master&lt;/em&gt; para poder acceder fácilmente a ella y que los &lt;em&gt;playbooks&lt;/em&gt; y parte del inventario vienen de la máquina &lt;em&gt;host&lt;/em&gt;, para su fácil edición con un editor adecuado.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AVISO&lt;/strong&gt;: Las máquinas &lt;em&gt;slave&lt;/em&gt; van a disputarse la dirección flotante; esto no es posible en un contenedor normal. Para evitar ese problema, les he puesto el &lt;em&gt;flag privilieged&lt;/em&gt;, para que puedan hacer lo que quieran.&lt;/p&gt;
&lt;p&gt;Finalmente creamos el entorno, con el comando adecuado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@gatria:~/docker/keepalived$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;keepalived_default&amp;quot;&lt;/span&gt; with the default driver
Creating gemini
Creating castor
Creating pollux
gerard@gatria:~/docker/keepalived$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Todos los comandos a partir de ahora se vana ejecutar en &lt;em&gt;gemini&lt;/em&gt;, que es el &lt;em&gt;master&lt;/em&gt; de &lt;strong&gt;ansible&lt;/strong&gt;, y al que vamos a acceder por &lt;strong&gt;SSH&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@gatria:~/docker/keepalived$ ssh root@localhost
Warning: Permanently added &lt;span class="s1"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;ECDSA&lt;span class="o"&gt;)&lt;/span&gt; to the list of known hosts.
root@localhost&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;s password: 

The programs included with the Debian GNU/Linux system are free software&lt;span class="p"&gt;;&lt;/span&gt;
the exact distribution terms &lt;span class="k"&gt;for&lt;/span&gt; each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@gemini:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El fichero de &lt;em&gt;hosts&lt;/em&gt; no tiene ningún secreto (aparte de las variables dependientes de cada máquina, que ya veremos), pero lo ponemos por completitud:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~# cat inventory/hosts 
&lt;span class="o"&gt;[&lt;/span&gt;gemini&lt;span class="o"&gt;]&lt;/span&gt;
castor &lt;span class="nv"&gt;keepalived_priority&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;101&lt;/span&gt;
pollux &lt;span class="nv"&gt;keepalived_priority&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;
root@gemini:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Ofreciendo un servicio cualquiera&lt;/h2&gt;
&lt;p&gt;Normalmente, las máquinas en configuración de &lt;em&gt;failover&lt;/em&gt; suelen ser balanceadores o servidores web. En realidad, esto es irrelevante para &lt;strong&gt;keepalived&lt;/strong&gt;, así que vamos a poner cualquiera.&lt;/p&gt;
&lt;p&gt;Nos hemos decantado por un servidor &lt;strong&gt;nginx&lt;/strong&gt; en configuración de servidor web estático, para que no absorba la atención del artículo. En la vida real, estaría en configuración de balanceador web, o sería directamente un &lt;strong&gt;haproxy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Nos movemos a la carpeta de trabajo para este &lt;em&gt;playbook&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~# &lt;span class="nb"&gt;cd&lt;/span&gt; ~/playbooks/webserver/
root@gemini:~/playbooks/webserver# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El &lt;em&gt;playbook&lt;/em&gt; en sí no tiene mucho misterio; instala &lt;strong&gt;nginx&lt;/strong&gt;, lo levanta con una configuración propia y pone un fichero &lt;strong&gt;HTML&lt;/strong&gt; con una plantilla indicando el nombre de la máquina que ha servido la petición, a modo de chivato.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/webserver# cat webserver.yml 
- hosts: gemini
  gather_facts: &lt;span class="nb"&gt;false&lt;/span&gt;
  tasks:
    - apt: &lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;nginx-light &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;present
    - file: &lt;span class="nv"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/etc/nginx/sites-enabled/default &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;absent
    - file: &lt;span class="nv"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/www &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;directory
    - template: &lt;span class="nv"&gt;src&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;web.j2 &lt;span class="nv"&gt;dest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/etc/nginx/sites-enabled/web
    - template: &lt;span class="nv"&gt;src&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;index.html.j2 &lt;span class="nv"&gt;dest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/www/index.html
    - service: &lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;nginx &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;started
    - service: &lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;nginx &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;reloaded
root@gemini:~/playbooks/webserver# cat web.j2 
server &lt;span class="o"&gt;{&lt;/span&gt;
    listen &lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    server_name _&lt;span class="p"&gt;;&lt;/span&gt;
    root /www&lt;span class="p"&gt;;&lt;/span&gt;
    index index.html&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@gemini:~/playbooks/webserver# cat index.html.j2 
&amp;lt;p&amp;gt;Hello from &amp;lt;em&amp;gt;&lt;span class="o"&gt;{{&lt;/span&gt; inventory_hostname &lt;span class="o"&gt;}}&lt;/span&gt;&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
root@gemini:~/playbooks/webserver# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lo lanzamos y ya tenemos dos servidores web para nuestra demostración:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/webserver# ansible-playbook webserver.yml 

PLAY &lt;span class="o"&gt;[&lt;/span&gt;gemini&lt;span class="o"&gt;]&lt;/span&gt; ******************************************************************

TASK &lt;span class="o"&gt;[&lt;/span&gt;apt&lt;span class="o"&gt;]&lt;/span&gt; *********************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;castor&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;pollux&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;file&lt;span class="o"&gt;]&lt;/span&gt; ********************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;castor&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;pollux&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;file&lt;span class="o"&gt;]&lt;/span&gt; ********************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;pollux&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;castor&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;template&lt;span class="o"&gt;]&lt;/span&gt; ****************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;pollux&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;castor&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;template&lt;span class="o"&gt;]&lt;/span&gt; ****************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;castor&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;pollux&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;service&lt;span class="o"&gt;]&lt;/span&gt; *****************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;castor&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;pollux&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;service&lt;span class="o"&gt;]&lt;/span&gt; *****************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;pollux&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;castor&lt;span class="o"&gt;]&lt;/span&gt;

PLAY RECAP *********************************************************************
castor                     : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;   
pollux                     : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;   

root@gemini:~/playbooks/webserver# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Comprobamos que funciona, y con esto estamos:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/webserver# wget -qO- http://castor/
&amp;lt;p&amp;gt;Hello from &amp;lt;em&amp;gt;castor&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
root@gemini:~/playbooks/webserver# wget -qO- http://pollux/
&amp;lt;p&amp;gt;Hello from &amp;lt;em&amp;gt;pollux&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
root@gemini:~/playbooks/webserver# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Keepalived, o como compartir una dirección IP&lt;/h2&gt;
&lt;p&gt;Ante nada, nos movemos a la carpeta de trabajo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/webserver# &lt;span class="nb"&gt;cd&lt;/span&gt; ~/playbooks/keepalived/
root@gemini:~/playbooks/keepalived# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Realmente, el servicio &lt;strong&gt;keepalived&lt;/strong&gt; es como cualquier otro: instalar, configurar y recargar la configuración. Lo importante en este caso son las configuraciones. Así que el &lt;em&gt;playbook&lt;/em&gt; queda un poco simple, pero mejor. Solo cabe destacar que he instalado también &lt;strong&gt;rsyslog&lt;/strong&gt; que me va a proporcionar capacidades de &lt;em&gt;syslog&lt;/em&gt;, que es donde &lt;strong&gt;keepalived&lt;/strong&gt; deja lo &lt;em&gt;logs&lt;/em&gt;. Gracias este &lt;em&gt;log&lt;/em&gt;, pude ver que hacía una operación no permitida para un contenedor normal.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/keepalived# cat keepalived.yml 
- hosts: gemini
  gather_facts: &lt;span class="nb"&gt;false&lt;/span&gt;
  tasks:
    - apt: &lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;rsyslog &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;present
    - service: &lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;rsyslog &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;started
- hosts: gemini
  gather_facts: &lt;span class="nb"&gt;false&lt;/span&gt;
  tasks:
    - apt: &lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;keepalived &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;present
    - template: &lt;span class="nv"&gt;src&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;keepalived.conf.j2 &lt;span class="nv"&gt;dest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/etc/keepalived/keepalived.conf
    - service: &lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;keepalived &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;restarted
root@gemini:~/playbooks/keepalived# cat keepalived.conf.j2 
vrrp_script chk_nginx &lt;span class="o"&gt;{&lt;/span&gt;
      script &lt;span class="s2"&gt;&amp;quot;killall -0 nginx&amp;quot;&lt;/span&gt;
      interval &lt;span class="m"&gt;2&lt;/span&gt;
      weight &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

vrrp_instance VI_1 &lt;span class="o"&gt;{&lt;/span&gt;
      interface eth0
      state MASTER
      virtual_router_id &lt;span class="m"&gt;51&lt;/span&gt;
      priority &lt;span class="o"&gt;{{&lt;/span&gt; keepalived_priority &lt;span class="o"&gt;}}&lt;/span&gt;
      virtual_ipaddress &lt;span class="o"&gt;{&lt;/span&gt;
           &lt;span class="m"&gt;172&lt;/span&gt;.18.0.10
      &lt;span class="o"&gt;}&lt;/span&gt;
      track_script &lt;span class="o"&gt;{&lt;/span&gt;
           chk_nginx
      &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@gemini:~/playbooks/keepalived# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La idea es que cada máquina del &lt;em&gt;cluster&lt;/em&gt; tiene una prioridad, y la máquina con mas prioridad va a obtener la IP flotante, dando el servicio efectivo. Esta prioridad se ve afectada por los &lt;em&gt;checks&lt;/em&gt; que pongamos, sumando el &lt;em&gt;weight&lt;/em&gt; de cada &lt;em&gt;check&lt;/em&gt; que devuelva un código de retorno 0 (se considera un OK).&lt;/p&gt;
&lt;p&gt;Con los valores 101 y 100 (que salen del fichero de &lt;em&gt;hosts&lt;/em&gt;) y el propio funcionamiento de &lt;strong&gt;keepalived&lt;/strong&gt;, nos aseguramos de que:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Si una máquina está caída no es candidata a tener la IP flotante (las dos caídas son un tema serio).&lt;/li&gt;
&lt;li&gt;Si &lt;em&gt;castor&lt;/em&gt; tiene el &lt;strong&gt;nginx&lt;/strong&gt; funcional, suma 103, y gana a &lt;em&gt;pollux&lt;/em&gt; (102 o 100, dependiendo si corre o no el &lt;strong&gt;nginx&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;Si el &lt;strong&gt;nginx&lt;/strong&gt; de &lt;em&gt;castor&lt;/em&gt; no funciona, depende; si el de &lt;em&gt;pollux funciona, pasa este a ser el &lt;/em&gt;MASTER&lt;em&gt; del &lt;/em&gt;cluster&lt;em&gt; (101 vs 102); sino, gana &lt;/em&gt;castor* (101 vs 100), aunque este caso también es un problema.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lanzamos el &lt;em&gt;playbook&lt;/em&gt; para instalar &lt;strong&gt;keepalived&lt;/strong&gt; y su configuración:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/keepalived# ansible-playbook keepalived.yml 

PLAY &lt;span class="o"&gt;[&lt;/span&gt;gemini&lt;span class="o"&gt;]&lt;/span&gt; ******************************************************************

TASK &lt;span class="o"&gt;[&lt;/span&gt;apt&lt;span class="o"&gt;]&lt;/span&gt; *********************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;castor&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;pollux&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;service&lt;span class="o"&gt;]&lt;/span&gt; *****************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;pollux&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;castor&lt;span class="o"&gt;]&lt;/span&gt;

PLAY &lt;span class="o"&gt;[&lt;/span&gt;gemini&lt;span class="o"&gt;]&lt;/span&gt; ******************************************************************

TASK &lt;span class="o"&gt;[&lt;/span&gt;apt&lt;span class="o"&gt;]&lt;/span&gt; *********************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;pollux&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;castor&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;template&lt;span class="o"&gt;]&lt;/span&gt; ****************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;castor&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;pollux&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;service&lt;span class="o"&gt;]&lt;/span&gt; *****************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;castor&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;pollux&lt;span class="o"&gt;]&lt;/span&gt;

PLAY RECAP *********************************************************************
castor                     : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;   
pollux                     : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;   

root@gemini:~/playbooks/keepalived# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y solo nos queda observar quien tiene la IP flotante (es &lt;em&gt;castor&lt;/em&gt; porque ambos &lt;strong&gt;nginx&lt;/strong&gt; funcionan y es un 103 vs 102).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/keepalived# ansible -m raw -a &lt;span class="s1"&gt;&amp;#39;ip addr | grep &amp;quot;inet &amp;quot;&amp;#39;&lt;/span&gt; gemini
castor &lt;span class="p"&gt;|&lt;/span&gt; SUCCESS &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; &amp;gt;&amp;gt;

    inet &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1/8 scope host lo
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.3/16 scope global eth0
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.10/32 scope global eth0


pollux &lt;span class="p"&gt;|&lt;/span&gt; SUCCESS &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; &amp;gt;&amp;gt;

    inet &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1/8 scope host lo
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.4/16 scope global eth0


root@gemini:~/playbooks/keepalived# wget -qO- http://172.18.0.10/
&amp;lt;p&amp;gt;Hello from &amp;lt;em&amp;gt;castor&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
root@gemini:~/playbooks/keepalived# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Pruebas de alta disponibilidad&lt;/h2&gt;
&lt;p&gt;Vamos a simular una caída de &lt;em&gt;castor&lt;/em&gt; o de su &lt;strong&gt;nginx&lt;/strong&gt; (el resultado es el mismo):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/keepalived# ansible -m service -a &lt;span class="s2"&gt;&amp;quot;name=nginx state=stopped&amp;quot;&lt;/span&gt; castor
castor &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;SUCCESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;changed&amp;quot;&lt;/span&gt;: true, 
    &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;nginx&amp;quot;&lt;/span&gt;, 
    &lt;span class="s2"&gt;&amp;quot;state&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;stopped&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@gemini:~/playbooks/keepalived# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;¿Que pasa con la IP flotante? Por supuesto, la hereda &lt;em&gt;pollux&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/keepalived# ansible -m raw -a &lt;span class="s1"&gt;&amp;#39;ip addr | grep &amp;quot;inet &amp;quot;&amp;#39;&lt;/span&gt; gemini
castor &lt;span class="p"&gt;|&lt;/span&gt; SUCCESS &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; &amp;gt;&amp;gt;

    inet &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1/8 scope host lo
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.3/16 scope global eth0


pollux &lt;span class="p"&gt;|&lt;/span&gt; SUCCESS &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; &amp;gt;&amp;gt;


    inet &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1/8 scope host lo
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.4/16 scope global eth0
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.10/32 scope global eth0


root@gemini:~/playbooks/keepalived# wget -qO- http://172.18.0.10/
&amp;lt;p&amp;gt;Hello from &amp;lt;em&amp;gt;pollux&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
root@gemini:~/playbooks/keepalived# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora simularemos que se cae &lt;em&gt;pollux&lt;/em&gt;. Esto nos deja sin servicio...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/keepalived# ansible -m service -a &lt;span class="s2"&gt;&amp;quot;name=nginx state=stopped&amp;quot;&lt;/span&gt; pollux
pollux &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;SUCCESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;changed&amp;quot;&lt;/span&gt;: true, 
    &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;nginx&amp;quot;&lt;/span&gt;, 
    &lt;span class="s2"&gt;&amp;quot;state&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;stopped&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@gemini:~/playbooks/keepalived# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Como las dos máquinas están levantadas, se trata de un 101 vs 100, lo que le da la IP a &lt;em&gt;castor&lt;/em&gt;. El servicio no responde porque el &lt;strong&gt;nginx&lt;/strong&gt; de &lt;em&gt;castor&lt;/em&gt; está caído. Mal asunto.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/keepalived# ansible -m raw -a &lt;span class="s1"&gt;&amp;#39;ip addr | grep &amp;quot;inet &amp;quot;&amp;#39;&lt;/span&gt; gemini
pollux &lt;span class="p"&gt;|&lt;/span&gt; SUCCESS &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; &amp;gt;&amp;gt;

    inet &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1/8 scope host lo
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.4/16 scope global eth0


castor &lt;span class="p"&gt;|&lt;/span&gt; SUCCESS &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; &amp;gt;&amp;gt;

    inet &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1/8 scope host lo
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.3/16 scope global eth0
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.10/32 scope global eth0


root@gemini:~/playbooks/keepalived# wget -O- http://172.18.0.10/
converted &lt;span class="s1"&gt;&amp;#39;http://172.18.0.10/&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;ANSI_X3.4-1968&lt;span class="o"&gt;)&lt;/span&gt; -&amp;gt; &lt;span class="s1"&gt;&amp;#39;http://172.18.0.10/&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;UTF-8&lt;span class="o"&gt;)&lt;/span&gt;
--2016-10-14 &lt;span class="m"&gt;11&lt;/span&gt;:00:08--  http://172.18.0.10/
Connecting to &lt;span class="m"&gt;172&lt;/span&gt;.18.0.10:80... failed: Connection refused.
root@gemini:~/playbooks/keepalived# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora, supongamos que &lt;em&gt;pollux&lt;/em&gt; se recupera.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/keepalived# ansible -m service -a &lt;span class="s2"&gt;&amp;quot;name=nginx state=started&amp;quot;&lt;/span&gt; pollux
pollux &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;SUCCESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;changed&amp;quot;&lt;/span&gt;: true, 
    &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;nginx&amp;quot;&lt;/span&gt;, 
    &lt;span class="s2"&gt;&amp;quot;state&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;started&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@gemini:~/playbooks/keepalived# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Sin sorpresas, asume la IP flotante (101 vs 102).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/keepalived# ansible -m raw -a &lt;span class="s1"&gt;&amp;#39;ip addr | grep &amp;quot;inet &amp;quot;&amp;#39;&lt;/span&gt; gemini
pollux &lt;span class="p"&gt;|&lt;/span&gt; SUCCESS &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; &amp;gt;&amp;gt;

    inet &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1/8 scope host lo
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.4/16 scope global eth0
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.10/32 scope global eth0


castor &lt;span class="p"&gt;|&lt;/span&gt; SUCCESS &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; &amp;gt;&amp;gt;

    inet &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1/8 scope host lo
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.3/16 scope global eth0


root@gemini:~/playbooks/keepalived# wget -qO- http://172.18.0.10/
&amp;lt;p&amp;gt;Hello from &amp;lt;em&amp;gt;pollux&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
root@gemini:~/playbooks/keepalived# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y finalmente se recupera &lt;em&gt;castor&lt;/em&gt;, lo que le da la prioridad para asumir su posición como &lt;em&gt;master&lt;/em&gt; del &lt;em&gt;cluster&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/keepalived# ansible -m service -a &lt;span class="s2"&gt;&amp;quot;name=nginx state=started&amp;quot;&lt;/span&gt; castor
castor &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;SUCCESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;changed&amp;quot;&lt;/span&gt;: true, 
    &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;nginx&amp;quot;&lt;/span&gt;, 
    &lt;span class="s2"&gt;&amp;quot;state&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;started&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@gemini:~/playbooks/keepalived# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y sin sorpresas, recibe las peticiones para sí mismo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@gemini:~/playbooks/keepalived# ansible -m raw -a &lt;span class="s1"&gt;&amp;#39;ip addr | grep &amp;quot;inet &amp;quot;&amp;#39;&lt;/span&gt; gemini
castor &lt;span class="p"&gt;|&lt;/span&gt; SUCCESS &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; &amp;gt;&amp;gt;

    inet &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1/8 scope host lo
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.3/16 scope global eth0
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.10/32 scope global eth0


pollux &lt;span class="p"&gt;|&lt;/span&gt; SUCCESS &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; &amp;gt;&amp;gt;

    inet &lt;span class="m"&gt;127&lt;/span&gt;.0.0.1/8 scope host lo
    inet &lt;span class="m"&gt;172&lt;/span&gt;.18.0.4/16 scope global eth0


root@gemini:~/playbooks/keepalived# wget -qO- http://172.18.0.10/
&amp;lt;p&amp;gt;Hello from &amp;lt;em&amp;gt;castor&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
root@gemini:~/playbooks/keepalived# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En caso de haberse levantado antes &lt;em&gt;castor&lt;/em&gt;, habría ejercido como &lt;em&gt;master&lt;/em&gt; enseguida, y el levantamiento de &lt;em&gt;pollux&lt;/em&gt; no habría provocado un &lt;em&gt;failover&lt;/em&gt; nuevo (103 vs 100 y 103 vs 102, respectivamente).&lt;/p&gt;
&lt;p&gt;Y con esto ya podemos tener nuestros servicios y balanceadores tipo "ventanilla única" redundados y con alta disponibilidad. Cabe indicar que esto no es útil ni con los &lt;em&gt;backends&lt;/em&gt; (el balanceador ya suele controlar si una de ellos está caído o no), ni con los &lt;em&gt;clusters&lt;/em&gt; con tecnología de &lt;em&gt;clustering&lt;/em&gt; propia (bases de datos, colas, ...).&lt;/p&gt;</content><category term="keepalived"></category><category term="failover"></category><category term="ansible"></category><category term="ip flotante"></category></entry><entry><title>Compartiendo carpetas con NFS</title><link href="https://www.linuxsysadmin.ml/2017/02/compartiendo-carpetas-con-nfs.html" rel="alternate"></link><published>2017-02-06T10:00:00+01:00</published><updated>2017-02-06T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-02-06:/2017/02/compartiendo-carpetas-con-nfs.html</id><summary type="html">&lt;p&gt;Son muchas las veces que queremos tener una carpeta disponible en todas las máquinas que usamos habitualmente, sea una unidad de &lt;em&gt;backup&lt;/em&gt;, o sea una carpeta de intercambio de fotos. Disponemos de servidores tipo FTP, pero es mas cómodo tener una unidad remota como una carpeta mas de nuestra máquina …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Son muchas las veces que queremos tener una carpeta disponible en todas las máquinas que usamos habitualmente, sea una unidad de &lt;em&gt;backup&lt;/em&gt;, o sea una carpeta de intercambio de fotos. Disponemos de servidores tipo FTP, pero es mas cómodo tener una unidad remota como una carpeta mas de nuestra máquina.&lt;/p&gt;
&lt;p&gt;Para esta guía, vamos a utilizar dos máquinas &lt;em&gt;Debian Jessie&lt;/em&gt;, actuando como el servidor (el que tiene las carpetas compartidas) y como el cliente (el ordenador de un usuario concreto).&lt;/p&gt;
&lt;p&gt;Un &lt;em&gt;setup&lt;/em&gt; mas realista sería poner el servidor en un servidor casero (tipo &lt;em&gt;Raspberry Pi&lt;/em&gt;), mientras que los ordenadores cliente serían los de los diferentes usuarios de casa.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CUIDADO&lt;/strong&gt;: El servidor NFS de &lt;em&gt;linux&lt;/em&gt; va por &lt;em&gt;kernel&lt;/em&gt;, así que nos os va a funcionar desde un contenedor, por ejemplo &lt;em&gt;LXC&lt;/em&gt; o &lt;em&gt;Docker&lt;/em&gt;. En este caso, las máquinas disponen de &lt;em&gt;kernel&lt;/em&gt; completo, porque se han utilizado &lt;em&gt;VirtualBox&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Así pues, disponemos de 2 máquinas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;server&lt;/strong&gt; &amp;rarr; 10.0.0.2&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;client&lt;/strong&gt; &amp;rarr; 10.0.0.3&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Preparando el servidor&lt;/h2&gt;
&lt;p&gt;Para preparar el servidor, necesitamos instalar el paquete que provee el servidor de &lt;strong&gt;NFS&lt;/strong&gt;, que en este caso es &lt;strong&gt;nfs-kernel-server&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# apt-get install -y nfs-kernel-server
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
Se instalarán los siguientes paquetes extras:
  file libevent-2.0-5 libldap-2.4-2 libmagic1 libnfsidmap2 libpython-stdlib libpython2.7-minimal libpython2.7-stdlib libsasl2-2
  libsasl2-modules libsasl2-modules-db libsqlite3-0 libtirpc1 mime-support nfs-common python python-minimal python2.7
  python2.7-minimal rpcbind
Paquetes sugeridos:
  libsasl2-modules-otp libsasl2-modules-ldap libsasl2-modules-sql libsasl2-modules-gssapi-mit libsasl2-modules-gssapi-heimdal
  open-iscsi watchdog python-doc python-tk python2.7-doc binutils binfmt-support
Se instalarán los siguientes paquetes NUEVOS:
  file libevent-2.0-5 libldap-2.4-2 libmagic1 libnfsidmap2 libpython-stdlib libpython2.7-minimal libpython2.7-stdlib libsasl2-2
  libsasl2-modules libsasl2-modules-db libsqlite3-0 libtirpc1 mime-support nfs-common nfs-kernel-server python python-minimal
  python2.7 python2.7-minimal rpcbind
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;21&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;6&lt;/span&gt;.072 kB de archivos.
Se utilizarán &lt;span class="m"&gt;23&lt;/span&gt;,7 MB de espacio de disco adicional después de esta operación.
...
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Supongamos que queremos compartir la carpeta &lt;em&gt;/shared/&lt;/em&gt;, lo que significa que tenemos que crearla si no existiera, y hay que darle los permisos adecuados al uso que se le vaya a dar. A modo de ejemplo, vamos a dar todos los permisos posibles a todo el mundo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mkdir /shared
root@server:~# chmod &lt;span class="m"&gt;777&lt;/span&gt; /shared/
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a declarar este punto &lt;em&gt;exportable&lt;/em&gt;, con permisos de escritura para las máquinas que lo necesiten.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cat /etc/exports
/shared &lt;span class="m"&gt;10&lt;/span&gt;.0.0.3&lt;span class="o"&gt;(&lt;/span&gt;rw,sync&lt;span class="o"&gt;)&lt;/span&gt;
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finalmente hacemos un &lt;em&gt;restart&lt;/em&gt; o un &lt;em&gt;reload&lt;/em&gt; del servicio de &lt;strong&gt;NFS&lt;/strong&gt; para que recargue la configuración.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AVISO&lt;/strong&gt;: Este servicio no arranca en contenedores.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# systemctl restart nfs-kernel-server
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto ya tenemos nuestra carpeta &lt;em&gt;exportable&lt;/em&gt; disponible para los clientes definidos en &lt;em&gt;/etc/exports&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Preparando una de las máquinas cliente&lt;/h2&gt;
&lt;p&gt;El primer paso consiste en instalar el paquete &lt;strong&gt;nfs-common&lt;/strong&gt;, que nos va a proveer de las utilidades necesarias para montar el sistema de ficheros remoto.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# apt-get install -y nfs-common
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
Se instalarán los siguientes paquetes extras:
  file libevent-2.0-5 libldap-2.4-2 libmagic1 libnfsidmap2 libpython-stdlib libpython2.7-minimal libpython2.7-stdlib libsasl2-2
  libsasl2-modules libsasl2-modules-db libsqlite3-0 libtirpc1 mime-support python python-minimal python2.7 python2.7-minimal
  rpcbind
Paquetes sugeridos:
  libsasl2-modules-otp libsasl2-modules-ldap libsasl2-modules-sql libsasl2-modules-gssapi-mit libsasl2-modules-gssapi-heimdal
  open-iscsi watchdog python-doc python-tk python2.7-doc binutils binfmt-support
Se instalarán los siguientes paquetes NUEVOS:
  file libevent-2.0-5 libldap-2.4-2 libmagic1 libnfsidmap2 libpython-stdlib libpython2.7-minimal libpython2.7-stdlib libsasl2-2
  libsasl2-modules libsasl2-modules-db libsqlite3-0 libtirpc1 mime-support nfs-common python python-minimal python2.7
  python2.7-minimal rpcbind
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;20&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;5&lt;/span&gt;.954 kB de archivos.
Se utilizarán &lt;span class="m"&gt;23&lt;/span&gt;,3 MB de espacio de disco adicional después de esta operación.
...
root@client:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Creamos un punto de montaje para la carpeta remota, en caso de necesitarla.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# mkdir /compartida
root@client:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y montamos la carpeta remota, usando las herramientas estándar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# mount -t nfs &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:/shared /compartida
root@client:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Si nos gustara el resultado, podemos hacer el montaje automático añadiendo una línea en &lt;em&gt;/etc/fstab&lt;/em&gt;, que podéis copiar tal cual de &lt;em&gt;/etc/mtab&lt;/em&gt; cuando la carpeta remota esté montada.&lt;/p&gt;
&lt;h2&gt;Algunas pruebas de funcionamiento&lt;/h2&gt;
&lt;p&gt;Partimos de una carpeta compartida vacía, y vemos que también está vacía en el cliente.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# ls /shared/
root@server:~#

root@client:~# ls /compartida/
root@client:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora podemos crear un fichero cualquiera en la máquina cliente.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# touch /compartida/client_data
root@client:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos ver que no hay ningún problema por trabajar en la carpeta desde otra máquina, por ejemplo, creando un fichero en la máquina servidor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# touch /shared/server_data
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finalmente vemos que la carpeta, sea la carpeta local del servidor o la carpeta montada remotamente del cliente, reflejan ambos cambios aplicados anteriormente.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# ls /shared/
client_data  server_data
root@server:~#

root@client:~# ls /compartida/
client_data  server_data
root@client:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto vemos que funciona como debe.&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="jessie"></category><category term="NFS"></category></entry><entry><title>Un registro local de Docker</title><link href="https://www.linuxsysadmin.ml/2017/01/un-registro-local-de-docker.html" rel="alternate"></link><published>2017-01-09T10:00:00+01:00</published><updated>2017-01-09T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2017-01-09:/2017/01/un-registro-local-de-docker.html</id><summary type="html">&lt;p&gt;He llegado a ese momento en el que el número de imágenes &lt;strong&gt;docker&lt;/strong&gt; que he construido se me ha ido de las manos. Ya no pueden seguir ocupando espacio en mi local, así que me he decidido a montar mi propio registro de imágenes, para mi uso y disfrute privado …&lt;/p&gt;</summary><content type="html">&lt;p&gt;He llegado a ese momento en el que el número de imágenes &lt;strong&gt;docker&lt;/strong&gt; que he construido se me ha ido de las manos. Ya no pueden seguir ocupando espacio en mi local, así que me he decidido a montar mi propio registro de imágenes, para mi uso y disfrute privado.&lt;/p&gt;
&lt;p&gt;Si miramos en &lt;a href="https://hub.docker.com/_/registry/"&gt;DockerHub&lt;/a&gt; no nos va a costar demasiado encontrar una imagen que nos proporciones este servicio. Ejecutar esta imagen para uso local no tiene ninguna complicación, y basta con seguir las instrucciones. La cosa se complica si queremos sacarlo de nuestra infraestructura, pero no va a ser el caso de hoy.&lt;/p&gt;
&lt;h2&gt;Levantando un registro local&lt;/h2&gt;
&lt;p&gt;Siguiendo las instrucciones, lanzamos el comando indicado en la documentación:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/registry$ docker run -d -p &lt;span class="m"&gt;5000&lt;/span&gt;:5000 registry
735016f722f25c0d8a8f09c1e2b856011d46fa4efd3a4d6c7846405140443128
gerard@aldebaran:~/docker/registry$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo necesitamos exponer el puerto 5000 que, por comodidad, va a usar el mismo puerto en mi máquina local. La única parte con estado de la imagen es &lt;em&gt;/var/lib/registry&lt;/em&gt;, y puede resultar interesante saberlo para hacer copias de seguridad; no voy a hacerlo porque la imagen ya lleva por defecto un &lt;em&gt;container volume&lt;/em&gt; declarado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/registry$ docker &lt;span class="nb"&gt;history&lt;/span&gt; registry
IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT
c9bd19d022f6        &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         /bin/sh -c &lt;span class="c1"&gt;#(nop)  CMD [&amp;quot;/etc/docker/registry   0 B                 &lt;/span&gt;
&amp;lt;missing&amp;gt;           &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         /bin/sh -c &lt;span class="c1"&gt;#(nop)  ENTRYPOINT [&amp;quot;/entrypoint.s   0 B                 &lt;/span&gt;
&amp;lt;missing&amp;gt;           &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         /bin/sh -c &lt;span class="c1"&gt;#(nop) COPY file:7b57f7ab1a8cf85c0   155 B               &lt;/span&gt;
&amp;lt;missing&amp;gt;           &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         /bin/sh -c &lt;span class="c1"&gt;#(nop)  EXPOSE 5000/tcp              0 B                 &lt;/span&gt;
&amp;lt;missing&amp;gt;           &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         /bin/sh -c &lt;span class="c1"&gt;#(nop)  VOLUME [/var/lib/registry]   0 B                 &lt;/span&gt;
&amp;lt;missing&amp;gt;           &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         /bin/sh -c &lt;span class="c1"&gt;#(nop) COPY file:6c4758d509045dc45   295 B               &lt;/span&gt;
&amp;lt;missing&amp;gt;           &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         /bin/sh -c &lt;span class="c1"&gt;#(nop) COPY file:3f73dd916d906a0db   27.21 MB            &lt;/span&gt;
&amp;lt;missing&amp;gt;           &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         /bin/sh -c &lt;span class="nb"&gt;set&lt;/span&gt; -ex     &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; apk add --no-cache    &lt;span class="m"&gt;1&lt;/span&gt;.287 MB            
&amp;lt;missing&amp;gt;           &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         /bin/sh -c &lt;span class="c1"&gt;#(nop) ADD file:7afbc23fda8b0b3872   4.803 MB            &lt;/span&gt;
gerard@aldebaran:~/docker/registry$ docker inspect 735016f722f2
&lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt;
...  
        &lt;span class="s2"&gt;&amp;quot;Mounts&amp;quot;&lt;/span&gt;: &lt;span class="o"&gt;[&lt;/span&gt;
            &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;Name&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;875b58044b85426eb82b5ea74f51f22865994d1bb84d26317c91abaaf1d5f83c&amp;quot;&lt;/span&gt;,
                &lt;span class="s2"&gt;&amp;quot;Source&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;/var/lib/docker/volumes/875b58044b85426eb82b5ea74f51f22865994d1bb84d26317c91abaaf1d5f83c/_data&amp;quot;&lt;/span&gt;,
                &lt;span class="s2"&gt;&amp;quot;Destination&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;/var/lib/registry&amp;quot;&lt;/span&gt;,
                &lt;span class="s2"&gt;&amp;quot;Driver&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;local&amp;quot;&lt;/span&gt;,
                &lt;span class="s2"&gt;&amp;quot;Mode&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;,
                &lt;span class="s2"&gt;&amp;quot;RW&amp;quot;&lt;/span&gt;: true,
                &lt;span class="s2"&gt;&amp;quot;Propagation&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
            &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;]&lt;/span&gt;,
...  
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;]&lt;/span&gt;
gerard@aldebaran:~/docker/registry$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Si preferís la versión usando &lt;strong&gt;docker-compose&lt;/strong&gt;, no difiere demasiado:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/registry$ cat docker-compose.yml 
version: &lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;
services:
  registry:
    image: registry
    hostname: registry
    container_name: registry
    volumes:
      - ./data:/var/lib/registry
    ports:
      - &lt;span class="s2"&gt;&amp;quot;5000:5000&amp;quot;&lt;/span&gt;
gerard@aldebaran:~/docker/registry$ docker-compose up -d
Creating network &lt;span class="s2"&gt;&amp;quot;registry_default&amp;quot;&lt;/span&gt; with the default driver
Creating registry
gerard@aldebaran:~/docker/registry$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En este caso, se ha optado por mapear el volumen &lt;em&gt;/var/lib/registry&lt;/em&gt; en una carpeta local, para poder inspeccionarlo fácilmente y para sacar &lt;em&gt;backups&lt;/em&gt; con mas facilidad todavía.&lt;/p&gt;
&lt;h2&gt;Uso de nuestro registro local&lt;/h2&gt;
&lt;p&gt;Se puede trabajar con nuestro registro de la misma forma con la que lo haríamos con &lt;em&gt;DockerHub&lt;/em&gt;, a base de usar el comando &lt;em&gt;docker push&lt;/em&gt; y &lt;em&gt;docker pull&lt;/em&gt;. Solo hay que mencionar que el registro destino viene especificado en el nombre de la imagen, en el formato &lt;code&gt;&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;imagen&amp;gt;:&amp;lt;tag&amp;gt;&lt;/code&gt;. Por ejemplo, vamos a subir una imagen &lt;em&gt;alpine:3.4&lt;/em&gt;, aunque podría ser una imagen nuestra.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/registry$ docker tag alpine:3.4 localhost:5000/alpine:3.4
gerard@aldebaran:~/docker/registry$ docker images
REPOSITORY                                                                                      TAG                 IMAGE ID            CREATED             SIZE
alpine                                                                                          &lt;span class="m"&gt;3&lt;/span&gt;.4                 baa5d63471ea        &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         &lt;span class="m"&gt;4&lt;/span&gt;.803 MB
localhost:5000/alpine                                                                           &lt;span class="m"&gt;3&lt;/span&gt;.4                 baa5d63471ea        &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         &lt;span class="m"&gt;4&lt;/span&gt;.803 MB
gerard@aldebaran:~/docker/registry$ docker push localhost:5000/alpine:3.4
The push refers to a repository &lt;span class="o"&gt;[&lt;/span&gt;localhost:5000/alpine&lt;span class="o"&gt;]&lt;/span&gt;
011b303988d2: Pushed 
&lt;span class="m"&gt;3&lt;/span&gt;.4: digest: sha256:1354db23ff5478120c980eca1611a51c9f2b88b61f24283ee8200bf9a54f2e5c size: &lt;span class="m"&gt;528&lt;/span&gt;
gerard@aldebaran:~/docker/registry$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y las capas que conforman nuestra imagen, quedan guardadas en nuestro registro local, como podemos ver:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/registry$ tree data/
data/
└── docker
    └── registry
        └── v2
            ├── blobs
            │   └── sha256
            │       ├── &lt;span class="m"&gt;13&lt;/span&gt;
            │       │   └── 1354db23ff5478120c980eca1611a51c9f2b88b61f24283ee8200bf9a54f2e5c
            │       │       └── data
            │       ├── &lt;span class="m"&gt;36&lt;/span&gt;
            │       │   └── 3690ec4760f95690944da86dc4496148a63d85c9e3100669a318110092f6862f
            │       │       └── data
            │       └── ba
            │           └── baa5d63471ead618ff91ddfacf1e2c81bf0612bfeb1daf00eb0843a41fbfade3
            │               └── data
            └── repositories
                └── alpine
                    ├── _layers
                    │   └── sha256
                    │       ├── 3690ec4760f95690944da86dc4496148a63d85c9e3100669a318110092f6862f
                    │       │   └── link
                    │       └── baa5d63471ead618ff91ddfacf1e2c81bf0612bfeb1daf00eb0843a41fbfade3
                    │           └── link
                    ├── _manifests
                    │   ├── revisions
                    │   │   └── sha256
                    │   │       └── 1354db23ff5478120c980eca1611a51c9f2b88b61f24283ee8200bf9a54f2e5c
                    │   │           └── link
                    │   └── tags
                    │       └── &lt;span class="m"&gt;3&lt;/span&gt;.4
                    │           ├── current
                    │           │   └── link
                    │           └── index
                    │               └── sha256
                    │                   └── 1354db23ff5478120c980eca1611a51c9f2b88b61f24283ee8200bf9a54f2e5c
                    │                       └── link
                    └── _uploads

&lt;span class="m"&gt;28&lt;/span&gt; directories, &lt;span class="m"&gt;8&lt;/span&gt; files
gerard@aldebaran:~/docker/registry$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Nada nos impediría hacer un &lt;code&gt;docker pull localhost:5000/alpine:3.4&lt;/code&gt; en el futuro.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: En caso de querer hacer un &lt;em&gt;push&lt;/em&gt; de un tag &lt;em&gt;default&lt;/em&gt; de la imagen &lt;code&gt;tools/tsung&lt;/code&gt;, bastaría con que usar &lt;code&gt;localhost:5000/tools/tsung&lt;/code&gt;; el nombre de la imagen puede contener el separador &lt;code&gt;/&lt;/code&gt; y el &lt;em&gt;tag&lt;/em&gt; es opcional, usando el &lt;em&gt;tag&lt;/em&gt; por defecto &lt;em&gt;default&lt;/em&gt;, justo como pasa con &lt;em&gt;DockerHub&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Consultando el contenido de nuestro registro&lt;/h2&gt;
&lt;p&gt;Ahora mismo nuestro registro solo tiene guardado un &lt;em&gt;alpine:3.4&lt;/em&gt;. Para poder ver una salida mas interesante de la API del registro, subo una &lt;em&gt;alpine:edge&lt;/em&gt; y una &lt;em&gt;debian:jessie&lt;/em&gt;. Así podemos apreciar dos imágenes, y una de ellas, con dos &lt;em&gt;tags&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Empezamos dando un nombre que contenga &lt;em&gt;localhost:5000&lt;/em&gt; como ya hemos visto más arriba:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/registry$ docker tag &lt;span class="o"&gt;{&lt;/span&gt;,localhost:5000/&lt;span class="o"&gt;}&lt;/span&gt;alpine:edge
gerard@aldebaran:~/docker/registry$ docker tag &lt;span class="o"&gt;{&lt;/span&gt;,localhost:5000/&lt;span class="o"&gt;}&lt;/span&gt;debian:jessie
gerard@aldebaran:~/docker/registry$ docker images &lt;span class="p"&gt;|&lt;/span&gt; grep localhost
localhost:5000/debian                                                                           jessie              73e72bf822ca        &lt;span class="m"&gt;4&lt;/span&gt; weeks ago         &lt;span class="m"&gt;123&lt;/span&gt; MB
localhost:5000/alpine                                                                           edge                a1a3cae7a75e        &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         &lt;span class="m"&gt;3&lt;/span&gt;.979 MB
localhost:5000/alpine                                                                           &lt;span class="m"&gt;3&lt;/span&gt;.4                 baa5d63471ea        &lt;span class="m"&gt;7&lt;/span&gt; weeks ago         &lt;span class="m"&gt;4&lt;/span&gt;.803 MB
gerard@aldebaran:~/docker/registry$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y los empujamos al registro:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/registry$ docker push localhost:5000/alpine:edge
The push refers to a repository &lt;span class="o"&gt;[&lt;/span&gt;localhost:5000/alpine&lt;span class="o"&gt;]&lt;/span&gt;
6f4ada5745cd: Pushed 
edge: digest: sha256:cd9c03c2d382fcf00c31dc1635445163ec185dfffb51242d9e097892b3b0d5b4 size: &lt;span class="m"&gt;528&lt;/span&gt;
gerard@aldebaran:~/docker/registry$ docker push localhost:5000/debian:jessie
The push refers to a repository &lt;span class="o"&gt;[&lt;/span&gt;localhost:5000/debian&lt;span class="o"&gt;]&lt;/span&gt;
fe4c16cbf7a4: Pushed 
jessie: digest: sha256:c1ce85a0f7126a3b5cbf7c57676b01b37c755b9ff9e2f39ca88181c02b985724 size: &lt;span class="m"&gt;529&lt;/span&gt;
gerard@aldebaran:~/docker/registry$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Disponemos de una API con -al menos- dos métodos que nos sirven para ver lo que hay en el registro:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Listar las imágenes de nuestro repositorio -&amp;gt; &lt;code&gt;GET /v2/_catalog&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Listar los &lt;em&gt;tags&lt;/em&gt; de una imagen concreta -&amp;gt; &lt;code&gt;GET /v2/&amp;lt;imagen&amp;gt;/tags/list&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Así, podemos ver lo que hay con las siguientes invocaciones:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/registry$ curl http://localhost:5000/v2/_catalog
&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;repositories&amp;quot;&lt;/span&gt;:&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;alpine&amp;quot;&lt;/span&gt;,&lt;span class="s2"&gt;&amp;quot;debian&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]}&lt;/span&gt;
gerard@aldebaran:~/docker/registry$ curl http://localhost:5000/v2/alpine/tags/list
&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;:&lt;span class="s2"&gt;&amp;quot;alpine&amp;quot;&lt;/span&gt;,&lt;span class="s2"&gt;&amp;quot;tags&amp;quot;&lt;/span&gt;:&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;edge&amp;quot;&lt;/span&gt;,&lt;span class="s2"&gt;&amp;quot;3.4&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]}&lt;/span&gt;
gerard@aldebaran:~/docker/registry$ curl http://localhost:5000/v2/debian/tags/list
&lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;:&lt;span class="s2"&gt;&amp;quot;debian&amp;quot;&lt;/span&gt;,&lt;span class="s2"&gt;&amp;quot;tags&amp;quot;&lt;/span&gt;:&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;jessie&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]}&lt;/span&gt;
gerard@aldebaran:~/docker/registry$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Se puede automatizar esta salida con un simple &lt;em&gt;script&lt;/em&gt;, que he escrito en &lt;strong&gt;python&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/registry$ cat list_registry.py 
&lt;span class="c1"&gt;#!/usr/bin/env python&lt;/span&gt;

import httplib
import json

def get_json_response&lt;span class="o"&gt;(&lt;/span&gt;host, port, uri&lt;span class="o"&gt;)&lt;/span&gt;:
    &lt;span class="nv"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; httplib.HTTPConnection&lt;span class="o"&gt;(&lt;/span&gt;host, port&lt;span class="o"&gt;)&lt;/span&gt;
    c.request&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;GET&amp;#39;&lt;/span&gt;, uri&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="nv"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; c.getresponse&lt;span class="o"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; json.load&lt;span class="o"&gt;(&lt;/span&gt;r&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="nv"&gt;catalog&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; get_json_response&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;, &lt;span class="m"&gt;5000&lt;/span&gt;, &lt;span class="s1"&gt;&amp;#39;/v2/_catalog&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; image in catalog&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;repositories&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:
    print &lt;span class="s1"&gt;&amp;#39;* %s&amp;#39;&lt;/span&gt; % image
    &lt;span class="nv"&gt;taginfo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; get_json_response&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;, &lt;span class="m"&gt;5000&lt;/span&gt;, &lt;span class="s1"&gt;&amp;#39;/v2/%s/tags/list&amp;#39;&lt;/span&gt; % image&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; tag in taginfo&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tags&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;:
        print &lt;span class="s1"&gt;&amp;#39;    * %s:%s&amp;#39;&lt;/span&gt; % &lt;span class="o"&gt;(&lt;/span&gt;taginfo&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;name&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;, tag&lt;span class="o"&gt;)&lt;/span&gt;
gerard@aldebaran:~/docker/registry$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto vemos una salida bastante legible.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@aldebaran:~/docker/registry$ ./list_registry.py 
* alpine
    * alpine:edge
    * alpine:3.4
* debian
    * debian:jessie
gerard@aldebaran:~/docker/registry$ 
&lt;/pre&gt;&lt;/div&gt;</content><category term="registro"></category><category term="docker"></category></entry><entry><title>Disparando acciones en respuesta a modificaciones en el sistema de fichero con incron</title><link href="https://www.linuxsysadmin.ml/2016/10/disparando-acciones-en-respuesta-a-modificaciones-en-el-sistema-de-fichero-con-incron.html" rel="alternate"></link><published>2016-10-03T08:00:00+02:00</published><updated>2016-10-03T08:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-10-03:/2016/10/disparando-acciones-en-respuesta-a-modificaciones-en-el-sistema-de-fichero-con-incron.html</id><summary type="html">&lt;p&gt;El otro día recibí una petición diferente en mi trabajo. Se necesitaba monitorizar una carpeta, de forma que cuando alguien dejara ficheros, se lanzara una tarea para procesarlos. Tras buscar un poco por internet, topé con una herramienta tipo &lt;em&gt;cron&lt;/em&gt;, que ejecutaba comandos ante eventos en el sistema de ficheros …&lt;/p&gt;</summary><content type="html">&lt;p&gt;El otro día recibí una petición diferente en mi trabajo. Se necesitaba monitorizar una carpeta, de forma que cuando alguien dejara ficheros, se lanzara una tarea para procesarlos. Tras buscar un poco por internet, topé con una herramienta tipo &lt;em&gt;cron&lt;/em&gt;, que ejecutaba comandos ante eventos en el sistema de ficheros.&lt;/p&gt;
&lt;p&gt;Como me pareció interesante, me puse a investigar como funcionaba y, aunque no sirvió para cubrir nuestras necesidades, decidí apuntarla por sus usos potenciales en el futuro.&lt;/p&gt;
&lt;p&gt;Como no podía ser de otra forma, vamos a empezar instalando el paquete que lo contiene:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@65b056d5d699:~# apt-get install -y incron
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following NEW packages will be installed:
  incron
&lt;span class="m"&gt;0&lt;/span&gt; upgraded, &lt;span class="m"&gt;1&lt;/span&gt; newly installed, &lt;span class="m"&gt;0&lt;/span&gt; to remove and &lt;span class="m"&gt;0&lt;/span&gt; not upgraded.
Need to get &lt;span class="m"&gt;68&lt;/span&gt;.8 kB of archives.
After this operation, &lt;span class="m"&gt;321&lt;/span&gt; kB of additional disk space will be used.
...  
Processing triggers &lt;span class="k"&gt;for&lt;/span&gt; systemd &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;215&lt;/span&gt;-17+deb8u4&lt;span class="o"&gt;)&lt;/span&gt; ...
root@65b056d5d699:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Normalmente, los servicios se levantan solos en &lt;strong&gt;Debian&lt;/strong&gt;, pero como estamos trabajando con un contenedor &lt;strong&gt;docker&lt;/strong&gt; mediante &lt;strong&gt;SSH&lt;/strong&gt;, vamos a tener que hacerlo manualmente.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@65b056d5d699:~# service incron start
&lt;span class="o"&gt;[&lt;/span&gt; ok &lt;span class="o"&gt;]&lt;/span&gt; Starting File system events scheduler: incron.
root@65b056d5d699:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Esta herramienta se puede configurar de forma similar al &lt;strong&gt;cron&lt;/strong&gt;; tenemos la opción de poner ficheros de configuración en &lt;em&gt;/etc/incron.d/&lt;/em&gt; o podemos usar comandos de &lt;strong&gt;incron&lt;/strong&gt; por usuario, mediante el comando &lt;em&gt;incron&lt;/em&gt;, con los flags correspondientes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;incron -l&lt;/strong&gt; &amp;rarr; lista la tabla de incron del usuario actual&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;incron -e&lt;/strong&gt; &amp;rarr; edita la tabla de incron del usuario actual&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;incron -r&lt;/strong&gt; &amp;rarr; elimina la tabla de incron del usuario actual&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Para usar &lt;strong&gt;incron&lt;/strong&gt; a nivel de usuario, este debe aparecer en &lt;em&gt;/etc/incron.allow&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Un caso práctico&lt;/h2&gt;
&lt;p&gt;Supongamos que tenemos un servidor web &lt;strong&gt;nginx&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@65b056d5d699:/etc/nginx/sites-enabled# netstat -lntp
Active Internet connections &lt;span class="o"&gt;(&lt;/span&gt;only servers&lt;span class="o"&gt;)&lt;/span&gt;
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:80              &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:*               LISTEN      &lt;span class="m"&gt;249&lt;/span&gt;/nginx       
tcp        &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:22              &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:*               LISTEN      &lt;span class="m"&gt;1&lt;/span&gt;/sshd          
tcp6       &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; :::22                   :::*                    LISTEN      &lt;span class="m"&gt;1&lt;/span&gt;/sshd          
root@65b056d5d699:/etc/nginx/sites-enabled# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Estamos cansados de recargar la configuración cada vez que modificamos alguno de los ficheros de configuración, así que vamos a delegar esta tarea a &lt;strong&gt;incron&lt;/strong&gt;. Concretamente queremos que cada vez que se modifique el fichero &lt;em&gt;/etc/nginx/nginx.conf&lt;/em&gt; o algunos de los &lt;em&gt;virtualhosts&lt;/em&gt; en &lt;em&gt;/etc/nginx/sites-enabled/&lt;/em&gt;, se haga un &lt;em&gt;reload&lt;/em&gt; del servicio.&lt;/p&gt;
&lt;p&gt;Miramos la tabla de eventos posibles a monitorizar y vemos que nos interesa el evento &lt;strong&gt;IN_MODIFY&lt;/strong&gt;. Se pueden monitorizar varios eventos uniéndolos con un &lt;strong&gt;OR&lt;/strong&gt; lógico, pero no se pueden poner mas de una línea por carpeta monitorizada.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;IN_ACCESS&lt;/strong&gt; &amp;rarr; Se ha accedido al fichero o directorio.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_ATTRIB&lt;/strong&gt; &amp;rarr; Se han cambiado los &lt;em&gt;metadatos&lt;/em&gt; (o los &lt;em&gt;timestamps&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_CLOSE_WRITE&lt;/strong&gt; &amp;rarr; Se ha cerrado un fichero abierto en modo distinto al de escritura.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_CLOSE_NOWRITE&lt;/strong&gt; &amp;rarr; Se ha cerrado un fichero abierto en modo de escritura.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_CREATE&lt;/strong&gt; &amp;rarr; Se ha creado un fichero en el directorio monitorizado.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_DELETE&lt;/strong&gt; &amp;rarr; Se ha borrado un fichero en la carpeta.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_DELETE_SELF&lt;/strong&gt; &amp;rarr; El fichero o directorio monitorizado ha sido borrado.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_MODIFY&lt;/strong&gt; &amp;rarr; El fichero ha sido modificado.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_MOVE_SELF&lt;/strong&gt; &amp;rarr; El fichero o carpeta monitorizado se ha movido.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_MOVED_FROM&lt;/strong&gt; &amp;rarr; Un fichero se ha movido desde la carpeta monitorizada.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_MOVED_TO&lt;/strong&gt; &amp;rarr; Un fichero se ha movido hacia la carpeta monitorizada.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_OPEN&lt;/strong&gt; &amp;rarr; Se ha abierto un fichero en la carpeta monitorizada.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_MOVE&lt;/strong&gt; &amp;rarr; Combinación de IN_MOVED_FROM y de IN_MOVED_TO.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_CLOSE&lt;/strong&gt; &amp;rarr; Combinación de IN_CLOSE_WRITE y IN_CLOSE_NOWRITE.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IN_ALL_EVENTS&lt;/strong&gt; &amp;rarr; Todos los eventos anteriores.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Así pues, ponemos la tripleta &lt;em&gt;carpeta, evento, acción&lt;/em&gt;, ya sea con el comando &lt;em&gt;incron -e&lt;/em&gt; o mediante un fichero en &lt;em&gt;/etc/incron.d/&lt;/em&gt;. Es importante indicar que la salida del comando no se puede recoger en este fichero; si es necesario, habría que llamar a un &lt;em&gt;script&lt;/em&gt; que tuviera la redirección.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@65b056d5d699:~# cat /etc/incron.d/nginx 
/etc/nginx/nginx.conf IN_MODIFY /usr/sbin/service nginx reload
/etc/nginx/sites-enabled/ IN_MODIFY /usr/sbin/service nginx reload
root@65b056d5d699:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En este punto no hay que recargar nada; &lt;strong&gt;incron&lt;/strong&gt; ha detectado el cambio y se ha recargado solo.&lt;/p&gt;
&lt;p&gt;Vamos a cambiar, por ejemplo, el puerto en el que escucha nuestra web.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ANTES:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@65b056d5d699:~# cat /etc/nginx/sites-enabled/mysite 
server &lt;span class="o"&gt;{&lt;/span&gt;
    listen &lt;span class="m"&gt;80&lt;/span&gt; default_server&lt;span class="p"&gt;;&lt;/span&gt;
    root /var/www/html&lt;span class="p"&gt;;&lt;/span&gt;
    server_name _&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;DESPUES:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@65b056d5d699:~# cat /etc/nginx/sites-enabled/mysite 
server &lt;span class="o"&gt;{&lt;/span&gt;
    listen &lt;span class="m"&gt;8080&lt;/span&gt; default_server&lt;span class="p"&gt;;&lt;/span&gt;
    root /var/www/html&lt;span class="p"&gt;;&lt;/span&gt;
    server_name _&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@65b056d5d699:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y sin recargar la configuración de &lt;strong&gt;nginx&lt;/strong&gt; -puesto que ya lo ha hecho &lt;strong&gt;incron&lt;/strong&gt;-, vemos que se ha puesto a escuchar en el nuevo puerto.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@65b056d5d699:~# netstat -lntp
Active Internet connections &lt;span class="o"&gt;(&lt;/span&gt;only servers&lt;span class="o"&gt;)&lt;/span&gt;
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:8080            &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:*               LISTEN      &lt;span class="m"&gt;249&lt;/span&gt;/nginx       
tcp        &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:22              &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:*               LISTEN      &lt;span class="m"&gt;1&lt;/span&gt;/sshd          
tcp6       &lt;span class="m"&gt;0&lt;/span&gt;      &lt;span class="m"&gt;0&lt;/span&gt; :::22                   :::*                    LISTEN      &lt;span class="m"&gt;1&lt;/span&gt;/sshd          
root@65b056d5d699:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Ejecución parametrizada de nuestras tareas&lt;/h2&gt;
&lt;p&gt;El proceso de &lt;strong&gt;incron&lt;/strong&gt; nos ofrece unos parámetros para discernir cual de los eventos disparó el &lt;em&gt;trigger&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;$$&lt;/strong&gt; &amp;rarr; Símbolo de dólar.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$@&lt;/strong&gt; &amp;rarr; Ruta de la carpeta monitorizada.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$#&lt;/strong&gt; &amp;rarr; Fichero modificado.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$%&lt;/strong&gt; &amp;rarr; Evento que disparó la acción, en texto.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$&amp;amp;&lt;/strong&gt; &amp;rarr; Evento que disparó la acción, en número.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Si quisiéramos procesar un fichero tal como nos lo dejen en una carpeta, podemos usar un &lt;em&gt;script&lt;/em&gt; con parámetros que haga lo que deba con el mismo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@65b056d5d699:~# cat /etc/incron.d/batch_process 
/opt/batch_process/inbox/ IN_WRITE_CLOSE /opt/batch_process/process_file.py &lt;span class="nv"&gt;$@&lt;/span&gt; &lt;span class="nv"&gt;$#&lt;/span&gt;
root@65b056d5d699:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Este &lt;em&gt;script&lt;/em&gt; recibe la ruta de la carpeta monitorizada y el fichero. Lo que hace el &lt;em&gt;script&lt;/em&gt; se deja a la imaginación del lector. Como punto adicional, recalcar que se ha usado el evento &lt;strong&gt;IN_WRITE_CLOSE&lt;/strong&gt; en vez de &lt;strong&gt;IN_CREATE&lt;/strong&gt; porque este último salta inmediatamente, y podríamos encontrarnos con un fichero a medio subir, en caso de ser puesto por un protocolo remoto.&lt;/p&gt;
&lt;p&gt;Estoy seguro que en un futuro no muy lejano, esta herramienta me va a ser de gran utilidad.&lt;/p&gt;</content><category term="incron"></category></entry><entry><title>Un servidor pypi local</title><link href="https://www.linuxsysadmin.ml/2016/09/un-servidor-pypi-local.html" rel="alternate"></link><published>2016-09-05T08:00:00+02:00</published><updated>2016-09-05T08:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-09-05:/2016/09/un-servidor-pypi-local.html</id><summary type="html">&lt;p&gt;Cuando trabajamos con &lt;strong&gt;python&lt;/strong&gt;, muchas veces necesitamos instalar librerías con &lt;em&gt;pip&lt;/em&gt; o &lt;em&gt;easy_install&lt;/em&gt;. Dependiendo de la naturaleza de nuestros proyectos, las librerías suelen variar, pero siempre solemos utilizar los mismos. En estos casos puede ser útil tenerlos cerca, cacheados en un servidor en nuestra red local, para su rápido acceso …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Cuando trabajamos con &lt;strong&gt;python&lt;/strong&gt;, muchas veces necesitamos instalar librerías con &lt;em&gt;pip&lt;/em&gt; o &lt;em&gt;easy_install&lt;/em&gt;. Dependiendo de la naturaleza de nuestros proyectos, las librerías suelen variar, pero siempre solemos utilizar los mismos. En estos casos puede ser útil tenerlos cerca, cacheados en un servidor en nuestra red local, para su rápido acceso.&lt;/p&gt;
&lt;p&gt;Para estos casos podemos montar un servidor exactamente igual que el de &lt;a href="https://pypi.python.org/pypi"&gt;PyPI&lt;/a&gt;, que se distribuye como una librería &lt;strong&gt;python&lt;/strong&gt; adicional, que nos ofrece una aplicación &lt;strong&gt;WSGI&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Nuestro despliegue es bastante básico; con un solo servidor nos basta, y puede estar compartido con otros usos. El único servicio que vamos a poner es un servidor &lt;strong&gt;WSGI&lt;/strong&gt; capaz de servir la aplicación. En nuestro caso vamos a usar &lt;strong&gt;uwsgi&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Así pues, creamos dos máquinas, una va a ser el servidor y la otra, un cliente de ejemplo que necesite los paquetes locales.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# lxc-ls -f
NAME        STATE    IPV4      IPV6  AUTOSTART
----------------------------------------------
pyclient    RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.3  -     NO
pypiserver  RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2  -     NO
root@lxc:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Montando el servidor&lt;/h2&gt;
&lt;p&gt;Como decisión de diseño, he optado por instalar el paquete &lt;em&gt;pypiserver&lt;/em&gt; en un &lt;em&gt;virtualenv&lt;/em&gt;, para no interferir con otros paquetes que pudiera haber en el servidor.&lt;/p&gt;
&lt;p&gt;Empezaremos creando una carpeta contenedora, en donde va a ir el &lt;em&gt;virtualenv&lt;/em&gt;, la aplicación y el índice de paquetes disponibles en el servidor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pypiserver:~# mkdir /opt/pypi &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;cd&lt;/span&gt; /opt/pypi
root@pypiserver:/opt/pypi#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a descargar &lt;em&gt;virtualenv&lt;/em&gt;, sin instalarlo, para "usar y tirar". Para ello vamos a necesitar alguna herramienta para descargarlo de la red, por ejemplo, &lt;strong&gt;wget&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pypiserver:/opt/pypi# apt-get install -y wget
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
...
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;13&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;9&lt;/span&gt;.875 kB de archivos.
Se utilizarán &lt;span class="m"&gt;35&lt;/span&gt;,7 MB de espacio de disco adicional después de esta operación.
...
root@pypiserver:/opt/pypi#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Tanto la herramienta &lt;em&gt;virtualenv&lt;/em&gt; como la herramienta &lt;em&gt;pip&lt;/em&gt; que vamos a necesitar mas adelante, usan &lt;strong&gt;python&lt;/strong&gt;. Es un buen momento para asegurar que esté instalado, y si no lo está, lo instalamos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pypiserver:/opt/pypi# apt-get install -y python
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
...
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;12&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;4&lt;/span&gt;.991 kB de archivos.
Se utilizarán &lt;span class="m"&gt;21&lt;/span&gt;,2 MB de espacio de disco adicional después de esta operación.
...
root@pypiserver:/opt/pypi#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Descargamos el paquete &lt;em&gt;virtualenv&lt;/em&gt; y lo descomprimimos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pypiserver:/opt/pypi# wget -q https://pypi.python.org/packages/5c/79/5dae7494b9f5ed061cff9a8ab8d6e1f02db352f3facf907d9eb614fb80e9/virtualenv-15.0.2.tar.gz
root@pypiserver:/opt/pypi# tar xzf virtualenv-15.0.2.tar.gz
root@pypiserver:/opt/pypi#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a crear el &lt;em&gt;virtualenv&lt;/em&gt; dentro de nuestra carpeta contenedora. Luego instalamos el paquete &lt;em&gt;pypiserver&lt;/em&gt;, previo activado del &lt;em&gt;virtualenv&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pypiserver:/opt/pypi# ./virtualenv-15.0.2/virtualenv.py env
New python executable in /opt/pypi/env/bin/python
Installing setuptools, pip, wheel...done.
root@pypiserver:/opt/pypi# . env/bin/activate
&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@pypiserver:/opt/pypi# pip install pypiserver
Collecting pypiserver
  Downloading pypiserver-1.1.10-py2.py3-none-any.whl &lt;span class="o"&gt;(&lt;/span&gt;75kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 81kB 632kB/s
Installing collected packages: pypiserver
Successfully installed pypiserver-1.1.10
&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@pypiserver:/opt/pypi# deactivate
root@pypiserver:/opt/pypi#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto tenemos el &lt;em&gt;virtualenv&lt;/em&gt;. Si no ha habido problemas, y no lo pensamos reconstruir, es un buen momento para eliminar los &lt;em&gt;scripts&lt;/em&gt; de creación del mismo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pypiserver:/opt/pypi# rm -R virtualenv-15.0.2*
root@pypiserver:/opt/pypi#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a hacer que nuestro servidor sirva los paquetes de una carpeta &lt;em&gt;packages&lt;/em&gt;, dentro de la carpeta contenedora. Como no existe esta carpeta &lt;em&gt;packages&lt;/em&gt;, la creamos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pypiserver:/opt/pypi# mkdir packages
root@pypiserver:/opt/pypi#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y finalmente creamos una aplicación &lt;strong&gt;WSGI&lt;/strong&gt; para poder servir nuestros paquetes. Realmente es una instancia de la aplicación que ofrece el paquete &lt;em&gt;pypiserver&lt;/em&gt;, con la única diferencia que consiste en especificar la raíz de los paquetes servidos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pypiserver:/opt/pypi# cat app.py
import pypiserver
&lt;span class="nv"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; pypiserver.app&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;root&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/opt/pypi/packages&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
root@pypiserver:/opt/pypi#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lo único que queda es servir la aplicación en un servidor de nuestra preferencia. En mi caso he optado por &lt;strong&gt;uwsgi&lt;/strong&gt;, por lo que lo instalo. Se va a usar el modo &lt;em&gt;emperor&lt;/em&gt; por comodidad.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pypiserver:/opt/pypi# apt-get install -y uwsgi-emperor uwsgi-plugin-python
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
...
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;13&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;3&lt;/span&gt;.452 kB de archivos.
Se utilizarán &lt;span class="m"&gt;9&lt;/span&gt;.799 kB de espacio de disco adicional después de esta operación.
...
root@pypiserver:/opt/pypi#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;De acuerdo con el modo &lt;em&gt;emperor&lt;/em&gt;, necesitamos declarar la aplicación mediante un fichero de configuración. Con esto el &lt;em&gt;emperor&lt;/em&gt; la tiene fichada y se encarga de mantenerla levantada.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pypiserver:/opt/pypi# cat /etc/uwsgi-emperor/vassals/pypiserver.ini
&lt;span class="o"&gt;[&lt;/span&gt;uwsgi&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;plugins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; python
http-socket &lt;span class="o"&gt;=&lt;/span&gt; :8080
&lt;span class="nv"&gt;master&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="nv"&gt;workers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="nv"&gt;chdir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /opt/pypi
&lt;span class="nv"&gt;virtualenv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /opt/pypi/env/
&lt;span class="nv"&gt;module&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; app:app
root@pypiserver:/opt/pypi#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Añadiendo paquetes a nuestro servidor&lt;/h2&gt;
&lt;p&gt;Esta es la parte mas fácil de todas; basta con dejar nuestros paquetes &lt;strong&gt;python&lt;/strong&gt; en la carpeta &lt;em&gt;/opt/pypi/packages/&lt;/em&gt;. Así de fácil.&lt;/p&gt;
&lt;p&gt;El formato es cualquiera aceptado por &lt;em&gt;pip&lt;/em&gt; o &lt;em&gt;easy_install&lt;/em&gt;, pudiendo ser ficheros &lt;em&gt;.zip&lt;/em&gt;, &lt;em&gt;.egg&lt;/em&gt; o &lt;em&gt;.whl&lt;/em&gt; entre otros; pueden ser descargados, compilados, o creados por nosotros mismos.&lt;/p&gt;
&lt;p&gt;Para ver un ejemplo, voy a generar unos ficheros &lt;em&gt;.whl&lt;/em&gt;, mediante el uso de &lt;em&gt;pip&lt;/em&gt;. Esto nos garantiza que los tendremos cerca, pero que también van a estar ya compilados para la arquitectura concreta del servidor (presumiblemente la misma que van a usar los clientes).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pypiserver:~# /opt/pypi/env/bin/pip wheel -w /opt/pypi/packages/ flask mongoengine
Collecting flask
  Downloading Flask-0.11.1-py2.py3-none-any.whl &lt;span class="o"&gt;(&lt;/span&gt;80kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 81kB 599kB/s
  Saved /opt/pypi/packages/Flask-0.11.1-py2.py3-none-any.whl
Collecting mongoengine
  Downloading mongoengine-0.10.6.tar.gz &lt;span class="o"&gt;(&lt;/span&gt;346kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 348kB 560kB/s
Collecting click&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.0 &lt;span class="o"&gt;(&lt;/span&gt;from flask&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading click-6.6.tar.gz &lt;span class="o"&gt;(&lt;/span&gt;283kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 286kB &lt;span class="m"&gt;2&lt;/span&gt;.0MB/s
Collecting Werkzeug&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.7 &lt;span class="o"&gt;(&lt;/span&gt;from flask&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading Werkzeug-0.11.10-py2.py3-none-any.whl &lt;span class="o"&gt;(&lt;/span&gt;306kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 307kB &lt;span class="m"&gt;1&lt;/span&gt;.0MB/s
  Saved /opt/pypi/packages/Werkzeug-0.11.10-py2.py3-none-any.whl
Collecting Jinja2&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.4 &lt;span class="o"&gt;(&lt;/span&gt;from flask&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading Jinja2-2.8-py2.py3-none-any.whl &lt;span class="o"&gt;(&lt;/span&gt;263kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 266kB &lt;span class="m"&gt;2&lt;/span&gt;.1MB/s
  Saved /opt/pypi/packages/Jinja2-2.8-py2.py3-none-any.whl
Collecting itsdangerous&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.21 &lt;span class="o"&gt;(&lt;/span&gt;from flask&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading itsdangerous-0.24.tar.gz &lt;span class="o"&gt;(&lt;/span&gt;46kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 51kB &lt;span class="m"&gt;2&lt;/span&gt;.3MB/s
Collecting pymongo&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.7.1 &lt;span class="o"&gt;(&lt;/span&gt;from mongoengine&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading pymongo-3.2.2.tar.gz &lt;span class="o"&gt;(&lt;/span&gt;504kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 512kB 657kB/s
Collecting MarkupSafe &lt;span class="o"&gt;(&lt;/span&gt;from Jinja2&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.4-&amp;gt;flask&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading MarkupSafe-0.23.tar.gz
Skipping flask, due to already being wheel.
Skipping Werkzeug, due to already being wheel.
Skipping Jinja2, due to already being wheel.
Building wheels &lt;span class="k"&gt;for&lt;/span&gt; collected packages: mongoengine, click, itsdangerous, pymongo, MarkupSafe
  Running setup.py bdist_wheel &lt;span class="k"&gt;for&lt;/span&gt; mongoengine ... &lt;span class="k"&gt;done&lt;/span&gt;
  Stored in directory: /opt/pypi/packages
  Running setup.py bdist_wheel &lt;span class="k"&gt;for&lt;/span&gt; click ... &lt;span class="k"&gt;done&lt;/span&gt;
  Stored in directory: /opt/pypi/packages
  Running setup.py bdist_wheel &lt;span class="k"&gt;for&lt;/span&gt; itsdangerous ... &lt;span class="k"&gt;done&lt;/span&gt;
  Stored in directory: /opt/pypi/packages
  Running setup.py bdist_wheel &lt;span class="k"&gt;for&lt;/span&gt; pymongo ... &lt;span class="k"&gt;done&lt;/span&gt;
  Stored in directory: /opt/pypi/packages
  Running setup.py bdist_wheel &lt;span class="k"&gt;for&lt;/span&gt; MarkupSafe ... &lt;span class="k"&gt;done&lt;/span&gt;
  Stored in directory: /opt/pypi/packages
Successfully built mongoengine click itsdangerous pymongo MarkupSafe
root@pypiserver:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y podemos ver que tenemos varios paquetes en la carpeta, algunos de ellos descargados ya en formato &lt;em&gt;wheel&lt;/em&gt; (por ejemplo &lt;em&gt;flask&lt;/em&gt;), y otros que se descargaron en formato &lt;em&gt;source&lt;/em&gt; y se compilaron (por ejemplo &lt;em&gt;pymongo&lt;/em&gt;).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pypiserver:~# ls -1 /opt/pypi/packages/
click-6.6-py2.py3-none-any.whl
Flask-0.11.1-py2.py3-none-any.whl
itsdangerous-0.24-py2-none-any.whl
Jinja2-2.8-py2.py3-none-any.whl
MarkupSafe-0.23-py2-none-any.whl
mongoengine-0.10.6-py2-none-any.whl
pymongo-3.2.2-cp27-cp27mu-linux_i686.whl
Werkzeug-0.11.10-py2.py3-none-any.whl
root@pypiserver:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Usando el servidor desde un cliente&lt;/h2&gt;
&lt;p&gt;Ya que vamos a trabajar con &lt;strong&gt;python&lt;/strong&gt;, aseguramos que lo tenemos instalado, o lo instalamos. Vamos a poner también la herramienta &lt;strong&gt;wget&lt;/strong&gt; porque la vamos a necesitar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pyclient:~# apt-get install -y wget python
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
...
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;25&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;14&lt;/span&gt;,9 MB de archivos.
Se utilizarán &lt;span class="m"&gt;56&lt;/span&gt;,9 MB de espacio de disco adicional después de esta operación.
...
root@pyclient:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Una prueba rápida: hay que ver que llegamos al servidor creado, y que este ofrece los paquetes en un formato adecuado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pyclient:~# wget -qO- http://10.0.0.2:8080/simple/&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
    &amp;lt;html&amp;gt;
        &amp;lt;head&amp;gt;
            &amp;lt;title&amp;gt;Simple Index&amp;lt;/title&amp;gt;
        &amp;lt;/head&amp;gt;
        &amp;lt;body&amp;gt;
            &amp;lt;h1&amp;gt;Simple Index&amp;lt;/h1&amp;gt;
                 &amp;lt;a &lt;span class="nv"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Flask/&amp;quot;&lt;/span&gt;&amp;gt;Flask&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;
                 &amp;lt;a &lt;span class="nv"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Jinja2/&amp;quot;&lt;/span&gt;&amp;gt;Jinja2&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;
                 &amp;lt;a &lt;span class="nv"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;MarkupSafe/&amp;quot;&lt;/span&gt;&amp;gt;MarkupSafe&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;
                 &amp;lt;a &lt;span class="nv"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Werkzeug/&amp;quot;&lt;/span&gt;&amp;gt;Werkzeug&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;
                 &amp;lt;a &lt;span class="nv"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;click/&amp;quot;&lt;/span&gt;&amp;gt;click&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;
                 &amp;lt;a &lt;span class="nv"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;itsdangerous/&amp;quot;&lt;/span&gt;&amp;gt;itsdangerous&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;
                 &amp;lt;a &lt;span class="nv"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mongoengine/&amp;quot;&lt;/span&gt;&amp;gt;mongoengine&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;
                 &amp;lt;a &lt;span class="nv"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pymongo/&amp;quot;&lt;/span&gt;&amp;gt;pymongo&amp;lt;/a&amp;gt;&amp;lt;br&amp;gt;
        &amp;lt;/body&amp;gt;
    &amp;lt;/html&amp;gt;

root@pyclient:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Descargamos los &lt;em&gt;scripts&lt;/em&gt; de creación del &lt;em&gt;virtualenv&lt;/em&gt;, tal como lo hacemos mas arriba.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pyclient:~# wget -q https://pypi.python.org/packages/5c/79/5dae7494b9f5ed061cff9a8ab8d6e1f02db352f3facf907d9eb614fb80e9/virtualenv-15.0.2.tar.gz
root@pyclient:~# tar xzf virtualenv-15.0.2.tar.gz
root@pyclient:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Creamos un &lt;em&gt;virtualenv&lt;/em&gt; en donde instalar los paquetes y lo activamos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pyclient:~# ./virtualenv-15.0.2/virtualenv.py env
New python executable in /root/env/bin/python
Installing setuptools, pip, wheel...done.
root@pyclient:~# . env/bin/activate
&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@pyclient:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y lo usamos para instalar alguno de los paquetes. Es importante ver que modificamos el &lt;em&gt;index url&lt;/em&gt;, para usar nuestro servidor, y que debemos indicarle que confíe en nuestro servidor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@pyclient:~# pip install --trusted-host &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2 -i http://10.0.0.2:8080/simple/ mongoengine
Collecting mongoengine
  Downloading http://10.0.0.2:8080/packages/mongoengine-0.10.6-py2-none-any.whl &lt;span class="o"&gt;(&lt;/span&gt;90kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 92kB &lt;span class="m"&gt;10&lt;/span&gt;.9MB/s
Collecting pymongo&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.7.1 &lt;span class="o"&gt;(&lt;/span&gt;from mongoengine&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading http://10.0.0.2:8080/packages/pymongo-3.2.2-cp27-cp27mu-linux_i686.whl &lt;span class="o"&gt;(&lt;/span&gt;209kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 215kB &lt;span class="m"&gt;9&lt;/span&gt;.9MB/s
Installing collected packages: pymongo, mongoengine
Successfully installed mongoengine-0.10.6 pymongo-3.2.2
&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@pyclient:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Podemos crear un fichero de configuración de &lt;em&gt;pip&lt;/em&gt; para que esos parámetros queden ocultos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@pyclient:~# mkdir -p /root/.config/pip/
&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@pyclient:~# cat /root/.config/pip/pip.conf
&lt;span class="o"&gt;[&lt;/span&gt;global&lt;span class="o"&gt;]&lt;/span&gt;
index-url &lt;span class="o"&gt;=&lt;/span&gt; http://10.0.0.2:8080/simple/
trusted-host &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2
&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@pyclient:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Tras aplicar el truco, nos queda una orden &lt;em&gt;pip&lt;/em&gt; bastante mas bonita, sin tantos parámetros que recordar y nos permite trabajar como la haríamos sin el servidor intermedio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@pyclient:~# pip install flask
Collecting flask
  Downloading http://10.0.0.2:8080/packages/Flask-0.11.1-py2.py3-none-any.whl &lt;span class="o"&gt;(&lt;/span&gt;80kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 81kB &lt;span class="m"&gt;10&lt;/span&gt;.5MB/s
Collecting click&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.0 &lt;span class="o"&gt;(&lt;/span&gt;from flask&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading http://10.0.0.2:8080/packages/click-6.6-py2.py3-none-any.whl &lt;span class="o"&gt;(&lt;/span&gt;71kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 71kB &lt;span class="m"&gt;11&lt;/span&gt;.3MB/s
Collecting Werkzeug&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.7 &lt;span class="o"&gt;(&lt;/span&gt;from flask&lt;span class="o"&gt;)&lt;/span&gt;
  Retrying &lt;span class="o"&gt;(&lt;/span&gt;Retry&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;total&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;, &lt;span class="nv"&gt;connect&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;None, &lt;span class="nv"&gt;read&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;None, &lt;span class="nv"&gt;redirect&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;None&lt;span class="o"&gt;))&lt;/span&gt; after connection broken by &lt;span class="s1"&gt;&amp;#39;ProtocolError(&amp;#39;&lt;/span&gt;Connection aborted.&lt;span class="s1"&gt;&amp;#39;, error(104, &amp;#39;&lt;/span&gt;Conexi&lt;span class="se"&gt;\x&lt;/span&gt;c3&lt;span class="se"&gt;\x&lt;/span&gt;b3n reinicializada por la m&lt;span class="se"&gt;\x&lt;/span&gt;c3&lt;span class="se"&gt;\x&lt;/span&gt;a1quina remota&lt;span class="s1"&gt;&amp;#39;))&amp;#39;&lt;/span&gt;: /packages/Werkzeug-0.11.10-py2.py3-none-any.whl
  Downloading http://10.0.0.2:8080/packages/Werkzeug-0.11.10-py2.py3-none-any.whl &lt;span class="o"&gt;(&lt;/span&gt;306kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 307kB &lt;span class="m"&gt;8&lt;/span&gt;.3MB/s
Collecting Jinja2&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.4 &lt;span class="o"&gt;(&lt;/span&gt;from flask&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading http://10.0.0.2:8080/packages/Jinja2-2.8-py2.py3-none-any.whl &lt;span class="o"&gt;(&lt;/span&gt;263kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 266kB &lt;span class="m"&gt;9&lt;/span&gt;.0MB/s
Collecting itsdangerous&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.21 &lt;span class="o"&gt;(&lt;/span&gt;from flask&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading http://10.0.0.2:8080/packages/itsdangerous-0.24-py2-none-any.whl
Collecting MarkupSafe &lt;span class="o"&gt;(&lt;/span&gt;from Jinja2&amp;gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.4-&amp;gt;flask&lt;span class="o"&gt;)&lt;/span&gt;
  Downloading http://10.0.0.2:8080/packages/MarkupSafe-0.23-py2-none-any.whl
Installing collected packages: click, Werkzeug, MarkupSafe, Jinja2, itsdangerous, flask
Successfully installed Jinja2-2.8 MarkupSafe-0.23 Werkzeug-0.11.10 click-6.6 flask-0.11.1 itsdangerous-0.24
&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@pyclient:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finalmente salimos del &lt;em&gt;virtualenv&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@pyclient:~# deactivate
root@pyclient:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Como punto final, queda indicar que si el paquete no está en nuestro servidor, no pasa nada; nuestro servidor va a pasar la petición al índica &lt;em&gt;pypi&lt;/em&gt; titular, de forma transparente.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@pyclient:~# ./env/bin/pip install requests
Collecting requests
  Downloading requests-2.10.0-py2.py3-none-any.whl &lt;span class="o"&gt;(&lt;/span&gt;506kB&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="p"&gt;|&lt;/span&gt;████████████████████████████████&lt;span class="p"&gt;|&lt;/span&gt; 512kB &lt;span class="m"&gt;1&lt;/span&gt;.0MB/s
Installing collected packages: requests
Successfully installed requests-2.10.0
root@pyclient:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Activar un &lt;em&gt;virtualenv&lt;/em&gt; solo pone su carpeta &lt;em&gt;bin&lt;/em&gt; en el &lt;em&gt;PATH&lt;/em&gt;. Podemos ahorrarnos comandos invocando directamente esos binarios, por ejemplo &lt;em&gt;pip&lt;/em&gt;. Esto es lo que se ha hecho en el comando anterior.&lt;/p&gt;</content><category term="python"></category><category term="PyPI"></category><category term="wheel"></category><category term="virtualenv"></category><category term="uWSGI"></category></entry><entry><title>El servidor de aplicaciones uWSGI</title><link href="https://www.linuxsysadmin.ml/2016/08/el-servidor-de-aplicaciones-uwsgi.html" rel="alternate"></link><published>2016-08-01T10:00:00+02:00</published><updated>2016-08-01T10:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-08-01:/2016/08/el-servidor-de-aplicaciones-uwsgi.html</id><summary type="html">&lt;p&gt;Estaba yo el otro día buscando un servidor de aplicaciones para aplicaciones &lt;em&gt;python&lt;/em&gt;, y entre todas las opciones encontré uno que es una auténtica joya: &lt;strong&gt;uWSGI&lt;/strong&gt;. Se trata de un servidor modular, que permite servir un amplio abanico de posibilidades en cuanto a lenguajes se refiere, usando un &lt;em&gt;plugin&lt;/em&gt; adecuado …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Estaba yo el otro día buscando un servidor de aplicaciones para aplicaciones &lt;em&gt;python&lt;/em&gt;, y entre todas las opciones encontré uno que es una auténtica joya: &lt;strong&gt;uWSGI&lt;/strong&gt;. Se trata de un servidor modular, que permite servir un amplio abanico de posibilidades en cuanto a lenguajes se refiere, usando un &lt;em&gt;plugin&lt;/em&gt; adecuado.&lt;/p&gt;
&lt;p&gt;Concretamente me llamó la atención el modo de funcionamiento llamado &lt;em&gt;emperor&lt;/em&gt;, que es un proceso que se dedica a monitorizar una carpeta concreta, de forma que se asegura de que cada fichero de configuración mantiene levantada una instancia que la sirva.&lt;/p&gt;
&lt;p&gt;Si levantamos el &lt;em&gt;emperor&lt;/em&gt;, leerá la carpeta de &lt;em&gt;vassals&lt;/em&gt;, levantando todos los que entienda. Si añadimos un fichero de configuración nuevo en caliente, levantará una instancia nueva. Si eliminamos un fichero de configuración, matará la instancia referida. Finalmente, si ese mismo fichero de configuración se modifica (un &lt;em&gt;touch&lt;/em&gt; vale), se adaptará a las nuevas directrices, recargando el código de nuestra aplicación.&lt;/p&gt;
&lt;p&gt;Vamos a empezar instalando la variante &lt;em&gt;emperor&lt;/em&gt;, que no es mas que el &lt;strong&gt;uwsgi&lt;/strong&gt; básico, con una configuración de &lt;em&gt;emperor&lt;/em&gt; y un &lt;em&gt;init script&lt;/em&gt; adecuado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# apt-get install -y uwsgi-emperor
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
...  
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;11&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;2&lt;/span&gt;.258 kB de archivos.
Se utilizarán &lt;span class="m"&gt;5&lt;/span&gt;.724 kB de espacio de disco adicional después de esta operación.
..  
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para entender un lenguaje cualquiera, hay que declarar el uso de un &lt;em&gt;plugin&lt;/em&gt; para ese lenguaje. Vamos a poner los &lt;em&gt;plugins&lt;/em&gt; para tres de los lenguajes mas utilizados, que nos van a servir como demostración para este artículo: &lt;strong&gt;python&lt;/strong&gt;, &lt;strong&gt;PHP&lt;/strong&gt; y &lt;strong&gt;ruby&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# apt-get install -y uwsgi-plugin-python uwsgi-plugin-php uwsgi-plugin-rack-ruby2.1
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
...  
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;22&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;12&lt;/span&gt;,0 MB de archivos.
Se utilizarán &lt;span class="m"&gt;49&lt;/span&gt;,9 MB de espacio de disco adicional después de esta operación.
...  
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Adicionalmente, el &lt;em&gt;plugin&lt;/em&gt; de &lt;strong&gt;ruby&lt;/strong&gt; necesita tener el paquete &lt;em&gt;rack&lt;/em&gt; instalado, así que lo ponemos también.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# apt-get install -y ruby-rack
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
...  
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;8&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;1&lt;/span&gt;.354 kB de archivos.
Se utilizarán &lt;span class="m"&gt;2&lt;/span&gt;.687 kB de espacio de disco adicional después de esta operación.
...  
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En todos los casos, bastará con poner un fichero de configuración en &lt;em&gt;/etc/uwsgi-emperor/vassals/&lt;/em&gt; para activar cada una de las aplicaciones.&lt;/p&gt;
&lt;h2&gt;Sirviendo ficheros PHP&lt;/h2&gt;
&lt;p&gt;Crearemos una carpeta contenedora para nuestros ficheros &lt;strong&gt;PHP&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mkdir /opt/php/
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En esa carpeta vamos a poner algún fichero &lt;em&gt;.php&lt;/em&gt; para tener algo que servir y demostrar que funciona. Con algo simple nos vale.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cat /opt/php/index.php
Hello from PHP, version &amp;lt;?php &lt;span class="nb"&gt;echo&lt;/span&gt; phpversion&lt;span class="o"&gt;()&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; ?&amp;gt;
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a poner un fichero de configuración que sirva &lt;strong&gt;PHP&lt;/strong&gt;, prácticamente copiado de la documentación.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cat /etc/uwsgi-emperor/vassals/php.ini
&lt;span class="o"&gt;[&lt;/span&gt;uwsgi&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;plugins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;:php
http-socket &lt;span class="o"&gt;=&lt;/span&gt; :8080
&lt;span class="nv"&gt;master&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="nv"&gt;workers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="nv"&gt;project_dir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /opt/php/
check-static &lt;span class="o"&gt;=&lt;/span&gt; %&lt;span class="o"&gt;(&lt;/span&gt;project_dir&lt;span class="o"&gt;)&lt;/span&gt;
static-skip-ext &lt;span class="o"&gt;=&lt;/span&gt; .php
static-skip-ext &lt;span class="o"&gt;=&lt;/span&gt; .inc
php-docroot &lt;span class="o"&gt;=&lt;/span&gt; %&lt;span class="o"&gt;(&lt;/span&gt;project_dir&lt;span class="o"&gt;)&lt;/span&gt;
php-allowed-ext &lt;span class="o"&gt;=&lt;/span&gt; .php
php-index &lt;span class="o"&gt;=&lt;/span&gt; index.php
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y el &lt;em&gt;emperor&lt;/em&gt; se dedicará a levantar un proceso para servir esta aplicación. No hay que reiniciar nada. Lo comprobamos con una petición desde una máquina que vea a nuestro servidor:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# wget -qO- http://10.0.0.2:8080/&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
Hello from PHP, version &lt;span class="m"&gt;5&lt;/span&gt;.6.20-0+deb8u1
root@lxc:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Sirviendo una aplicación ruby mediante el protocolo rack&lt;/h2&gt;
&lt;p&gt;Siguiendo los mismos pasos que en el paso anterior, creamos la carpeta contenedora.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mkdir /opt/ruby/
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En esta carpeta ponemos una aplicación &lt;em&gt;rack&lt;/em&gt; mínima, que he copiado de internet. Normalmente, la gente suele usar &lt;em&gt;frameworks&lt;/em&gt;, pero el resultado es el mismo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cat /opt/ruby/ruby.ru
&lt;span class="nv"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; lambda &lt;span class="k"&gt;do&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt;env&lt;span class="p"&gt;|&lt;/span&gt;
  &lt;span class="nv"&gt;body&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Hello, World!&amp;quot;&lt;/span&gt;
  &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;200&lt;/span&gt;, &lt;span class="o"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Content-Type&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="s2"&gt;&amp;quot;text/plain&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;Content-Length&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; body.length.to_s&lt;span class="o"&gt;}&lt;/span&gt;, &lt;span class="o"&gt;[&lt;/span&gt;body&lt;span class="o"&gt;]]&lt;/span&gt; end
run app
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y basta con declarar un fichero de configuración para que se active la nueva aplicación.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cat /etc/uwsgi-emperor/vassals/ruby.ini
&lt;span class="o"&gt;[&lt;/span&gt;uwsgi&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;plugins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; rack_ruby21
http-socket &lt;span class="o"&gt;=&lt;/span&gt; :3031
&lt;span class="nv"&gt;master&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="nv"&gt;workers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="nv"&gt;rack&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /opt/ruby/ruby.ru
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Nuevamente podemos comprobar que el resultado es el esperado:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# wget -qO- http://10.0.0.2:3031/&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
Hello, World!
root@lxc:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Sirviendo una aplicación python mediante el protocolo WSGI&lt;/h2&gt;
&lt;p&gt;Supongamos que tenemos una carpeta contenedora para nuestra aplicación, como en los casos anteriores:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mkdir /opt/py/
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En ella tenemos una aplicación que cumple con el protocolo &lt;strong&gt;WSGI&lt;/strong&gt;. Nuevamente vamos a simplificar el ejemplo a base de no utilizar ningún &lt;em&gt;framework&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cat /opt/py/app.py
def application&lt;span class="o"&gt;(&lt;/span&gt;environ, start_response&lt;span class="o"&gt;)&lt;/span&gt;:
    start_response&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;200 OK&amp;#39;&lt;/span&gt;, &lt;span class="o"&gt;[(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Content-Type&amp;#39;&lt;/span&gt;, &lt;span class="s1"&gt;&amp;#39;text/plain&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)])&lt;/span&gt;
    yield &lt;span class="s1"&gt;&amp;#39;Hello World from python&amp;#39;&lt;/span&gt;
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Configuramos el &lt;em&gt;vassal&lt;/em&gt; que va a dar a conocer la aplicación al &lt;em&gt;emperor&lt;/em&gt;, de forma que este la pueda levantar automáticamente.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cat /etc/uwsgi-emperor/vassals/py.ini
&lt;span class="o"&gt;[&lt;/span&gt;uwsgi&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;plugins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; python
http-socket &lt;span class="o"&gt;=&lt;/span&gt; :5000
&lt;span class="nv"&gt;master&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="nv"&gt;workers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="nv"&gt;chdir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /opt/py/
&lt;span class="nv"&gt;module&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; app:application
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y vemos como todo funciona como debe:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# wget -qO- http://10.0.0.2:5000/&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
Hello World from python
root@lxc:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Es importante indicar que el &lt;em&gt;plugin&lt;/em&gt; de &lt;strong&gt;python&lt;/strong&gt; soporta muchas mas directivas, entre ellas, la posibilidad de añadir variables de sistema como el &lt;em&gt;PYTHONPATH&lt;/em&gt;, o la de usar un &lt;em&gt;virtualenv&lt;/em&gt; propio para nuestra aplicación. Es por este motivo que me enamoré de este servidor de aplicaciones.&lt;/p&gt;</content><category term="uWSGI"></category><category term="plugins"></category><category term="PHP"></category><category term="ruby"></category><category term="python"></category></entry><entry><title>Instalando una máquina con Archlinux</title><link href="https://www.linuxsysadmin.ml/2016/07/instalando-una-maquina-con-archlinux.html" rel="alternate"></link><published>2016-07-04T20:00:00+02:00</published><updated>2016-07-04T20:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-07-04:/2016/07/instalando-una-maquina-con-archlinux.html</id><summary type="html">&lt;p&gt;Hoy quiero presentar una distribución de &lt;em&gt;linux&lt;/em&gt; que es una maravilla; es rápida, altamente actualizada, y lo último en innovación. Se trata de una distribución tipo &lt;em&gt;rolling&lt;/em&gt;, con una filosofía de última tendencia que es especialmente útil en un entorno no tan crítico, como puede ser una máquina tipo escritorio …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Hoy quiero presentar una distribución de &lt;em&gt;linux&lt;/em&gt; que es una maravilla; es rápida, altamente actualizada, y lo último en innovación. Se trata de una distribución tipo &lt;em&gt;rolling&lt;/em&gt;, con una filosofía de última tendencia que es especialmente útil en un entorno no tan crítico, como puede ser una máquina tipo escritorio.&lt;/p&gt;
&lt;p&gt;La parte menos buena, a parte del hecho de que los paquetes cambian mucho y pueden entrar algunos con algún fallo menor, es que la instalación no cuenta con un &lt;em&gt;wizard&lt;/em&gt;, aunque en el proceso podemos aprender como funciona fácilmente.&lt;/p&gt;
&lt;p&gt;Vamos a seguir bastante el procedimiento oficial de instalación, que podemos encontrar &lt;a href="https://wiki.archlinux.org/index.php/installation_guide"&gt;aquí&lt;/a&gt;. Este procedimiento de instalación lo vamos a lanzar sobre una máquina virtual, con un disco de 4gb y 512mb de memoria, aunque sin entorno gráfico necesita muchos menos recursos.&lt;/p&gt;
&lt;p&gt;El primer paso consiste en descargar una imagen de instalación que vamos a introducir (o montar, que es el equivalente en &lt;em&gt;VirtualBox&lt;/em&gt;), previo encendido de la máquina.&lt;/p&gt;
&lt;p&gt;Un detalle es que la imagen de instalación lleva instalado un servidor &lt;em&gt;SSH&lt;/em&gt;, que nos viene muy bien para capturar la salida de los diferentes comandos. Solo hay que levantar el servicio y darle una contraseña al usuario &lt;strong&gt;root&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@archiso ~ &lt;span class="c1"&gt;# passwd&lt;/span&gt;
Enter new UNIX password:
Retype new UNIX password:
passwd: password updated successfully
root@archiso ~ &lt;span class="c1"&gt;# systemctl start sshd&lt;/span&gt;
root@archiso ~ &lt;span class="c1"&gt;# &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A partir de aquí, sigo el procedimiento desde una sesión &lt;em&gt;SSH&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Preparaciones&lt;/h2&gt;
&lt;p&gt;Aunque esto no es necesario, es recomendable usar nuestro teclado favorito. No hay nada mas frustrante que darle a una tecla pensando en un carácter y que te salga otro. Así que vamos a cargar la distribución de teclado que nos parezca.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@archiso ~ &lt;span class="c1"&gt;# loadkeys es&lt;/span&gt;
root@archiso ~ &lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El siguiente paso consiste en configurar la red que se va a usar durante la instalación. Por defecto viene preparado para usar &lt;em&gt;DHCP&lt;/em&gt;, que nos vale, así que la dejamos como está.&lt;/p&gt;
&lt;p&gt;Uno de los pasos mas importantes de toda la instalación es el particionado. Hacerlo mal en este punto es un problema futuro, y de hecho, mucha gente utiliza tecnologías como &lt;em&gt;LVM&lt;/em&gt; que les dan cierta flexibilidad para cambios futuros.&lt;/p&gt;
&lt;p&gt;En nuestro caso concreto, se trata de una máquina virtual que no va a durar mucho, así que nos basta con hacerlo a un nivel aceptable. Como disponemos de un solo disco de 4gb, vamos a particionarlo en dos, uno para el disco local, y otro para la partición de &lt;em&gt;swap&lt;/em&gt; (una pequeña, que no nos sobra el disco). Personalmente he usado &lt;strong&gt;cfdisk&lt;/strong&gt;, que me parece mas intuitivo que el resto, dejando las particiones de esta manera:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@archiso ~ &lt;span class="c1"&gt;# fdisk -l&lt;/span&gt;
Disk /dev/sda: &lt;span class="m"&gt;4&lt;/span&gt; GiB, &lt;span class="m"&gt;4294967296&lt;/span&gt; bytes, &lt;span class="m"&gt;8388608&lt;/span&gt; sectors
Units: sectors of &lt;span class="m"&gt;1&lt;/span&gt; * &lt;span class="nv"&gt;512&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;512&lt;/span&gt; bytes
Sector size &lt;span class="o"&gt;(&lt;/span&gt;logical/physical&lt;span class="o"&gt;)&lt;/span&gt;: &lt;span class="m"&gt;512&lt;/span&gt; bytes / &lt;span class="m"&gt;512&lt;/span&gt; bytes
I/O size &lt;span class="o"&gt;(&lt;/span&gt;minimum/optimal&lt;span class="o"&gt;)&lt;/span&gt;: &lt;span class="m"&gt;512&lt;/span&gt; bytes / &lt;span class="m"&gt;512&lt;/span&gt; bytes
Disklabel type: gpt
Disk identifier: A45FE619-7FFC-4EA2-8253-628FD2138198

Device       Start     End Sectors  Size Type
/dev/sda1     &lt;span class="m"&gt;2048&lt;/span&gt; &lt;span class="m"&gt;7317503&lt;/span&gt; &lt;span class="m"&gt;7315456&lt;/span&gt;  &lt;span class="m"&gt;3&lt;/span&gt;.5G Linux filesystem
/dev/sda2  &lt;span class="m"&gt;7317504&lt;/span&gt; &lt;span class="m"&gt;8388574&lt;/span&gt; &lt;span class="m"&gt;1071071&lt;/span&gt;  523M Linux swap


Disk /dev/loop0: &lt;span class="m"&gt;318&lt;/span&gt;.9 MiB, &lt;span class="m"&gt;334385152&lt;/span&gt; bytes, &lt;span class="m"&gt;653096&lt;/span&gt; sectors
Units: sectors of &lt;span class="m"&gt;1&lt;/span&gt; * &lt;span class="nv"&gt;512&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;512&lt;/span&gt; bytes
Sector size &lt;span class="o"&gt;(&lt;/span&gt;logical/physical&lt;span class="o"&gt;)&lt;/span&gt;: &lt;span class="m"&gt;512&lt;/span&gt; bytes / &lt;span class="m"&gt;512&lt;/span&gt; bytes
I/O size &lt;span class="o"&gt;(&lt;/span&gt;minimum/optimal&lt;span class="o"&gt;)&lt;/span&gt;: &lt;span class="m"&gt;512&lt;/span&gt; bytes / &lt;span class="m"&gt;512&lt;/span&gt; bytes
root@archiso ~ &lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Siguiendo el manual, necesitamos formatear las particiones según las funciones que van a desempeñar, montando los discos en &lt;em&gt;/mnt/&lt;/em&gt; y sus subcarpetas. Como no tenemos particiones para &lt;em&gt;/home/&lt;/em&gt;, &lt;em&gt;/var/&lt;/em&gt; y &lt;em&gt;/tmp/&lt;/em&gt;, con montar la primera nos basta.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@archiso ~ &lt;span class="c1"&gt;# mkfs.ext4 /dev/sda1&lt;/span&gt;
mke2fs &lt;span class="m"&gt;1&lt;/span&gt;.42.13 &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;17&lt;/span&gt;-May-2015&lt;span class="o"&gt;)&lt;/span&gt;
Creating filesystem with &lt;span class="m"&gt;914432&lt;/span&gt; 4k blocks and &lt;span class="m"&gt;228928&lt;/span&gt; inodes
Filesystem UUID: 295665be-7b09-4cad-9689-7fed5471bf25
Superblock backups stored on blocks:
        &lt;span class="m"&gt;32768&lt;/span&gt;, &lt;span class="m"&gt;98304&lt;/span&gt;, &lt;span class="m"&gt;163840&lt;/span&gt;, &lt;span class="m"&gt;229376&lt;/span&gt;, &lt;span class="m"&gt;294912&lt;/span&gt;, &lt;span class="m"&gt;819200&lt;/span&gt;, &lt;span class="m"&gt;884736&lt;/span&gt;

Allocating group tables: &lt;span class="k"&gt;done&lt;/span&gt;
Writing inode tables: &lt;span class="k"&gt;done&lt;/span&gt;
Creating journal &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;16384&lt;/span&gt; blocks&lt;span class="o"&gt;)&lt;/span&gt;: &lt;span class="k"&gt;done&lt;/span&gt;
Writing superblocks and filesystem accounting information: &lt;span class="k"&gt;done&lt;/span&gt;

root@archiso ~ &lt;span class="c1"&gt;# mount /dev/sda1 /mnt&lt;/span&gt;
root@archiso ~ &lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Preparamos la partición de &lt;em&gt;swap&lt;/em&gt; y la dejamos activada. Eso nos permitirá utilizarla durante la instalación, y que esta la detecte automáticamente para crear el fichero &lt;em&gt;/etc/fstab&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@archiso ~ &lt;span class="c1"&gt;# mkswap /dev/sda2&lt;/span&gt;
Setting up swapspace version &lt;span class="m"&gt;1&lt;/span&gt;, &lt;span class="nv"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;523&lt;/span&gt; MiB &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;548380672&lt;/span&gt; bytes&lt;span class="o"&gt;)&lt;/span&gt;
no label, &lt;span class="nv"&gt;UUID&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;85cf7f55-cd3c-4002-9729-2d89ebadf942
root@archiso ~ &lt;span class="c1"&gt;# swapon /dev/sda2&lt;/span&gt;
root@archiso ~ &lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos verificar que está activada con un comando &lt;strong&gt;free&lt;/strong&gt;, por ejemplo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@archiso ~ &lt;span class="c1"&gt;# free -m&lt;/span&gt;
              total        used        free      shared  buff/cache   available
Mem:            &lt;span class="m"&gt;498&lt;/span&gt;          &lt;span class="m"&gt;20&lt;/span&gt;         &lt;span class="m"&gt;252&lt;/span&gt;          &lt;span class="m"&gt;44&lt;/span&gt;         &lt;span class="m"&gt;224&lt;/span&gt;         &lt;span class="m"&gt;413&lt;/span&gt;
Swap:           &lt;span class="m"&gt;522&lt;/span&gt;           &lt;span class="m"&gt;0&lt;/span&gt;         &lt;span class="m"&gt;522&lt;/span&gt;
root@archiso ~ &lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Instalación y configuración&lt;/h2&gt;
&lt;p&gt;El primer paso es descargar todos los paquetes de internet, mediante el comando &lt;strong&gt;pacstrap&lt;/strong&gt;. Para ello se recomienda editar el fichero &lt;em&gt;/etc/pacman.d/mirrorlist&lt;/em&gt; para utilizar los &lt;em&gt;mirrors&lt;/em&gt; que nos convengan, y que serán también los que use el sistema instalado. Como se pueden cambiar a &lt;em&gt;posteriori&lt;/em&gt; y los que hay me parecen bien, no vamos a cambiar nada.&lt;/p&gt;
&lt;p&gt;Así pues, lanzamos el &lt;strong&gt;pacstrap&lt;/strong&gt; tal como indica el manual de instalación.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@archiso ~ &lt;span class="c1"&gt;# pacstrap /mnt base&lt;/span&gt;
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Creating install root at /mnt
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Installing packages to /mnt
:: Synchronizing package databases...
 core                                                 &lt;span class="m"&gt;119&lt;/span&gt;.7 KiB   783K/s &lt;span class="m"&gt;00&lt;/span&gt;:00 &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="c1"&gt;############################################] 100%&lt;/span&gt;
 extra                                               &lt;span class="m"&gt;1755&lt;/span&gt;.7 KiB   810K/s &lt;span class="m"&gt;00&lt;/span&gt;:02 &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="c1"&gt;############################################] 100%&lt;/span&gt;
 community                                              &lt;span class="m"&gt;3&lt;/span&gt;.5 MiB   851K/s &lt;span class="m"&gt;00&lt;/span&gt;:04 &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="c1"&gt;############################################] 100%&lt;/span&gt;
:: There are &lt;span class="m"&gt;50&lt;/span&gt; members in group base:
:: Repository core
   &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; bash  &lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; bzip2  &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; coreutils  &lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; cryptsetup  &lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; device-mapper  &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; dhcpcd  &lt;span class="m"&gt;7&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; diffutils  &lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; e2fsprogs  &lt;span class="m"&gt;9&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; file
   &lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; filesystem  &lt;span class="m"&gt;11&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; findutils  &lt;span class="m"&gt;12&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; gawk  &lt;span class="m"&gt;13&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; gcc-libs  &lt;span class="m"&gt;14&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; gettext  &lt;span class="m"&gt;15&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; glibc  &lt;span class="m"&gt;16&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; grep  &lt;span class="m"&gt;17&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; gzip  &lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; inetutils  &lt;span class="m"&gt;19&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; iproute2
   &lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; iputils  &lt;span class="m"&gt;21&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; jfsutils  &lt;span class="m"&gt;22&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; less  &lt;span class="m"&gt;23&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; licenses  &lt;span class="m"&gt;24&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; linux  &lt;span class="m"&gt;25&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; logrotate  &lt;span class="m"&gt;26&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; lvm2  &lt;span class="m"&gt;27&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; man-db  &lt;span class="m"&gt;28&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; man-pages  &lt;span class="m"&gt;29&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; mdadm
   &lt;span class="m"&gt;30&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; nano  &lt;span class="m"&gt;31&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; netctl  &lt;span class="m"&gt;32&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; pacman  &lt;span class="m"&gt;33&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; pciutils  &lt;span class="m"&gt;34&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; pcmciautils  &lt;span class="m"&gt;35&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; perl  &lt;span class="m"&gt;36&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; procps-ng  &lt;span class="m"&gt;37&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; psmisc  &lt;span class="m"&gt;38&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; reiserfsprogs
   &lt;span class="m"&gt;39&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; s-nail  &lt;span class="m"&gt;40&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; sed  &lt;span class="m"&gt;41&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; shadow  &lt;span class="m"&gt;42&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; sysfsutils  &lt;span class="m"&gt;43&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; systemd-sysvcompat  &lt;span class="m"&gt;44&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; tar  &lt;span class="m"&gt;45&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; texinfo  &lt;span class="m"&gt;46&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; usbutils  &lt;span class="m"&gt;47&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; util-linux
   &lt;span class="m"&gt;48&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; vi  &lt;span class="m"&gt;49&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; which  &lt;span class="m"&gt;50&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; xfsprogs

Enter a selection &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;all&lt;span class="o"&gt;)&lt;/span&gt;:
...  
Total Download Size:   &lt;span class="m"&gt;185&lt;/span&gt;.14 MiB
Total Installed Size:  &lt;span class="m"&gt;572&lt;/span&gt;.80 MiB

:: Proceed with installation? &lt;span class="o"&gt;[&lt;/span&gt;Y/n&lt;span class="o"&gt;]&lt;/span&gt;
...
pacstrap /mnt base  &lt;span class="m"&gt;51&lt;/span&gt;.53s user &lt;span class="m"&gt;124&lt;/span&gt;.01s system &lt;span class="m"&gt;39&lt;/span&gt;% cpu &lt;span class="m"&gt;7&lt;/span&gt;:25.44 total
root@archiso ~ &lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con todos los paquetes instalados, empezaremos a configurar el sistema. El primer candidato es generar el fichero &lt;em&gt;/etc/fstab&lt;/em&gt;. Existe un &lt;em&gt;script&lt;/em&gt; llamado &lt;strong&gt;genfstab&lt;/strong&gt; que va a generar un fichero &lt;em&gt;fstab&lt;/em&gt; basado en lo que tenemos ahora mismo activado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@archiso ~ &lt;span class="c1"&gt;# genfstab -p /mnt &amp;gt;&amp;gt; /mnt/etc/fstab&lt;/span&gt;
root@archiso ~ &lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El resto de configuración se hace en un entorno &lt;strong&gt;chroot&lt;/strong&gt; sobre la carpeta instalada, que es donde tenemos montado el disco raíz. La imagen de instalación nos ofrece un &lt;em&gt;script&lt;/em&gt; de &lt;strong&gt;chroot&lt;/strong&gt; que ya se encarga de montar los sistemas de ficheros especiales como &lt;em&gt;/proc/&lt;/em&gt;, &lt;em&gt;/dev/&lt;/em&gt; o &lt;em&gt;/sys/&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@archiso ~ &lt;span class="c1"&gt;# arch-chroot /mnt&lt;/span&gt;
sh-4.3#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Los siguientes pasos son burocráticos y los mismos que en otras distribuciones: poner un nombre a la máquina, configurar el huso horario, generar &lt;em&gt;locales&lt;/em&gt; y configurar el teclado a nivel permanente.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sh-4.3# &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;archlinux&amp;quot;&lt;/span&gt; &amp;gt; /etc/hostname
sh-4.3# ln -s /usr/share/zoneinfo/Europe/Madrid /etc/localtime
sh-4.3# grep -v ^# /etc/locale.gen
es_ES.UTF-8 UTF-8
sh-4.3# locale-gen
Generating locales...
  es_ES.UTF-8... &lt;span class="k"&gt;done&lt;/span&gt;
Generation complete.
sh-4.3# &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;LANG=es_ES.UTF-8&amp;quot;&lt;/span&gt; &amp;gt; /etc/locale.conf
sh-4.3# &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;KEYMAP=es&amp;quot;&lt;/span&gt; &amp;gt; /etc/vconsole.conf
sh-4.3#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El siguiente paso no es fácil, y aunque puede hacerse a &lt;em&gt;posteriori&lt;/em&gt;, merece la pena prestar atención. Para la configuración de red, necesitamos activar el servicio &lt;em&gt;systemd-networkd&lt;/em&gt;, que va a leer los ficheros de configuración en &lt;em&gt;/etc/systemd/network/&lt;/em&gt; para levantar las interfaces con los parámetros adecuados.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sh-4.3# systemctl &lt;span class="nb"&gt;enable&lt;/span&gt; systemd-networkd
Created symlink /etc/systemd/system/multi-user.target.wants/systemd-networkd.service → /usr/lib/systemd/system/systemd-networkd.service.
Created symlink /etc/systemd/system/sockets.target.wants/systemd-networkd.socket → /usr/lib/systemd/system/systemd-networkd.socket.
sh-4.3# cat /etc/systemd/network/wired.network
&lt;span class="o"&gt;[&lt;/span&gt;Match&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;Name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;enp0s3

&lt;span class="o"&gt;[&lt;/span&gt;Network&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;DHCP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;ipv4
sh-4.3#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En caso de querer obtener los &lt;em&gt;DNS&lt;/em&gt; de forma automática necesitamos habilitar el servicio &lt;em&gt;systemd-resolved&lt;/em&gt;, que nos va a dejar un &lt;em&gt;resolv.conf&lt;/em&gt; en &lt;em&gt;/run/systemd/resolve/&lt;/em&gt;; con un simple enlace va a ser suficiente.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sh-4.3# systemctl &lt;span class="nb"&gt;enable&lt;/span&gt; systemd-resolved
Created symlink /etc/systemd/system/multi-user.target.wants/systemd-resolved.service → /usr/lib/systemd/system/systemd-resolved.service.
sh-4.3# rm /etc/resolv.conf
sh-4.3# ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf
sh-4.3#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Generamos un &lt;em&gt;initramfs&lt;/em&gt; para que en el siguiente arranque podamos disfrutar de todo lo nuevo que hemos configurado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sh-4.3# mkinitcpio -p &lt;span class="nv"&gt;linux&lt;/span&gt;
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Building image from preset: /etc/mkinitcpio.d/linux.preset: &lt;span class="s1"&gt;&amp;#39;default&amp;#39;&lt;/span&gt;
  -&amp;gt; -k /boot/vmlinuz-linux -c /etc/mkinitcpio.conf -g /boot/initramfs-linux.img
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Starting build: &lt;span class="m"&gt;4&lt;/span&gt;.5.4-1-ARCH
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;base&lt;span class="o"&gt;]&lt;/span&gt;
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;udev&lt;span class="o"&gt;]&lt;/span&gt;
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;autodetect&lt;span class="o"&gt;]&lt;/span&gt;
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;modconf&lt;span class="o"&gt;]&lt;/span&gt;
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;block&lt;span class="o"&gt;]&lt;/span&gt;
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;filesystems&lt;span class="o"&gt;]&lt;/span&gt;
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;keyboard&lt;span class="o"&gt;]&lt;/span&gt;
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;fsck&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Generating module &lt;span class="nv"&gt;dependencies&lt;/span&gt;
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Creating gzip-compressed initcpio image: /boot/initramfs-linux.img
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Image generation &lt;span class="nv"&gt;successful&lt;/span&gt;
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Building image from preset: /etc/mkinitcpio.d/linux.preset: &lt;span class="s1"&gt;&amp;#39;fallback&amp;#39;&lt;/span&gt;
  -&amp;gt; -k /boot/vmlinuz-linux -c /etc/mkinitcpio.conf -g /boot/initramfs-linux-fallback.img -S &lt;span class="nv"&gt;autodetect&lt;/span&gt;
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Starting build: &lt;span class="m"&gt;4&lt;/span&gt;.5.4-1-ARCH
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;base&lt;span class="o"&gt;]&lt;/span&gt;
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;udev&lt;span class="o"&gt;]&lt;/span&gt;
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;modconf&lt;span class="o"&gt;]&lt;/span&gt;
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;block&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; WARNING: Possibly missing firmware &lt;span class="k"&gt;for&lt;/span&gt; module: &lt;span class="nv"&gt;aic94xx&lt;/span&gt;
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; WARNING: Possibly missing firmware &lt;span class="k"&gt;for&lt;/span&gt; module: wd719x
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;filesystems&lt;span class="o"&gt;]&lt;/span&gt;
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;keyboard&lt;span class="o"&gt;]&lt;/span&gt;
  -&amp;gt; Running build hook: &lt;span class="o"&gt;[&lt;/span&gt;fsck&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Generating module &lt;span class="nv"&gt;dependencies&lt;/span&gt;
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Creating gzip-compressed initcpio image: /boot/initramfs-linux-fallback.img
&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; Image generation successful
sh-4.3#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y finalmente nos podemos dedicar a administrar usuarios y sus contraseñas. Como esto también se puede hacer a &lt;em&gt;posteriori&lt;/em&gt;, voy solo a desbloquear al usuario &lt;strong&gt;root&lt;/strong&gt;, dándole una &lt;em&gt;password&lt;/em&gt; adecuada.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sh-4.3# passwd
Enter new UNIX password:
Retype new UNIX password:
passwd: password updated successfully
sh-4.3#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto tenemos el disco raíz perfectamente preparado, y un &lt;em&gt;kernel&lt;/em&gt; listo para el arranque.&lt;/p&gt;
&lt;h2&gt;Instalando el bootloader&lt;/h2&gt;
&lt;p&gt;Para que el &lt;em&gt;kernel&lt;/em&gt;, &lt;em&gt;initrd&lt;/em&gt; y el disco puedan funcionar, es necesario que el disco tenga algún tipo de estructura que le indique como hacerlo. El nombre genérico para esta pieza de &lt;em&gt;software&lt;/em&gt; es &lt;em&gt;bootloader&lt;/em&gt;. De todos los que hay (que no son pocos), vamos a usar un viejo amigo: &lt;strong&gt;GRUB&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sh-4.3# pacman -S grub
resolving dependencies...
looking &lt;span class="k"&gt;for&lt;/span&gt; conflicting packages...

Packages &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; grub-1:2.02.beta2-6

Total Download Size:    &lt;span class="m"&gt;5&lt;/span&gt;.27 MiB
Total Installed Size:  &lt;span class="m"&gt;25&lt;/span&gt;.27 MiB

:: Proceed with installation? &lt;span class="o"&gt;[&lt;/span&gt;Y/n&lt;span class="o"&gt;]&lt;/span&gt; y
...
sh-4.3#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Instalamos el código de &lt;em&gt;boot&lt;/em&gt; en el &lt;em&gt;MBR&lt;/em&gt; con la herramienta que &lt;strong&gt;GRUB&lt;/strong&gt; nos ofrece, siguiendo el manual.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sh-4.3# grub-install --target&lt;span class="o"&gt;=&lt;/span&gt;i386-pc /dev/sda
Installing &lt;span class="k"&gt;for&lt;/span&gt; i386-pc platform.
grub-install: warning: this GPT partition label contains no BIOS Boot Partition&lt;span class="p"&gt;;&lt;/span&gt; embedding won&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;t be possible.
grub-install: warning: Embedding is not possible.  GRUB can only be installed in this setup by using blocklists.  However, blocklists are UNRELIABLE and their use is discouraged..
grub-install: error: will not proceed with blocklists.
sh-4.3#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En este caso ha fallado, pero siguiendo el manual de instalación, eso se corrige mediante el uso del &lt;em&gt;flag&lt;/em&gt; &lt;strong&gt;--force&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sh-4.3# grub-install --target&lt;span class="o"&gt;=&lt;/span&gt;i386-pc /dev/sda --force
Installing &lt;span class="k"&gt;for&lt;/span&gt; i386-pc platform.
grub-install: warning: this GPT partition label contains no BIOS Boot Partition&lt;span class="p"&gt;;&lt;/span&gt; embedding won&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;t be possible.
grub-install: warning: Embedding is not possible.  GRUB can only be installed in this setup by using blocklists.  However, blocklists are UNRELIABLE and their use is discouraged..
Installation finished. No error reported.
sh-4.3#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Como ya hemos conseguido instalar &lt;strong&gt;GRUB&lt;/strong&gt; de forma exitosa, nos queda generar un fichero de configuración del &lt;em&gt;bootloader&lt;/em&gt;, tal como dice el manual de instalación.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sh-4.3# grub-mkconfig -o /boot/grub/grub.cfg
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-linux
Found initrd image: /boot/initramfs-linux.img
Found fallback initramfs image: /boot/initramfs-linux-fallback.img
&lt;span class="k"&gt;done&lt;/span&gt;
sh-4.3#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto queda una instalación básica. Solo nos queda salir del entorno enjaulado, apagar la máquina, quitar el disco de instalación y encender.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sh-4.3# &lt;span class="nb"&gt;exit&lt;/span&gt;
&lt;span class="nb"&gt;exit&lt;/span&gt;
arch-chroot /mnt  &lt;span class="m"&gt;10&lt;/span&gt;.17s user &lt;span class="m"&gt;19&lt;/span&gt;.71s system &lt;span class="m"&gt;3&lt;/span&gt;% cpu &lt;span class="m"&gt;14&lt;/span&gt;:10.46 total
root has logged on pts/1 from &lt;span class="m"&gt;10&lt;/span&gt;.0.2.2.
root@archiso ~ &lt;span class="c1"&gt;# reboot&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Resultado final&lt;/h2&gt;
&lt;p&gt;Tras actualizar y limpiar caché de paquetes, vemos que tenemos una distribución minimalista con 743mb de disco ocupados y 10mb de memoria.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;root@archlinux ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# df -h&lt;/span&gt;
S.ficheros     Tamaño Usados  Disp Uso% Montado en
dev              247M      &lt;span class="m"&gt;0&lt;/span&gt;  247M   &lt;span class="m"&gt;0&lt;/span&gt;% /dev
run              250M   292K  249M   &lt;span class="m"&gt;1&lt;/span&gt;% /run
/dev/sda1        &lt;span class="m"&gt;3&lt;/span&gt;,4G   743M  &lt;span class="m"&gt;2&lt;/span&gt;,5G  &lt;span class="m"&gt;23&lt;/span&gt;% /
tmpfs            250M      &lt;span class="m"&gt;0&lt;/span&gt;  250M   &lt;span class="m"&gt;0&lt;/span&gt;% /dev/shm
tmpfs            250M      &lt;span class="m"&gt;0&lt;/span&gt;  250M   &lt;span class="m"&gt;0&lt;/span&gt;% /sys/fs/cgroup
tmpfs            250M      &lt;span class="m"&gt;0&lt;/span&gt;  250M   &lt;span class="m"&gt;0&lt;/span&gt;% /tmp
tmpfs             50M      &lt;span class="m"&gt;0&lt;/span&gt;   50M   &lt;span class="m"&gt;0&lt;/span&gt;% /run/user/0
&lt;span class="o"&gt;[&lt;/span&gt;root@archlinux ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;# free -m&lt;/span&gt;
              total        used        free      shared  buff/cache   available
Mem:            &lt;span class="m"&gt;498&lt;/span&gt;          &lt;span class="m"&gt;10&lt;/span&gt;         &lt;span class="m"&gt;441&lt;/span&gt;           &lt;span class="m"&gt;0&lt;/span&gt;          &lt;span class="m"&gt;46&lt;/span&gt;         &lt;span class="m"&gt;470&lt;/span&gt;
Swap:           &lt;span class="m"&gt;522&lt;/span&gt;           &lt;span class="m"&gt;0&lt;/span&gt;         &lt;span class="m"&gt;522&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;root@archlinux ~&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A partir de aquí podemos construir a base de instalar aquellos paquetes que necesitemos (escritorio, servicios, ...); sin embargo, esto queda para futuros artículos.&lt;/p&gt;</content><category term="linux"></category><category term="archlinux"></category><category term="distribución"></category></entry><entry><title>Balanceando peticiones con HAProxy</title><link href="https://www.linuxsysadmin.ml/2016/06/balanceando-peticiones-con-haproxy.html" rel="alternate"></link><published>2016-06-27T08:00:00+02:00</published><updated>2016-06-27T08:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-06-27:/2016/06/balanceando-peticiones-con-haproxy.html</id><summary type="html">&lt;p&gt;Cuando tenemos un entorno grande o con previsiones de crecimiento, nos interesa poder poner a trabajar varios servidores similares. En casos así nos hace falta un &lt;strong&gt;balanceador de carga&lt;/strong&gt;, que actúa como un agente de tráfico, dirigiendo las peticiones que él mismo recibe a los diferentes servidores, por ejemplo, &lt;strong&gt;haproxy …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Cuando tenemos un entorno grande o con previsiones de crecimiento, nos interesa poder poner a trabajar varios servidores similares. En casos así nos hace falta un &lt;strong&gt;balanceador de carga&lt;/strong&gt;, que actúa como un agente de tráfico, dirigiendo las peticiones que él mismo recibe a los diferentes servidores, por ejemplo, &lt;strong&gt;haproxy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;En este artículo vamos a montar el balanceador &lt;strong&gt;haproxy&lt;/strong&gt;, usando &lt;strong&gt;ansible&lt;/strong&gt;, basándonos en las imágenes &lt;strong&gt;docker&lt;/strong&gt; de &lt;a href="https://www.linuxsysadmin.ml/2016/06/controlando-contenedores-docker-con-ansible.html"&gt;otro artículo&lt;/a&gt;. La ideas es que vamos a poner un único balanceador que va a escuchar en dos puertos, balanceando dos &lt;em&gt;backends&lt;/em&gt; en cada uno, por ejemplo una web y una &lt;em&gt;api&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
slave               latest              22a9312a1315        About an hour ago   &lt;span class="m"&gt;186&lt;/span&gt; MB
ansible             latest              225b431d2133        About an hour ago   &lt;span class="m"&gt;245&lt;/span&gt;.5 MB
debian              latest              bb5d89f9b6cb        &lt;span class="m"&gt;2&lt;/span&gt; weeks ago         &lt;span class="m"&gt;125&lt;/span&gt;.1 MB
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Preparando el entorno&lt;/h2&gt;
&lt;p&gt;El primer paso es disponer de una red para que todos los servidores implicados se puedan comunicar entre ellos. La red que viene por defecto nos permite eso, pero vamos a crear una red &lt;em&gt;user defined&lt;/em&gt; que nos va a permitir que los contenedores &lt;strong&gt;docker&lt;/strong&gt; se conozcan entre ellos por su nombre.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ docker network create --subnet&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;172&lt;/span&gt;.20.0.0/16 balancing
4585a1abd0ed69bc9d1daf0dd019e1f129a9e7328471da77541f5b4a54c19626
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Se trata de levantar 5 servidores: 1 para &lt;strong&gt;haproxy&lt;/strong&gt; y otros 4 para representar los servidores de &lt;em&gt;backend&lt;/em&gt;, que vamos a trucar para que parezca lo que no son. Es importante &lt;em&gt;publicar&lt;/em&gt; los puertos que queramos exponer, para ver que funciona la solución final; si queremos balancear los puertos 8080 (web), 8081 (api) y 1936 (haproxy stats), pondríamos algo como esto:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~$ docker run -d -h balancer --name balancer --net balancing -p &lt;span class="m"&gt;8080&lt;/span&gt;:8080 -p &lt;span class="m"&gt;8081&lt;/span&gt;:8081 -p &lt;span class="m"&gt;1936&lt;/span&gt;:1936 slave
70e4811e6a498c7ecdec11ed91609d43749c327e33bd3b06b1532b507f3f2141
gerard@sirius:~/build$ &lt;span class="k"&gt;for&lt;/span&gt; host in web1 web2 api1 api2&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt; docker run -d -h &lt;span class="nv"&gt;$host&lt;/span&gt; --name &lt;span class="nv"&gt;$host&lt;/span&gt; --net balancing slave&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;done&lt;/span&gt;
82be4f7788f0186834311fe625f4ae24908a59a25df8b246aeb18d13cdff7b3d
a0514880a44ed9109d19dc594ae991245e052554d91f0b08da80d57808df3d29
1217fd05aa321e52e0038e4870dd29776e843d88de03520ad0bffee6cb786b54
635c9b2d04482018053dca4b4225a34d3a11be1c366e372b9536fcadea12a15a
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Verificamos que tenemos todos nuestros contenedores corriendo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ docker ps
CONTAINER ID        IMAGE               COMMAND               CREATED              STATUS              PORTS                                                      NAMES
635c9b2d0448        slave               &amp;quot;/usr/sbin/sshd -D&amp;quot;   10 seconds ago       Up 8 seconds                                                                   api2
1217fd05aa32        slave               &amp;quot;/usr/sbin/sshd -D&amp;quot;   11 seconds ago       Up 9 seconds                                                                   api1
a0514880a44e        slave               &amp;quot;/usr/sbin/sshd -D&amp;quot;   12 seconds ago       Up 10 seconds                                                                  web2
82be4f7788f0        slave               &amp;quot;/usr/sbin/sshd -D&amp;quot;   13 seconds ago       Up 12 seconds                                                                  web1
70e4811e6a49        slave               &amp;quot;/usr/sbin/sshd -D&amp;quot;   About a minute ago   Up About a minute   0.0.0.0:1936-&amp;gt;1936/tcp, 0.0.0.0:8080-8081-&amp;gt;8080-8081/tcp   balancer
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Preparando las herramientas&lt;/h2&gt;
&lt;p&gt;Como ya hemos comentado, vamos a utilizar &lt;strong&gt;ansible&lt;/strong&gt;. Para ejecutar los &lt;em&gt;playbooks&lt;/em&gt;, vamos a levantar una máquina para usar y tirar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ docker run -ti --rm --net balancing -h ansible --name ansible ansible
root@ansible:/# &lt;span class="nb"&gt;cd&lt;/span&gt; /root
root@ansible:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Especificamos un fichero de &lt;em&gt;hosts&lt;/em&gt; que va a servir para indicar los contenedores que tenemos y vamos a instalar paquetes en función de su grupo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# cat hosts 
&lt;span class="o"&gt;[&lt;/span&gt;all:vars&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;ansible_user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; ansible
&lt;span class="nv"&gt;ansible_ssh_pass&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; s3cr3t
&lt;span class="nv"&gt;ansible_become&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="nv"&gt;ansible_become_method&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; sudo
&lt;span class="nv"&gt;ansible_become_user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; root
&lt;span class="nv"&gt;ansible_become_pass&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; s3cr3t

&lt;span class="o"&gt;[&lt;/span&gt;balancers&lt;span class="o"&gt;]&lt;/span&gt;
balancer

&lt;span class="o"&gt;[&lt;/span&gt;webs&lt;span class="o"&gt;]&lt;/span&gt;
web1
web2

&lt;span class="o"&gt;[&lt;/span&gt;apis&lt;span class="o"&gt;]&lt;/span&gt;
api1
api2
root@ansible:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Opcionalmente, verificamos que todos los contenedores son accesibles desde &lt;strong&gt;ansible&lt;/strong&gt;. Eso nos puede evitar sorpresas futuras.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# ansible -i hosts -m ping all
balancer &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;SUCCESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;changed&amp;quot;&lt;/span&gt;: false, 
    &lt;span class="s2"&gt;&amp;quot;ping&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;pong&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
api2 &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;SUCCESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;changed&amp;quot;&lt;/span&gt;: false, 
    &lt;span class="s2"&gt;&amp;quot;ping&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;pong&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
api1 &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;SUCCESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;changed&amp;quot;&lt;/span&gt;: false, 
    &lt;span class="s2"&gt;&amp;quot;ping&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;pong&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
web2 &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;SUCCESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;changed&amp;quot;&lt;/span&gt;: false, 
    &lt;span class="s2"&gt;&amp;quot;ping&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;pong&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
web1 &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="nv"&gt;SUCCESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;changed&amp;quot;&lt;/span&gt;: false, 
    &lt;span class="s2"&gt;&amp;quot;ping&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;pong&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@ansible:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Instalando unos backends sustitutos&lt;/h2&gt;
&lt;p&gt;Sea lo que sea que vayan a ejecutar los &lt;em&gt;backends&lt;/em&gt; reales, los podemos ver como una caja negra que ofrecen sus servicios mediante protocolo TCP/IP en un puerto concreto.&lt;/p&gt;
&lt;p&gt;Como no nos importa demasiado lo que hagan, y para simplificar el artículo, los vamos a reemplazar con servidores web &lt;strong&gt;nginx&lt;/strong&gt;, sirviendo un fichero HTML con su nombre (para poder distinguirlos en las pruebas). De esta forma, podremos ver el tipo de servidor que responde (api o web) y su número, ya que ambos datos están en su &lt;em&gt;hostname&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Así pues, basta con un &lt;em&gt;playbook&lt;/em&gt; que instale el servidor web y ponga el fichero &lt;em&gt;.html&lt;/em&gt; en su sitio. Como &lt;strong&gt;docker&lt;/strong&gt; está ejecutando el servidor &lt;strong&gt;ssh&lt;/strong&gt; y no &lt;strong&gt;systemd&lt;/strong&gt;, el &lt;strong&gt;nginx&lt;/strong&gt; no se levanta. Con otra tarea para asegurar que está corriendo, basta.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# cat backends.yml 
- hosts: webs, apis
  tasks:
    - apt: &lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;nginx-light &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;present
    - copy: &lt;span class="nv"&gt;content&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Content from {{ inventory_hostname }}&amp;quot;&lt;/span&gt; &lt;span class="nv"&gt;dest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/var/www/html/index.html
    - service: &lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;nginx &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;started
root@ansible:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lanzamos el &lt;em&gt;playbook&lt;/em&gt;, y los dejamos preparados para que el balanceador los pueda usar. En un entorno real, dedicaríamos mas tiempo en poner servidores de aplicaciones normales, con aplicaciones adecuadas, y que posiblemente usarían algún tipo de base de datos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# ansible-playbook -i hosts backends.yml 

PLAY &lt;span class="o"&gt;[&lt;/span&gt;webs, apis&lt;span class="o"&gt;]&lt;/span&gt; **************************************************************

TASK &lt;span class="o"&gt;[&lt;/span&gt;setup&lt;span class="o"&gt;]&lt;/span&gt; *******************************************************************
ok: &lt;span class="o"&gt;[&lt;/span&gt;web2&lt;span class="o"&gt;]&lt;/span&gt;
ok: &lt;span class="o"&gt;[&lt;/span&gt;api1&lt;span class="o"&gt;]&lt;/span&gt;
ok: &lt;span class="o"&gt;[&lt;/span&gt;web1&lt;span class="o"&gt;]&lt;/span&gt;
ok: &lt;span class="o"&gt;[&lt;/span&gt;api2&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;apt&lt;span class="o"&gt;]&lt;/span&gt; *********************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;api1&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;api2&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;web1&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;web2&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;copy&lt;span class="o"&gt;]&lt;/span&gt; ********************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;web1&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;web2&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;api2&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;api1&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;service&lt;span class="o"&gt;]&lt;/span&gt; *****************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;web1&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;web2&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;api1&lt;span class="o"&gt;]&lt;/span&gt;
changed: &lt;span class="o"&gt;[&lt;/span&gt;api2&lt;span class="o"&gt;]&lt;/span&gt;

PLAY RECAP *********************************************************************
api1                       : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;   
api2                       : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;   
web1                       : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;   
web2                       : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;   

root@ansible:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Montando el balanceador&lt;/h2&gt;
&lt;p&gt;Para obtener un balanceador HTTP (o TCP, si lo necesitáramos), basta con elegir uno. Normalmente yo usaría un servidor &lt;strong&gt;nginx&lt;/strong&gt; para balancear HTTP (que además ofrece otras funcionalidades, aunque no soporte TCP directo); en este caso, y para variar un poco, vamos a poner &lt;strong&gt;haproxy&lt;/strong&gt;, que nos ofrece una bonita página de estadísticas.&lt;/p&gt;
&lt;p&gt;El truco está en instalar &lt;strong&gt;haproxy&lt;/strong&gt;, darle un fichero de configuración adecuado y recargar su configuración. Nuevamente, al tratarse de &lt;strong&gt;docker&lt;/strong&gt; hay que asegurarse que el servicio esté levantado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# cat balancer.yml 
- hosts: balancer
  tasks:
    - apt: &lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;haproxy &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;present
    - service: &lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;haproxy &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;started
    - copy: &lt;span class="nv"&gt;src&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;haproxy.cfg &lt;span class="nv"&gt;dest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/etc/haproxy/haproxy.cfg
    - service: &lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;haproxy &lt;span class="nv"&gt;state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;reloaded
root@ansible:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La funcionalidad con la que cumpla el balanceador se controla en &lt;em&gt;/etc/haproxy/haproxy.cfg&lt;/em&gt;, que el &lt;em&gt;playbook&lt;/em&gt; pone en su sitio, desde una carpeta en el contexto.&lt;/p&gt;
&lt;p&gt;HAProxy funciona mapeando &lt;em&gt;frontends&lt;/em&gt; (entradas del balanceador) con sus respectivos &lt;em&gt;backends&lt;/em&gt; (servidores que atienden peticiones).&lt;/p&gt;
&lt;p&gt;Disponemos de varios algoritmos de balanceo, así que vamos a poner uno distinto para cada &lt;em&gt;backend&lt;/em&gt;. Para la web, vamos a usar &lt;em&gt;roundrobin&lt;/em&gt;, que básicamente se trata de una petición a cada uno por turnos; la &lt;em&gt;api&lt;/em&gt; va a contar con el algoritmo &lt;em&gt;leastconn&lt;/em&gt;, que significa darle una petición al servidor que menos conexiones tiene abiertas.&lt;/p&gt;
&lt;p&gt;Como &lt;em&gt;bonus track&lt;/em&gt;, vamos a habilitar la página de estadísticas, siempre que la queramos, claro. La he copiado tal cual de la documentación de &lt;strong&gt;haproxy&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# cat haproxy.cfg 
listen stats :1936
    mode http
    stats enable
    stats hide-version
    stats uri /

frontend web
    bind :8080
    default_backend webs

backend webs
    balance roundrobin
    server web1 web1:80
    server web2 web2:80

frontend api
    bind :8081
    default_backend apis

backend apis
    balance leastconn
    server api1 api1:80
    server api2 api2:80
root@ansible:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Solo nos faltaría lanzar el &lt;em&gt;playbook&lt;/em&gt; para que quede todo correctamente montado. Si la configuración cambiara o hubiera que corregirla, se debe modificar el fichero local &lt;em&gt;haproxy.cfg&lt;/em&gt; y relanzar el &lt;em&gt;playbook&lt;/em&gt;. &lt;strong&gt;Ansible&lt;/strong&gt; no intentará cambiar nada que ya esté como debía; no instalará &lt;strong&gt;haproxy&lt;/strong&gt; de nuevo, no lo levantará si ya estaba corriendo, no copiará el fichero a menos que haya cambiado, y siempre va a recargar la configuración del servicio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# ansible-playbook -i hosts balancer.yml 

PLAY &lt;span class="o"&gt;[&lt;/span&gt;balancer&lt;span class="o"&gt;]&lt;/span&gt; ****************************************************************

TASK &lt;span class="o"&gt;[&lt;/span&gt;setup&lt;span class="o"&gt;]&lt;/span&gt; *******************************************************************
ok: &lt;span class="o"&gt;[&lt;/span&gt;balancer&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;apt&lt;span class="o"&gt;]&lt;/span&gt; *********************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;balancer&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;service&lt;span class="o"&gt;]&lt;/span&gt; *****************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;balancer&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;copy&lt;span class="o"&gt;]&lt;/span&gt; ********************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;balancer&lt;span class="o"&gt;]&lt;/span&gt;

TASK &lt;span class="o"&gt;[&lt;/span&gt;service&lt;span class="o"&gt;]&lt;/span&gt; *****************************************************************
changed: &lt;span class="o"&gt;[&lt;/span&gt;balancer&lt;span class="o"&gt;]&lt;/span&gt;

PLAY RECAP *********************************************************************
balancer                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;   

root@ansible:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Comprobando el funcionamiento&lt;/h2&gt;
&lt;p&gt;Si hacemos peticiones individuales, vemos que cada servidor funciona, pero lo que nos importa es el conjunto. Para ello vamos a solicitar peticiones a cada uno de los puertos que representan el &lt;em&gt;cluster&lt;/em&gt; de web y de &lt;em&gt;api&lt;/em&gt;. Puesto que los hemos publicado con el mismo número de puerto en la máquina anfitriona, lo podemos lanzar ahí mismo.&lt;/p&gt;
&lt;p&gt;Hacemos unas peticines al &lt;em&gt;cluster&lt;/em&gt; de web, que se esconde detrás del puerto 8080, y comprobamos que van alternando un &lt;em&gt;backend&lt;/em&gt; u otro por turnos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~$ curl http://localhost:8080/ &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
Content from web1
gerard@sirius:~$ curl http://localhost:8080/ &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
Content from web2
gerard@sirius:~$ curl http://localhost:8080/ &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
Content from web1
gerard@sirius:~$ curl http://localhost:8080/ &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
Content from web2
gerard@sirius:~$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Repetimos el procedimiento para la &lt;em&gt;api&lt;/em&gt;, que se esconde en el puerto 8081 del balanceador:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~$ curl http://localhost:8081/ &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
Content from api1
gerard@sirius:~$ curl http://localhost:8081/ &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
Content from api2
gerard@sirius:~$ curl http://localhost:8081/ &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
Content from api1
gerard@sirius:~$ curl http://localhost:8081/ &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
Content from api2
gerard@sirius:~$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y solo nos queda ver que la página de estadísticas funciona y nos resulta útil. Puesto que devuelve una página web completa, lo vamos a ver en un navegador cualquiera.&lt;/p&gt;
&lt;p&gt;&lt;img alt="HAProxy Stats" src="https://www.linuxsysadmin.ml/images/haproxy-stats.jpg"&gt;&lt;/p&gt;</content><category term="balanceador"></category><category term="haproxy"></category><category term="ansible"></category><category term="docker"></category></entry><entry><title>Preparando un servidor de repositorios GIT</title><link href="https://www.linuxsysadmin.ml/2016/06/preparando-un-servidor-de-repositorios-git.html" rel="alternate"></link><published>2016-06-20T08:30:00+02:00</published><updated>2016-06-20T08:30:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-06-20:/2016/06/preparando-un-servidor-de-repositorios-git.html</id><summary type="html">&lt;p&gt;Algunas veces tenemos necesidad de crear un proyecto con un equipo pequeño y necesitamos versionarlo en un sitio accesible para todos los participantes involucrados. El precio de soluciones en la nube suele ser prohibitivo, y montar una solución gráfica puede ser demasiado. Lo podemos hacer simplemente usando &lt;strong&gt;git&lt;/strong&gt; y &lt;strong&gt;ssh …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Algunas veces tenemos necesidad de crear un proyecto con un equipo pequeño y necesitamos versionarlo en un sitio accesible para todos los participantes involucrados. El precio de soluciones en la nube suele ser prohibitivo, y montar una solución gráfica puede ser demasiado. Lo podemos hacer simplemente usando &lt;strong&gt;git&lt;/strong&gt; y &lt;strong&gt;ssh&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;La idea es muy simple; solo se necesita un servidor de &lt;strong&gt;ssh&lt;/strong&gt;, que es la forma de transportar los datos, y los binarios de &lt;strong&gt;git&lt;/strong&gt; para que los organice a placer. También vamos a necesitar un usuario &lt;strong&gt;git&lt;/strong&gt;, que es el que vamos a usar para entrar, ya sea para crear y borrar repositorios, como para las operaciones remotas recibidas por el repositorio.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Podemos eliminar la petición de &lt;em&gt;password&lt;/em&gt; para todos los accesos que se hagan por &lt;strong&gt;SSH&lt;/strong&gt;, sean para entrar en las máquinas por &lt;strong&gt;SSH&lt;/strong&gt; mediante cualquier &lt;em&gt;shell&lt;/em&gt; de este artículo, o como resultado de una operación remota de &lt;strong&gt;git&lt;/strong&gt;. Esto se puede hacer usando autenticación &lt;strong&gt;SSH&lt;/strong&gt; por claves, como se explica en un &lt;a href="https://www.linuxsysadmin.ml/2016/05/autenticacion-ssh-por-claves.html"&gt;artículo anterior&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Montando el servidor&lt;/h2&gt;
&lt;p&gt;Para crear la máquina base, vamos a utilizar &lt;strong&gt;Docker&lt;/strong&gt; por comodidad. Aprovechando esta tecnología, podemos crear el contenedor partiendo de una imagen creada con un &lt;em&gt;Dockerfile&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ cat Dockerfile
FROM debian:jessie
RUN apt-get update &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    apt-get install -y git openssh-server &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    mkdir /var/run/sshd
RUN useradd git -G sudo -s /bin/bash -m &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;git:git&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; chpasswd
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/sbin/sshd&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-D&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Construimos la imagen, basándonos en el anterior &lt;em&gt;Dockerfile&lt;/em&gt;, y le añadimos el &lt;em&gt;tag&lt;/em&gt; "gitserver".&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ docker build -t gitserver .
Sending build context to Docker daemon &lt;span class="m"&gt;5&lt;/span&gt;.632 kB
Step &lt;span class="m"&gt;1&lt;/span&gt; : FROM debian:jessie
 ---&amp;gt; bb5d89f9b6cb
Step &lt;span class="m"&gt;2&lt;/span&gt; : RUN apt-get update &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;     apt-get install -y git openssh-server &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;     mkdir /var/run/sshd
 ---&amp;gt; Running in 6b612781b788
...
 ---&amp;gt; e88e644b0a53
Removing intermediate container 6b612781b788
Step &lt;span class="m"&gt;3&lt;/span&gt; : RUN useradd git -G sudo -s /bin/bash -m &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;     &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;git:git&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; chpasswd
 ---&amp;gt; Running in 0e865bda447e
 ---&amp;gt; 81d111c19c71
Removing intermediate container 0e865bda447e
Step &lt;span class="m"&gt;4&lt;/span&gt; : CMD /usr/sbin/sshd -D
 ---&amp;gt; Running in 67bbebe61c74
 ---&amp;gt; 81c2dd7b156a
Removing intermediate container 67bbebe61c74
Successfully built 81c2dd7b156a
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lanzamos una instancia del contenedor para que podamos utilizarla. La parte importante es el &lt;em&gt;flag&lt;/em&gt; &lt;strong&gt;-d&lt;/strong&gt; para ejecutar el contenedor en &lt;em&gt;background&lt;/em&gt;, y el &lt;em&gt;flag&lt;/em&gt; &lt;strong&gt;-p&lt;/strong&gt; que nos permite publicar el puerto 22 del contenedor en el puerto 22222 de la máquina &lt;em&gt;host&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ docker run -d --name gitserver1 -h gitserver1 -p &lt;span class="m"&gt;22222&lt;/span&gt;:22 gitserver
c26f30a94bb75b35c6d6cfe6a6bc5b1ef6929aafe1b5636acd207e019743540b
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a entrar en el servidor &lt;strong&gt;SSH&lt;/strong&gt; para crear el repositorio &lt;em&gt;myrepo.git&lt;/em&gt; que nos va a servir de ejemplo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ ssh git@localhost -p &lt;span class="m"&gt;22222&lt;/span&gt;
git@localhost&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;s password: 

The programs included with the Debian GNU/Linux system are free software&lt;span class="p"&gt;;&lt;/span&gt;
the exact distribution terms &lt;span class="k"&gt;for&lt;/span&gt; each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
git@gitserver1:~$ git init --bare myrepo.git
Initialized empty Git repository in /home/git/myrepo.git/
git@gitserver1:~$ &lt;span class="nb"&gt;exit&lt;/span&gt;
&lt;span class="nb"&gt;logout&lt;/span&gt;
Connection to localhost closed.
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Desde la máquina &lt;em&gt;host&lt;/em&gt; (o desde cualquier otra), podemos clonar el repositorio. Como tenemos el puerto del contenedor publicado en el puerto 22222 de la máquina &lt;em&gt;host&lt;/em&gt; (la de trabajo, en este caso), la usamos tal cual para clonar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ git clone ssh://git@localhost:22222/home/git/myrepo.git
Cloning into &lt;span class="s1"&gt;&amp;#39;myrepo&amp;#39;&lt;/span&gt;...
git@localhost&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;s password: 
warning: You appear to have cloned an empty repository.
Checking connectivity... &lt;span class="k"&gt;done&lt;/span&gt;.
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Hacemos un poco de trabajo local, con sus respectivos &lt;em&gt;commits&lt;/em&gt;. Finalmente podemos hacer un &lt;em&gt;push&lt;/em&gt; a nuestro repositorio remoto, siguiendo el &lt;em&gt;workflow&lt;/em&gt; de trabajo que queramos seguir.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ &lt;span class="nb"&gt;cd&lt;/span&gt; myrepo/
gerard@sirius:~/build/myrepo$ &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.1 &amp;gt; VERSION
gerard@sirius:~/build/myrepo$ git add VERSION 
gerard@sirius:~/build/myrepo$ git commit -m &lt;span class="s2"&gt;&amp;quot;Initial commit&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;master &lt;span class="o"&gt;(&lt;/span&gt;root-commit&lt;span class="o"&gt;)&lt;/span&gt; f30b82a&lt;span class="o"&gt;]&lt;/span&gt; Initial commit
 &lt;span class="m"&gt;1&lt;/span&gt; file changed, &lt;span class="m"&gt;1&lt;/span&gt; insertion&lt;span class="o"&gt;(&lt;/span&gt;+&lt;span class="o"&gt;)&lt;/span&gt;
 create mode &lt;span class="m"&gt;100644&lt;/span&gt; VERSION
gerard@sirius:~/build/myrepo$ git push -u origin master
git@localhost&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;s password: 
Counting objects: &lt;span class="m"&gt;3&lt;/span&gt;, &lt;span class="k"&gt;done&lt;/span&gt;.
Writing objects: &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;/3&lt;span class="o"&gt;)&lt;/span&gt;, &lt;span class="m"&gt;222&lt;/span&gt; bytes &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; bytes/s, &lt;span class="k"&gt;done&lt;/span&gt;.
Total &lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;delta &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;, reused &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;delta &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
To ssh://git@localhost:22222/home/git/myrepo.git
 * &lt;span class="o"&gt;[&lt;/span&gt;new branch&lt;span class="o"&gt;]&lt;/span&gt;      master -&amp;gt; master
Branch master &lt;span class="nb"&gt;set&lt;/span&gt; up to track remote branch master from origin.
gerard@sirius:~/build/myrepo$ &lt;span class="nb"&gt;cd&lt;/span&gt; ..
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Añadiendo restricciones a la sesión SSH&lt;/h2&gt;
&lt;p&gt;Es un poco peligroso permitir que el usuario &lt;em&gt;git&lt;/em&gt; entre mediante una sesión &lt;strong&gt;SSH&lt;/strong&gt; para hacer lo que le parezca.&lt;/p&gt;
&lt;p&gt;Los mismos binarios de &lt;strong&gt;git&lt;/strong&gt; incluyen &lt;strong&gt;git-shell&lt;/strong&gt;, que es un &lt;em&gt;shell&lt;/em&gt; que limita lo que puede hacer el usuario, aunque solo permitiría hacer las operaciones &lt;em&gt;push&lt;/em&gt; y &lt;em&gt;pull&lt;/em&gt; propias del trabajo remoto con &lt;strong&gt;git&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;¿Y como podemos crear y destruir repositorios? En principio, no se puede. Sin embargo, si creamos una carpeta &lt;em&gt;/home/git/git-shell-commands/&lt;/em&gt;, el usuario va a poder ejecutar los &lt;em&gt;scripts&lt;/em&gt; que allí pongamos.&lt;/p&gt;
&lt;p&gt;Siguiendo esta idea, vamos a mejorar el &lt;em&gt;Dockerfile&lt;/em&gt; para asignar &lt;strong&gt;git-shell&lt;/strong&gt; al usuario &lt;em&gt;git&lt;/em&gt; y para ponerle un par de comandos.&lt;/p&gt;
&lt;p&gt;Vamos a crear dos &lt;em&gt;scripts&lt;/em&gt; que nos permitan crear y destruir repositorios, que son los siguientes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ cat create 
&lt;span class="c1"&gt;#!/bin/bash&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="si"&gt;${#}&lt;/span&gt; -ne &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;[ERROR] Syntax: create &amp;lt;repository&amp;gt;&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;exit&lt;/span&gt; -1
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -e &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.git &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;[ERROR] Repository &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; exists&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;exit&lt;/span&gt; -1
&lt;span class="k"&gt;fi&lt;/span&gt;

git init --bare &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.git
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;[OK] Repository &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; created&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;
gerard@sirius:~/build$ cat destroy 
&lt;span class="c1"&gt;#!/bin/bash&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="si"&gt;${#}&lt;/span&gt; -ne &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;[ERROR] Syntax: destroy &amp;lt;repository&amp;gt;&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;exit&lt;/span&gt; -1
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; -e &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.git &lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
    rm -Rf &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;.git
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;[OK] Repository &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; deleted&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;[ERROR] Repository &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; does not exist&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;exit&lt;/span&gt; -1
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;También vamos a reescribir el &lt;em&gt;Dockerfile&lt;/em&gt; con las nuevas modificaciones.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ cat Dockerfile.shell 
FROM debian:jessie
RUN apt-get update &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    apt-get install -y git openssh-server &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    mkdir /var/run/sshd
RUN useradd git -G sudo -s /usr/bin/git-shell -m &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;git:git&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; chpasswd &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    mkdir /home/git/git-shell-commands
COPY create destroy /home/git/git-shell-commands/
RUN cp /usr/share/doc/git/contrib/git-shell-commands/help /home/git/git-shell-commands/ &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    cp /usr/share/doc/git/contrib/git-shell-commands/list /home/git/git-shell-commands/ &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    chmod &lt;span class="m"&gt;755&lt;/span&gt; /home/git/git-shell-commands/*
CMD &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/sbin/sshd&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;-D&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Creamos la imagen usando el &lt;em&gt;Dockerfile&lt;/em&gt; antes mencionado, siguiendo el mismo procedimiento de la versión básica. Le ponemos un &lt;em&gt;tag&lt;/em&gt; distinto para tener ambas imágenes funcionales.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ docker build -f Dockerfile.shell -t gitserver:shell .
Sending build context to Docker daemon &lt;span class="m"&gt;53&lt;/span&gt;.25 kB
Step &lt;span class="m"&gt;1&lt;/span&gt; : FROM debian:jessie
 ---&amp;gt; bb5d89f9b6cb
Step &lt;span class="m"&gt;2&lt;/span&gt; : RUN apt-get update &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;     apt-get install -y git openssh-server &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;     mkdir /var/run/sshd
 ---&amp;gt; Using cache
 ---&amp;gt; e88e644b0a53
Step &lt;span class="m"&gt;3&lt;/span&gt; : RUN useradd git -G sudo -s /usr/bin/git-shell -m &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;     &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;git:git&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; chpasswd &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;     mkdir /home/git/git-shell-commands
 ---&amp;gt; Running in 387d2791d63f
 ---&amp;gt; 0ab419cdfc2d
Removing intermediate container 387d2791d63f
Step &lt;span class="m"&gt;4&lt;/span&gt; : COPY create destroy /home/git/git-shell-commands/
 ---&amp;gt; e1aa5fa9cb44
Removing intermediate container 92fd282c6979
Step &lt;span class="m"&gt;5&lt;/span&gt; : RUN cp /usr/share/doc/git/contrib/git-shell-commands/help /home/git/git-shell-commands/ &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;     cp /usr/share/doc/git/contrib/git-shell-commands/list /home/git/git-shell-commands/ &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;     chmod &lt;span class="m"&gt;755&lt;/span&gt; /home/git/git-shell-commands/*
 ---&amp;gt; Running in 45ed8f24a547
 ---&amp;gt; 2237b87165bc
Removing intermediate container 45ed8f24a547
Step &lt;span class="m"&gt;6&lt;/span&gt; : CMD /usr/sbin/sshd -D
 ---&amp;gt; Running in b94b8c1ddf8a
 ---&amp;gt; e484e1465480
Removing intermediate container b94b8c1ddf8a
Successfully built e484e1465480
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lanzamos una instancia de la imagen creada. Es importante cambiar el puerto; puesto que el 22222 está ocupado por la instancia anterior, usaré el siguiente.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ docker run -d --name gitserver2 -h gitserver2 -p &lt;span class="m"&gt;22223&lt;/span&gt;:22 gitserver:shell
e732027d11b90657ff109a455f032327f0e24eebe54a7e121d86eff6eab1bc4b
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Entramos por &lt;strong&gt;SSH&lt;/strong&gt;. Nos podemos dar cuenta de que el &lt;em&gt;prompt&lt;/em&gt; ha cambiado; estamos en el &lt;strong&gt;git-shell&lt;/strong&gt; y tenemos limitados los comandos a los que añadimos en el &lt;em&gt;Dockerfile&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ ssh git@localhost -p &lt;span class="m"&gt;22223&lt;/span&gt;
git@localhost&lt;span class="s1"&gt;&amp;#39;s password: &lt;/span&gt;

&lt;span class="s1"&gt;The programs included with the Debian GNU/Linux system are free software;&lt;/span&gt;
&lt;span class="s1"&gt;the exact distribution terms for each program are described in the&lt;/span&gt;
&lt;span class="s1"&gt;individual files in /usr/share/doc/*/copyright.&lt;/span&gt;

&lt;span class="s1"&gt;Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent&lt;/span&gt;
&lt;span class="s1"&gt;permitted by applicable law.&lt;/span&gt;
&lt;span class="s1"&gt;Run &amp;#39;&lt;/span&gt;help&lt;span class="s1"&gt;&amp;#39; for help, or &amp;#39;&lt;/span&gt;exit&lt;span class="err"&gt;&amp;#39;&lt;/span&gt; to leave.  Available commands:
create
destroy
list
git&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Usamos el comando &lt;em&gt;create&lt;/em&gt; para crear el repositorio y verificamos que está usando el comando &lt;em&gt;list&lt;/em&gt;. Tendríamos disponible el comando &lt;em&gt;destroy&lt;/em&gt;, pero de momento no lo vamos a utilizar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git&amp;gt; create myrepo2
Initialized empty Git repository in /home/git/myrepo2.git/
&lt;span class="o"&gt;[&lt;/span&gt;OK&lt;span class="o"&gt;]&lt;/span&gt; Repository myrepo2 created
git&amp;gt; list
myrepo2.git
git&amp;gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
Connection to localhost closed.
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Verificamos que funciona, clonando el repositorio como hemos hecho antes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ git clone ssh://git@localhost:22223/home/git/myrepo2.git
Cloning into &lt;span class="s1"&gt;&amp;#39;myrepo2&amp;#39;&lt;/span&gt;...
git@localhost&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;s password: 
warning: You appear to have cloned an empty repository.
Checking connectivity... &lt;span class="k"&gt;done&lt;/span&gt;.
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Hacemos algunos &lt;em&gt;commits&lt;/em&gt; locales y finalmente los pasamos al repositorio remoto mediante un &lt;em&gt;push&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@sirius:~/build$ &lt;span class="nb"&gt;cd&lt;/span&gt; myrepo2/
gerard@sirius:~/build/myrepo2$ &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.1 &amp;gt; VERSION
gerard@sirius:~/build/myrepo2$ git add VERSION
gerard@sirius:~/build/myrepo2$ git commit -m &lt;span class="s2"&gt;&amp;quot;Initial commit&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;master &lt;span class="o"&gt;(&lt;/span&gt;root-commit&lt;span class="o"&gt;)&lt;/span&gt; fd40f39&lt;span class="o"&gt;]&lt;/span&gt; Initial commit
 &lt;span class="m"&gt;1&lt;/span&gt; file changed, &lt;span class="m"&gt;1&lt;/span&gt; insertion&lt;span class="o"&gt;(&lt;/span&gt;+&lt;span class="o"&gt;)&lt;/span&gt;
 create mode &lt;span class="m"&gt;100644&lt;/span&gt; VERSION
gerard@sirius:~/build/myrepo2$ git push -u origin master
git@localhost&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;s password: 
Counting objects: &lt;span class="m"&gt;3&lt;/span&gt;, &lt;span class="k"&gt;done&lt;/span&gt;.
Writing objects: &lt;span class="m"&gt;100&lt;/span&gt;% &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;/3&lt;span class="o"&gt;)&lt;/span&gt;, &lt;span class="m"&gt;222&lt;/span&gt; bytes &lt;span class="p"&gt;|&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; bytes/s, &lt;span class="k"&gt;done&lt;/span&gt;.
Total &lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;delta &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;, reused &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;delta &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
To ssh://git@localhost:22223/home/git/myrepo2.git
 * &lt;span class="o"&gt;[&lt;/span&gt;new branch&lt;span class="o"&gt;]&lt;/span&gt;      master -&amp;gt; master
Branch master &lt;span class="nb"&gt;set&lt;/span&gt; up to track remote branch master from origin.
gerard@sirius:~/build/myrepo2$ &lt;span class="nb"&gt;cd&lt;/span&gt; ..
gerard@sirius:~/build$ 
&lt;/pre&gt;&lt;/div&gt;</content><category term="linux"></category><category term="git"></category><category term="ssh"></category><category term="docker"></category></entry><entry><title>MongoDB sharding con ansible</title><link href="https://www.linuxsysadmin.ml/2016/05/mongodb-sharding-con-ansible.html" rel="alternate"></link><published>2016-05-02T00:00:00+02:00</published><updated>2016-05-02T00:00:00+02:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-05-02:/2016/05/mongodb-sharding-con-ansible.html</id><summary type="html">&lt;p&gt;Como ya vimos en un artículo anterior, los &lt;em&gt;replica sets&lt;/em&gt; nos ofrecen alta disponibilidad para nuestros despliegues de &lt;strong&gt;mongodb&lt;/strong&gt;. Sin embargo, algunas veces, necesitamos que nuestro &lt;em&gt;cluster&lt;/em&gt; ofrezca alto rendimiento, y esto se consigue mediante &lt;em&gt;sharding&lt;/em&gt;. Como no queremos renunciar a la alta disponibilidad, podemos aplicar ambas; hoy explicamos como …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Como ya vimos en un artículo anterior, los &lt;em&gt;replica sets&lt;/em&gt; nos ofrecen alta disponibilidad para nuestros despliegues de &lt;strong&gt;mongodb&lt;/strong&gt;. Sin embargo, algunas veces, necesitamos que nuestro &lt;em&gt;cluster&lt;/em&gt; ofrezca alto rendimiento, y esto se consigue mediante &lt;em&gt;sharding&lt;/em&gt;. Como no queremos renunciar a la alta disponibilidad, podemos aplicar ambas; hoy explicamos como.&lt;/p&gt;
&lt;p&gt;El mecanismo de &lt;em&gt;sharding&lt;/em&gt; es bastante simple: tenemos nuestros datos repartidos entre uno o mas &lt;em&gt;shards&lt;/em&gt;, que se van a repartir los datos del &lt;em&gt;cluster&lt;/em&gt;. Para mantener un control de donde están los datos, también vamos a necesitar unos procesos especiales llamados &lt;em&gt;config servers&lt;/em&gt;. Finalmente, habrá que poner algunos procesos &lt;em&gt;mongos&lt;/em&gt; que son unos &lt;em&gt;proxies&lt;/em&gt; al &lt;em&gt;cluster&lt;/em&gt; y sirven para ocultar la complejidad del mismo.&lt;/p&gt;
&lt;h2&gt;Visión del conjunto&lt;/h2&gt;
&lt;p&gt;Hay que decir que el mecanismo de &lt;em&gt;sharding&lt;/em&gt; permite poner y quitar &lt;em&gt;shards&lt;/em&gt; a &lt;em&gt;posteriori&lt;/em&gt;, igual que con los procesos &lt;em&gt;mongos&lt;/em&gt;, pero para empezar vamos a necesitar una arquitectura inicial que es lo que vamos a montar.&lt;/p&gt;
&lt;p&gt;Para empezar se ha decidido por un &lt;em&gt;cluster&lt;/em&gt; de 3 &lt;em&gt;shards&lt;/em&gt;, siendo cada uno de ellos un &lt;em&gt;replica set&lt;/em&gt; de dos nodos de datos y un árbitro cada uno. Usaremos la cantidad de &lt;em&gt;config servers&lt;/em&gt; que se recomienda en la documentación oficial.&lt;/p&gt;
&lt;p&gt;Así pues, y tras elegir nombres para los &lt;em&gt;shards&lt;/em&gt;, podemos pintar un esquema de nuestro &lt;em&gt;cluster&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Arquitectura lógica" src="https://www.linuxsysadmin.ml/images/sharding_arquitectura_logica.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Para repartir los procesos entre las máquinas, hay dos reglas que hay que respetar a rajatabla:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Los procesos de datos necesitan una máquina propia, para que no se disputen los recursos de disco y memoria.&lt;/li&gt;
&lt;li&gt;No hay que poner nunca dos o mas procesos de cada &lt;em&gt;shard&lt;/em&gt;, ya que la no disponibilidad de la máquina supondría la pérdida de la mayoría de las &lt;em&gt;replica sets&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;El resto de procesos pueden compartir servidor con los de datos. Hay muchas formas de cumplir con las dos reglas, por ejemplo, la que vamos a montar:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Arquitectura física" src="https://www.linuxsysadmin.ml/images/sharding_arquitectura_fisica.jpg"&gt;&lt;/p&gt;
&lt;h2&gt;Ansible al rescate&lt;/h2&gt;
&lt;p&gt;Debido a la gran cantidad de procesos que hay que levantar, se ha decidido por automatizar su despliegue mediante &lt;strong&gt;ansible&lt;/strong&gt;. El proceso es bastante similar a &lt;a href="https://www.linuxsysadmin.ml/2015/12/construyendo-una-replica-set-en-mongodb.html"&gt;otro de nuestros artículos&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Se ha utilizado el mecanismo de &lt;strong&gt;roles&lt;/strong&gt; de &lt;strong&gt;ansible&lt;/strong&gt;, para poder desplegar todos los procesos del mismo tipo; el detalle es que se han usado los parámetros en los &lt;strong&gt;roles&lt;/strong&gt; para los cambios menores. Si queréis intentarlo o entender como funcionan los despliegues, podéis encontrar los &lt;strong&gt;playbooks&lt;/strong&gt; &lt;a href="https://www.linuxsysadmin.ml/downloads/sharding_playbooks.tar.gz"&gt;aquí&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;El fichero comprimido no incluye los binarios de &lt;strong&gt;mongodb&lt;/strong&gt; para reducir tamaño, así que hay que añadirlos en las respectivas carpetas &lt;em&gt;files&lt;/em&gt;. Tras descomprimir el fichero &lt;em&gt;.tar.gz&lt;/em&gt; y poner los binarios ausentes, nos debería quedar algo como esto:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# tree
.
├── aquila_shard.yaml
├── clients.yaml
├── config_servers.yaml
├── cygnus_shard.yaml
├── hosts.yaml
├── lyra_shard.yaml
├── mongos_servers.yaml
└── roles
    ├── client
    │   ├── files
    │   │   └── mongo
    │   └── tasks
    │       └── main.yaml
    ├── config
    │   ├── meta
    │   │   └── main.yaml
    │   ├── tasks
    │   │   └── main.yaml
    │   └── templates
    │       ├── config.conf
    │       └── config.service
    ├── mongod
    │   ├── files
    │   │   └── mongod
    │   └── tasks
    │       └── main.yaml
    ├── mongos
    │   ├── files
    │   │   └── mongos
    │   ├── tasks
    │   │   └── main.yaml
    │   └── templates
    │       ├── mongos.conf
    │       └── mongos.service
    └── shard
        ├── meta
        │   └── main.yaml
        ├── tasks
        │   └── main.yaml
        └── templates
            ├── shard.conf
            └── shard.service

&lt;span class="m"&gt;19&lt;/span&gt; directories, &lt;span class="m"&gt;23&lt;/span&gt; files
root@ansible:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Preparación de las máquinas&lt;/h2&gt;
&lt;p&gt;De acuerdo con la arquitectura propuesta, vamos a necesitar 6 servidores para el &lt;em&gt;cluster&lt;/em&gt;, que vamos a montar como contenedores LXC y, aunque no es lo ideal, nos vale como demostración. En la séptima máquina es donde tenemos las herramientas de configuración, en este caso, &lt;strong&gt;ansible&lt;/strong&gt; y los &lt;strong&gt;playbooks&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# lxc-ls -f
NAME     STATE    IPV4        IPV6  AUTOSTART
---------------------------------------------
ansible  RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.254  -     NO
mongo01  RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2    -     NO
mongo02  RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.3    -     NO
mongo03  RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.4    -     NO
mongo04  RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.5    -     NO
mongo05  RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.6    -     NO
mongo06  RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.7    -     NO
root@lxc:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a declarar todas las máquina usadas en el fichero &lt;em&gt;hosts&lt;/em&gt; de &lt;strong&gt;ansible&lt;/strong&gt;. Ya de paso, los vamos a catalogar en grupos, para que los &lt;strong&gt;playbooks&lt;/strong&gt; se puedan lanzar a los grupos, indistintamente de los servidores que los formen.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# cat ansible/etc/hosts
&lt;span class="o"&gt;[&lt;/span&gt;mongo_servers&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.4
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.5
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.6
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.7

&lt;span class="o"&gt;[&lt;/span&gt;config_servers&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.4

&lt;span class="o"&gt;[&lt;/span&gt;aquila_shard_data&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.5

&lt;span class="o"&gt;[&lt;/span&gt;aquila_shard_arbiters&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.6

&lt;span class="o"&gt;[&lt;/span&gt;lyra_shard_data&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.6

&lt;span class="o"&gt;[&lt;/span&gt;lyra_shard_arbiters&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.7

&lt;span class="o"&gt;[&lt;/span&gt;cygnus_shard_data&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.4
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.7

&lt;span class="o"&gt;[&lt;/span&gt;cygnus_shard_arbiters&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.5

&lt;span class="o"&gt;[&lt;/span&gt;mongos_servers&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2

&lt;span class="o"&gt;[&lt;/span&gt;clients&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2
root@ansible:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Por comodidad, vamos a referirnos a las máquinas por su nombre, y a falta de un servidor DNS adecuado, vamos a rellenar sus ficheros &lt;em&gt;/etc/hosts&lt;/em&gt;; para ello vamos a usar un &lt;strong&gt;playbook&lt;/strong&gt; que se asegure que esas líneas están en el fichero.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# ansible-playbook hosts.yaml

...

PLAY RECAP *********************************************************************
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.4                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.5                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.6                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.7                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;

root@ansible:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Los config servers&lt;/h2&gt;
&lt;p&gt;Los &lt;em&gt;config servers&lt;/em&gt; son procesos &lt;strong&gt;mongod&lt;/strong&gt; con una configuración concreta. El &lt;strong&gt;playbook&lt;/strong&gt; se limita a crear una estructura en &lt;em&gt;/opt/mongodb/&lt;/em&gt; asegurándose que hay el binario &lt;strong&gt;mongod&lt;/strong&gt;, la configuración, la carpeta de datos y la &lt;em&gt;unit&lt;/em&gt; de &lt;strong&gt;systemd&lt;/strong&gt; activa.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# ansible-playbook config_servers.yaml

...

PLAY RECAP *********************************************************************
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.4                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;

root@ansible:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Un acceso al cluster&lt;/h2&gt;
&lt;p&gt;Para poder configurar el &lt;em&gt;cluster&lt;/em&gt; y para un uso futuro, hemos decidido poner un proceso &lt;strong&gt;mongos&lt;/strong&gt; y el binario &lt;strong&gt;mongo&lt;/strong&gt; para poder acceder al &lt;em&gt;mongo shell&lt;/em&gt;. Se ha optado por separar los &lt;strong&gt;playbooks&lt;/strong&gt;; así se podrá utilizar para desplegarlos por separado en futuras máquinas que los puedan usar.&lt;/p&gt;
&lt;p&gt;De hecho, la recomendación oficial es poner un &lt;strong&gt;mongos&lt;/strong&gt; en cada &lt;em&gt;backend&lt;/em&gt;, aunque no necesitan el binario &lt;strong&gt;mongo&lt;/strong&gt; porque disponen de los &lt;em&gt;drivers&lt;/em&gt; oficiales del lenguaje que utilicen.&lt;/p&gt;
&lt;p&gt;Empezaremos desplegando los procesos &lt;strong&gt;mongos&lt;/strong&gt; en donde toque (de momento solo en el servidor &lt;em&gt;mongo01&lt;/em&gt;). Este &lt;strong&gt;playbook&lt;/strong&gt; se limita a poner el binario &lt;strong&gt;mongos&lt;/strong&gt; y su respectiva &lt;em&gt;unit&lt;/em&gt; para &lt;strong&gt;systemd&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# ansible-playbook mongos_servers.yaml

...

PLAY RECAP *********************************************************************
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;

root@ansible:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para nuestra comodidad, también vamos a desplegar el &lt;em&gt;mongo shell&lt;/em&gt;. Este &lt;strong&gt;playbook&lt;/strong&gt; se limita a poner el binario &lt;strong&gt;mongo&lt;/strong&gt; en su sitio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# ansible-playbook clients.yaml

...

PLAY RECAP *********************************************************************
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;

root@ansible:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Los procesos de los shards&lt;/h2&gt;
&lt;p&gt;Tenemos 9 procesos de este tipo, así que los &lt;strong&gt;roles&lt;/strong&gt; de &lt;strong&gt;ansible&lt;/strong&gt; tienen un protagonismo especial. Los cambios entre los procesos son mínimos, y se pasan por parámetro para que el rol cree los ficheros necesarios a partir de una plantilla. El rol se encarga solamente de poner el binario &lt;strong&gt;mongod&lt;/strong&gt; en &lt;em&gt;/opt/mongodb/bin&lt;/em&gt;, crear la carpeta de datos y configurar el servicio como una &lt;em&gt;unit&lt;/em&gt; de &lt;strong&gt;systemd&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Se ha decidido separar los &lt;em&gt;shards&lt;/em&gt; en diferentes &lt;strong&gt;playbooks&lt;/strong&gt; para simplificar la creación de futuros nuevos &lt;em&gt;shards&lt;/em&gt;; así pues, lanzamos el &lt;strong&gt;playbook&lt;/strong&gt; para el primer &lt;em&gt;shard&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# ansible-playbook aquila_shard.yaml

...

PLAY RECAP *********************************************************************
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.5                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.6                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;

root@ansible:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Acto seguido, lanzamos el &lt;strong&gt;playbook&lt;/strong&gt; responsable de montar los procesos del segundo &lt;em&gt;shard&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# ansible-playbook lyra_shard.yaml

...

PLAY RECAP *********************************************************************
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.6                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.7                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;

root@ansible:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y finalmente, lanzamos el tercer &lt;strong&gt;playbook&lt;/strong&gt; para desplegar los procesos del último &lt;em&gt;shard&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@ansible:~# ansible-playbook cygnus_shard.yaml

...

PLAY RECAP *********************************************************************
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.4                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.5                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.7                   : &lt;span class="nv"&gt;ok&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;    &lt;span class="nv"&gt;changed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="nv"&gt;unreachable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;    &lt;span class="nv"&gt;failed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;

root@ansible:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Atando los replica sets&lt;/h2&gt;
&lt;p&gt;El paso anterior nos ha dejado todos los procesos en funcionamiento, pero no hemos iniciado los &lt;em&gt;replica sets&lt;/em&gt;. Para que funcionen como tal, tenemos que configurarlos uno por uno como ya sabemos hacer, usando &lt;em&gt;rs.status()&lt;/em&gt; para verificar que ha quedado todo como debe.&lt;/p&gt;
&lt;p&gt;Empezaremos con una máquina cualquiera del primer &lt;em&gt;shard&lt;/em&gt;; la configuración se propagará al resto sin nuestra intervención.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo01:~# /opt/mongodb/bin/mongo --host &lt;span class="m"&gt;10&lt;/span&gt;.0.0.5 --port &lt;span class="m"&gt;27018&lt;/span&gt;
MongoDB shell version: &lt;span class="m"&gt;3&lt;/span&gt;.2.5
connecting to: &lt;span class="m"&gt;10&lt;/span&gt;.0.0.5:27018/test
...
&amp;gt; &lt;span class="nv"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
...     _id : &lt;span class="s2"&gt;&amp;quot;aquila&amp;quot;&lt;/span&gt;,
...      members : &lt;span class="o"&gt;[&lt;/span&gt;
...          &lt;span class="o"&gt;{&lt;/span&gt;_id : &lt;span class="m"&gt;0&lt;/span&gt;, host : &lt;span class="s2"&gt;&amp;quot;mongo01:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;,
...          &lt;span class="o"&gt;{&lt;/span&gt;_id : &lt;span class="m"&gt;1&lt;/span&gt;, host : &lt;span class="s2"&gt;&amp;quot;mongo04:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;,
...          &lt;span class="o"&gt;{&lt;/span&gt;_id : &lt;span class="m"&gt;2&lt;/span&gt;, host : &lt;span class="s2"&gt;&amp;quot;mongo05:27020&amp;quot;&lt;/span&gt;, arbiterOnly: true&lt;span class="o"&gt;}&lt;/span&gt;,
...      &lt;span class="o"&gt;]&lt;/span&gt;
... &lt;span class="o"&gt;}&lt;/span&gt;
...
&amp;gt; rs.initiate&lt;span class="o"&gt;(&lt;/span&gt;config&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
aquila:OTHER&amp;gt; rs.status&lt;span class="o"&gt;()&lt;/span&gt;
...
aquila:PRIMARY&amp;gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
bye
root@mongo01:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Seguimos con el segundo &lt;em&gt;shard&lt;/em&gt;, entrando en una de sus máquinas y lanzando el comando de configuración.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo01:~# /opt/mongodb/bin/mongo --host &lt;span class="m"&gt;10&lt;/span&gt;.0.0.6 --port &lt;span class="m"&gt;27018&lt;/span&gt;
MongoDB shell version: &lt;span class="m"&gt;3&lt;/span&gt;.2.5
connecting to: &lt;span class="m"&gt;10&lt;/span&gt;.0.0.6:27018/test
...
&amp;gt; &lt;span class="nv"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
...     _id : &lt;span class="s2"&gt;&amp;quot;lyra&amp;quot;&lt;/span&gt;,
...      members : &lt;span class="o"&gt;[&lt;/span&gt;
...          &lt;span class="o"&gt;{&lt;/span&gt;_id : &lt;span class="m"&gt;0&lt;/span&gt;, host : &lt;span class="s2"&gt;&amp;quot;mongo02:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;,
...          &lt;span class="o"&gt;{&lt;/span&gt;_id : &lt;span class="m"&gt;1&lt;/span&gt;, host : &lt;span class="s2"&gt;&amp;quot;mongo05:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;,
...          &lt;span class="o"&gt;{&lt;/span&gt;_id : &lt;span class="m"&gt;2&lt;/span&gt;, host : &lt;span class="s2"&gt;&amp;quot;mongo06:27020&amp;quot;&lt;/span&gt;, arbiterOnly: true&lt;span class="o"&gt;}&lt;/span&gt;,
...      &lt;span class="o"&gt;]&lt;/span&gt;
... &lt;span class="o"&gt;}&lt;/span&gt;
...
&amp;gt; rs.initiate&lt;span class="o"&gt;(&lt;/span&gt;config&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
lyra:OTHER&amp;gt; rs.status&lt;span class="o"&gt;()&lt;/span&gt;
...
lyra:PRIMARY&amp;gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
bye
root@mongo01:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y montamos el tercer &lt;em&gt;shard&lt;/em&gt; desde una cualquiera de sus &lt;em&gt;replicas&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo01:~# /opt/mongodb/bin/mongo --host &lt;span class="m"&gt;10&lt;/span&gt;.0.0.7 --port &lt;span class="m"&gt;27018&lt;/span&gt;
MongoDB shell version: &lt;span class="m"&gt;3&lt;/span&gt;.2.5
connecting to: &lt;span class="m"&gt;10&lt;/span&gt;.0.0.7:27018/test
...
&amp;gt; &lt;span class="nv"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
...     _id : &lt;span class="s2"&gt;&amp;quot;cygnus&amp;quot;&lt;/span&gt;,
...      members : &lt;span class="o"&gt;[&lt;/span&gt;
...          &lt;span class="o"&gt;{&lt;/span&gt;_id : &lt;span class="m"&gt;0&lt;/span&gt;, host : &lt;span class="s2"&gt;&amp;quot;mongo03:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;,
...          &lt;span class="o"&gt;{&lt;/span&gt;_id : &lt;span class="m"&gt;1&lt;/span&gt;, host : &lt;span class="s2"&gt;&amp;quot;mongo06:27018&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;,
...          &lt;span class="o"&gt;{&lt;/span&gt;_id : &lt;span class="m"&gt;2&lt;/span&gt;, host : &lt;span class="s2"&gt;&amp;quot;mongo04:27020&amp;quot;&lt;/span&gt;, arbiterOnly: true&lt;span class="o"&gt;}&lt;/span&gt;,
...      &lt;span class="o"&gt;]&lt;/span&gt;
... &lt;span class="o"&gt;}&lt;/span&gt;
...
&amp;gt; rs.initiate&lt;span class="o"&gt;(&lt;/span&gt;config&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
cygnus:OTHER&amp;gt; rs.status&lt;span class="o"&gt;()&lt;/span&gt;
...
cygnus:PRIMARY&amp;gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
bye
root@mongo01:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Añadiendo los shards al cluster&lt;/h2&gt;
&lt;p&gt;Ahora tenemos un grupo de &lt;em&gt;config servers&lt;/em&gt;, que forman un &lt;em&gt;cluster&lt;/em&gt; de 0 &lt;em&gt;shards&lt;/em&gt; (válido pero inútil, ya que no tenemos donde guardar los datos). También disponemos de 3 &lt;em&gt;replica sets&lt;/em&gt; independientes, que se convertirán en los futuros &lt;em&gt;shards&lt;/em&gt;. Solo falta asociar los &lt;em&gt;shards&lt;/em&gt; al resto del &lt;em&gt;cluster&lt;/em&gt;, mediante el comando &lt;em&gt;sh.addShard()&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Para ello entramos en un &lt;strong&gt;mongos&lt;/strong&gt; desde donde lanzaremos los comandos. De hecho, solo tenemos uno, en &lt;em&gt;mongo01&lt;/em&gt;. Puesto que está en la misma máquina que el cliente &lt;strong&gt;mongo&lt;/strong&gt; y corre en el puerto estándar 27017, no hace falta especificar ni el &lt;em&gt;host&lt;/em&gt; ni el puerto.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo01:~# /opt/mongodb/bin/mongo
MongoDB shell version: &lt;span class="m"&gt;3&lt;/span&gt;.2.5
connecting to: &lt;span class="nb"&gt;test&lt;/span&gt;
...
mongos&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Veamos como está el cluster antes de añadir los &lt;em&gt;shards&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mongos&amp;gt; printShardingStatus&lt;span class="o"&gt;()&lt;/span&gt;
--- Sharding Status ---
  sharding version: &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;minCompatibleVersion&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;5&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;currentVersion&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;6&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;clusterId&amp;quot;&lt;/span&gt; : ObjectId&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;571dd47adbda7a5a80047a5d&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
  shards:
  active mongoses:
        &lt;span class="s2"&gt;&amp;quot;3.2.5&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
  balancer:
        Currently enabled:  yes
        Currently running:  no
        Failed balancer rounds in last &lt;span class="m"&gt;5&lt;/span&gt; attempts:  &lt;span class="m"&gt;0&lt;/span&gt;
        Migration Results &lt;span class="k"&gt;for&lt;/span&gt; the last &lt;span class="m"&gt;24&lt;/span&gt; hours:
                No recent migrations
  databases:

mongos&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Procedemos a lanzar el comando para añadir cada &lt;em&gt;shard&lt;/em&gt;. Es interesante saber que el proceso &lt;strong&gt;mongos&lt;/strong&gt; puede reconocer la forma de cada &lt;em&gt;replica set&lt;/em&gt; a partir de cualquiera de sus procesos. Podemos dar la URL con una sola máquina, o con varias de ellas. Lo importante es que alguna de ellas esté levantada, para que el proceso &lt;strong&gt;mongos&lt;/strong&gt; pueda descubrir el resto a partir de su configuración.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mongos&amp;gt; sh.addShard&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;aquila/mongo01:27018&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;shardAdded&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;aquila&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
mongos&amp;gt; sh.addShard&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lyra/mongo02:27018,mongo05:27018,mongo06:27020&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;shardAdded&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;lyra&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
mongos&amp;gt; sh.addShard&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cygnus/mongo06:27018&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;shardAdded&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;cygnus&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
mongos&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Después de añadir los &lt;em&gt;shards&lt;/em&gt;, podemos ver como queda el &lt;em&gt;cluster&lt;/em&gt; con una sola consulta.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mongos&amp;gt; printShardingStatus&lt;span class="o"&gt;()&lt;/span&gt;
--- Sharding Status ---
  sharding version: &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;minCompatibleVersion&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;5&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;currentVersion&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;6&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;clusterId&amp;quot;&lt;/span&gt; : ObjectId&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;571dd47adbda7a5a80047a5d&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
  shards:
        &lt;span class="o"&gt;{&lt;/span&gt;  &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;aquila&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;host&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;aquila/mongo01:27018,mongo04:27018&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;{&lt;/span&gt;  &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;cygnus&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;host&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;cygnus/mongo03:27018,mongo06:27018&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;{&lt;/span&gt;  &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;lyra&amp;quot;&lt;/span&gt;,  &lt;span class="s2"&gt;&amp;quot;host&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;lyra/mongo02:27018,mongo05:27018&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
  active mongoses:
        &lt;span class="s2"&gt;&amp;quot;3.2.5&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
  balancer:
        Currently enabled:  yes
        Currently running:  no
        Failed balancer rounds in last &lt;span class="m"&gt;5&lt;/span&gt; attempts:  &lt;span class="m"&gt;0&lt;/span&gt;
        Migration Results &lt;span class="k"&gt;for&lt;/span&gt; the last &lt;span class="m"&gt;24&lt;/span&gt; hours:
                No recent migrations
  databases:

mongos&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y como todo funciona como debe, salimos del &lt;em&gt;mongo shell&lt;/em&gt; para evitar meter la pata.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mongos&amp;gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
bye
root@mongo01:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto, tenemos nuestro &lt;em&gt;cluster&lt;/em&gt; listo y preparado para su uso.&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="jessie"></category><category term="mongodb"></category><category term="replica set"></category><category term="sharding"></category><category term="ansible"></category><category term="playbook"></category><category term="systemd"></category></entry><entry><title>Creando un entorno escalable (IV)</title><link href="https://www.linuxsysadmin.ml/2016/03/creando-un-entorno-escalable-4.html" rel="alternate"></link><published>2016-03-21T08:00:00+01:00</published><updated>2016-03-21T08:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-03-21:/2016/03/creando-un-entorno-escalable-4.html</id><summary type="html">&lt;p&gt;Acabamos el artículo anterior de esta serie con las aplicaciones corriendo en sus respectivas máquinas. En este artículo vamos a poner una fachada a todo el sistema, mediante un &lt;em&gt;proxy HTTP&lt;/em&gt; que haga las funciones de terminación &lt;em&gt;SSL&lt;/em&gt; y de &lt;em&gt;balanceador&lt;/em&gt;, exponiendo todo el sistema en una sola dirección IP …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Acabamos el artículo anterior de esta serie con las aplicaciones corriendo en sus respectivas máquinas. En este artículo vamos a poner una fachada a todo el sistema, mediante un &lt;em&gt;proxy HTTP&lt;/em&gt; que haga las funciones de terminación &lt;em&gt;SSL&lt;/em&gt; y de &lt;em&gt;balanceador&lt;/em&gt;, exponiendo todo el sistema en una sola dirección IP.&lt;/p&gt;
&lt;p&gt;Como &lt;em&gt;proxy HTTP&lt;/em&gt; tenemos varias opciones; solo se necesita un servidor web que soporte &lt;em&gt;virtual hosts&lt;/em&gt;, protocolo HTTP sobre SSL, capacidad de hacer de &lt;em&gt;proxy&lt;/em&gt; y capacidad para balancear las peticiones entre varias opciones.&lt;/p&gt;
&lt;p&gt;Si analizamos estos requisitos, podemos comprobar que las opciones son muchas; desde el todopoderoso &lt;strong&gt;apache&lt;/strong&gt; al &lt;strong&gt;nginx&lt;/strong&gt;, pasando por soluciones de balanceador puro como &lt;strong&gt;haproxy&lt;/strong&gt;, u opciones mas esotéricas como &lt;strong&gt;squid&lt;/strong&gt;. En este caso, se utiliza &lt;strong&gt;nginx&lt;/strong&gt; por su facilidad de uso y su bajo consumo de recursos. Cumple con el subconjunto básico de funcionalidades necesario, pero no dispone de tantos algoritmos de balanceo como otras opciones.&lt;/p&gt;
&lt;h2&gt;Instalación de paquetes&lt;/h2&gt;
&lt;p&gt;Empezamos instalando los requisitos para nuestra fachada; en principio solo se necesitaría el servidor web &lt;strong&gt;nginx&lt;/strong&gt; (en la versión mínima) y &lt;strong&gt;openssl&lt;/strong&gt; para generar los certificados. Adicionalmente instalaremos &lt;strong&gt;curl&lt;/strong&gt; para comprobar que el resultado es correcto.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@frontend:~# apt-get install nginx-light curl
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
Se instalarán los siguientes paquetes extras:
  ca-certificates libcurl3 libffi6 libgmp10 libgnutls-deb0-28 libhogweed2 libidn11 libldap-2.4-2 libnettle4 libp11-kit0 librtmp1
  libsasl2-2 libsasl2-modules libsasl2-modules-db libssh2-1 libtasn1-6 nginx-common openssl
Paquetes sugeridos:
  gnutls-bin libsasl2-modules-otp libsasl2-modules-ldap libsasl2-modules-sql libsasl2-modules-gssapi-mit
  libsasl2-modules-gssapi-heimdal fcgiwrap nginx-doc ssl-cert
Se instalarán los siguientes paquetes NUEVOS:
  ca-certificates curl libcurl3 libffi6 libgmp10 libgnutls-deb0-28 libhogweed2 libidn11 libldap-2.4-2 libnettle4 libp11-kit0
  librtmp1 libsasl2-2 libsasl2-modules libsasl2-modules-db libssh2-1 libtasn1-6 nginx-common nginx-light openssl
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;20&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;4&lt;/span&gt;.077 kB de archivos.
Se utilizarán &lt;span class="m"&gt;8&lt;/span&gt;.832 kB de espacio de disco adicional después de esta operación.
¿Desea continuar? &lt;span class="o"&gt;[&lt;/span&gt;S/n&lt;span class="o"&gt;]&lt;/span&gt; s
...
root@frontend:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El paquete &lt;strong&gt;nginx&lt;/strong&gt; de la distribución &lt;em&gt;Debian&lt;/em&gt; viene con una configuración por defecto en &lt;em&gt;/etc/nginx/sites-enabled/&lt;/em&gt;, que vamos a eliminar para evitar que se pise con nuestras configuraciones.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@frontend:~# ls -lh /etc/nginx/sites-enabled/
total &lt;span class="m"&gt;0&lt;/span&gt;
lrwxrwxrwx &lt;span class="m"&gt;1&lt;/span&gt; root root &lt;span class="m"&gt;34&lt;/span&gt; feb &lt;span class="m"&gt;26&lt;/span&gt; &lt;span class="m"&gt;11&lt;/span&gt;:28 default -&amp;gt; /etc/nginx/sites-available/default
root@frontend:~# unlink /etc/nginx/sites-enabled/default
root@frontend:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Consideraciones de seguridad&lt;/h2&gt;
&lt;p&gt;Cuando nuestro servidor web recibe una petición, va a iniciar una nueva conexión contra el servidor de &lt;em&gt;backend&lt;/em&gt; que toque o el de &lt;em&gt;backoffice&lt;/em&gt;. Para habilitar esto, se necesitan nuevas reglas en el &lt;em&gt;firewall&lt;/em&gt;, que en este caso es &lt;strong&gt;firehol&lt;/strong&gt;, instalado en la máquina anfitriona.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# cat /etc/firehol/firehol.conf
...  
&lt;span class="nv"&gt;app_servers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;10.0.0.3 10.0.0.4 10.0.0.5&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;frontend_server&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;10.0.0.2&amp;quot;&lt;/span&gt;
...
router internal inface lxc0 outface lxc0
...  
    route webcache accept src &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$frontend_server&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; dst &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$app_servers&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
root@lxc:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;No os olvidéis de reiniciar &lt;strong&gt;firehol&lt;/strong&gt;, para que se apliquen las nuevas reglas.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# service firehol restart
...  
root@lxc:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Montando los virtualhosts de ambas aplicaciones&lt;/h2&gt;
&lt;p&gt;La parte privada va a estar escondida tras una terminación &lt;strong&gt;HTTPS&lt;/strong&gt;. Esa aplicación se podría esconder tras una &lt;a href="https://www.linuxsysadmin.ml/2016/02/restringiendo-accesos-mediante-certificados-de-cliente.html"&gt;autenticación de certificados cliente&lt;/a&gt; o mediante &lt;a href="https://www.linuxsysadmin.ml/2016/02/restringiendo-accesos-web-mediante-autenticacion-basica.html"&gt;autenticación básica&lt;/a&gt;. Por simplicidad vamos a usar esta última.&lt;/p&gt;
&lt;p&gt;Empezamos generando un certificado autofirmado para el servidor web, directamente firmado, y su clave. Fijaos que no generamos ningún certificado de CA, ya que no tenemos ninguna intención de generar autenticación cliente en el futuro.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@frontend:~# openssl req -new -newkey rsa:2048 -days &lt;span class="m"&gt;365&lt;/span&gt; -nodes -x509 -keyout server.key -out server.crt -subj &lt;span class="s2"&gt;&amp;quot;/C=ES/ST=Spain/L=Barcelona/O=LinuxSysadmin/CN=shop.linuxsysadmin.tk&amp;quot;&lt;/span&gt;
Generating a &lt;span class="m"&gt;2048&lt;/span&gt; bit RSA private key
.......................................+++
.................................................................................................................................................................+++
writing new private key to &lt;span class="s1"&gt;&amp;#39;server.key&amp;#39;&lt;/span&gt;
-----
root@frontend:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ponemos la clave y el certificado generado en sus respectivas localizaciones, de acuerdo a los estándares.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@frontend:~# cp server.key /etc/ssl/private/
root@frontend:~# cp server.crt /etc/ssl/certs/
root@frontend:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Como verificación, así quedaría la carpeta &lt;em&gt;/etc/ssl/&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@frontend:~# tree /etc/ssl/
/etc/ssl/
├── certs
│   └── server.crt
├── openssl.cnf
└── private
    └── server.key

&lt;span class="m"&gt;2&lt;/span&gt; directories, &lt;span class="m"&gt;3&lt;/span&gt; files
root@frontend:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para poder autenticar mediante autenticación básica, generamos un usuario en un fichero tipo &lt;strong&gt;htpasswd&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@frontend:~# &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;admin:&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;openssl passwd -crypt s3cr3t&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt; /etc/nginx/shop.basic_auth
root@frontend:~# cat /etc/nginx/shop.basic_auth
admin:rOU9H0ABEB2H6
root@frontend:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con todas las piezas listas, montamos los virtualhosts, en un fichero de configuración o en varios, según nos apetezca.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@frontend:~# cat /etc/nginx/sites-enabled/shop
upstream backends &lt;span class="o"&gt;{&lt;/span&gt;
        server backend1:8080&lt;span class="p"&gt;;&lt;/span&gt;
        server backend2:8080&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

server &lt;span class="o"&gt;{&lt;/span&gt;
        listen &lt;span class="m"&gt;80&lt;/span&gt; default_server&lt;span class="p"&gt;;&lt;/span&gt;
        server_name _&lt;span class="p"&gt;;&lt;/span&gt;

        location / &lt;span class="o"&gt;{&lt;/span&gt;
                proxy_pass http://backends&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

server &lt;span class="o"&gt;{&lt;/span&gt;
        listen &lt;span class="m"&gt;443&lt;/span&gt; ssl&lt;span class="p"&gt;;&lt;/span&gt;
        server_name _&lt;span class="p"&gt;;&lt;/span&gt;

        ssl_certificate /etc/ssl/certs/server.crt&lt;span class="p"&gt;;&lt;/span&gt;
        ssl_certificate_key /etc/ssl/private/server.key&lt;span class="p"&gt;;&lt;/span&gt;

        auth_basic &lt;span class="s2"&gt;&amp;quot;Admin Area&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        auth_basic_user_file /etc/nginx/shop.basic_auth&lt;span class="p"&gt;;&lt;/span&gt;

        location / &lt;span class="o"&gt;{&lt;/span&gt;
                proxy_pass http://backoffice:8080&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@frontend:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La configuración es bastante estándar; se trata de un &lt;em&gt;server&lt;/em&gt; (equivalente en &lt;strong&gt;nginx&lt;/strong&gt; a un &lt;em&gt;virtualhost&lt;/em&gt; de &lt;strong&gt;apache&lt;/strong&gt;) para cada protocolo. La parte de administración es solamente la mediación &lt;strong&gt;SSL&lt;/strong&gt; y un &lt;em&gt;proxy_pass&lt;/em&gt; hacia el &lt;em&gt;backoffice&lt;/em&gt;. La parte de la API pública también se limita a hacer un &lt;em&gt;proxy_pass&lt;/em&gt;, solo que se hace contra &lt;em&gt;backends&lt;/em&gt; que es un objeto &lt;strong&gt;upstream&lt;/strong&gt;, que es el que define el balanceador.&lt;/p&gt;
&lt;p&gt;Ahora solo queda reiniciar el servidor web para aplicar los cambios. De acuerdo a la documentación, habría bastado un &lt;em&gt;reload&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@frontend:~# service nginx restart
root@frontend:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Comprobando que las aplicaciones funcionan&lt;/h2&gt;
&lt;p&gt;Para comprobar que la parte de la API funciona y balancea adecuadamente, basta con hacer peticiones. Podemos comprobar el &lt;em&gt;backend&lt;/em&gt; que la ha servido porque la aplicación pone una cabecera que especifica el nombre del &lt;em&gt;host&lt;/em&gt; que la resolvió. Con dos peticiones veremos que va alternativamente a cada &lt;em&gt;backend&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@frontend:~# curl -i http://localhost/products/
HTTP/1.1 &lt;span class="m"&gt;200&lt;/span&gt; OK
Server: nginx/1.6.2
Date: Fri, &lt;span class="m"&gt;26&lt;/span&gt; Feb &lt;span class="m"&gt;2016&lt;/span&gt; &lt;span class="m"&gt;11&lt;/span&gt;:04:38 GMT
Content-Type: application/json
Content-Length: &lt;span class="m"&gt;3&lt;/span&gt;
Connection: keep-alive
Backend: backend1

&lt;span class="o"&gt;[]&lt;/span&gt;
root@frontend:~# curl -i http://localhost/products/
HTTP/1.1 &lt;span class="m"&gt;200&lt;/span&gt; OK
Server: nginx/1.6.2
Date: Fri, &lt;span class="m"&gt;26&lt;/span&gt; Feb &lt;span class="m"&gt;2016&lt;/span&gt; &lt;span class="m"&gt;11&lt;/span&gt;:04:40 GMT
Content-Type: application/json
Content-Length: &lt;span class="m"&gt;3&lt;/span&gt;
Connection: keep-alive
Backend: backend2

&lt;span class="o"&gt;[]&lt;/span&gt;
root@frontend:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para la parte privada, haremos la petición, de la misma manera; vamos a añadir el flag &lt;em&gt;-k&lt;/em&gt; para sobrepasar el certificado autofirmado. Como no hemos indicado el usuario y la contraseña, nos devuelve un error 401, que indica que no estamos autorizados a pasar mas allá.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@frontend:~# curl -i -k https://localhost/products
HTTP/1.1 &lt;span class="m"&gt;401&lt;/span&gt; Unauthorized
Server: nginx/1.6.2
Date: Fri, &lt;span class="m"&gt;26&lt;/span&gt; Feb &lt;span class="m"&gt;2016&lt;/span&gt; &lt;span class="m"&gt;11&lt;/span&gt;:05:35 GMT
Content-Type: text/html
Content-Length: &lt;span class="m"&gt;194&lt;/span&gt;
Connection: keep-alive
WWW-Authenticate: Basic &lt;span class="nv"&gt;realm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Admin Area&amp;quot;&lt;/span&gt;

&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;401 Authorization Required&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;
&amp;lt;body &lt;span class="nv"&gt;bgcolor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;white&amp;quot;&lt;/span&gt;&amp;gt;
&amp;lt;center&amp;gt;&amp;lt;h1&amp;gt;401 Authorization Required&amp;lt;/h1&amp;gt;&amp;lt;/center&amp;gt;
&amp;lt;hr&amp;gt;&amp;lt;center&amp;gt;nginx/1.6.2&amp;lt;/center&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
root@frontend:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto parece que funciona, a falta de probar con un navegador adecuado.&lt;/p&gt;
&lt;h2&gt;Un pequeño detalle: abrimos los puertos&lt;/h2&gt;
&lt;p&gt;Puesto que este entorno está montado sobre virtualización &lt;strong&gt;LXC&lt;/strong&gt;, necesitamos que la dirección IP de la maquina anfitriona exponga los puertos de la máquina &lt;em&gt;frontend&lt;/em&gt;. Para ello hay que habilitar un mecanismo que se llama &lt;em&gt;port forwarding&lt;/em&gt;, coloquialmente conocido como "abrir el puerto".&lt;/p&gt;
&lt;p&gt;Mediante una directiva de &lt;strong&gt;firehol&lt;/strong&gt; indicamos que pasaremos todas las peticiones recibidas a los puertos 80 y 443 directamente a la máquina de &lt;em&gt;frontend&lt;/em&gt;. Hay que habilitar ese tráfico de &lt;strong&gt;FORWARD&lt;/strong&gt;, mediante otras reglas.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# cat /etc/firehol/firehol.conf
...
&lt;span class="nv"&gt;frontend_server&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;10.0.0.2&amp;quot;&lt;/span&gt;
...
dnat to &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$frontend_server&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; proto tcp dport &lt;span class="m"&gt;80&lt;/span&gt;
dnat to &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$frontend_server&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; proto tcp dport &lt;span class="m"&gt;443&lt;/span&gt;
...
router world2lan inface eth0 outface lxc0
    route http accept dst &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$frontend_server&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    route https accept dst &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$frontend_server&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
...
root@lxc:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y nuevamente reiniciamos el servicio para aplicar las nuevas reglas.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# service firehol restart
...
root@lxc:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Accediendo a las aplicaciones en la IP pública&lt;/h2&gt;
&lt;p&gt;Vamos a acceder con un navegador a la parte de administración, para ver que funciona y para rellenar algunos datos, para que se vea una respuesta de la API con fundamento.&lt;/p&gt;
&lt;p&gt;El primer paso consiste en abrir el navegador con la URL adecuada, y nos tropezamos con la autenticación.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Auth basic" src="https://www.linuxsysadmin.ml/images/entorno-escalable-auth-basic.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Tras pasar la autenticación podemos acceder a los formularios para añadir productos.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Admin form" src="https://www.linuxsysadmin.ml/images/entorno-escalable-admin-form.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Tras añadir tres productos, vemos que ya se genera la lista, en formato web.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Admin list" src="https://www.linuxsysadmin.ml/images/entorno-escalable-admin-list.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Con los datos introducidos podemos consumir la API, para comprobar que los datos que hemos introducido en la base de datos (mediante la aplicación de administración) están disponibles.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@desktop:~$ wget -qO- http://192.168.1.232/products/
&lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;price&amp;quot;&lt;/span&gt;: &lt;span class="m"&gt;1&lt;/span&gt;.5, 
        &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;123&amp;quot;&lt;/span&gt;, 
        &lt;span class="s2"&gt;&amp;quot;description&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;Apples&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;, 
    &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;price&amp;quot;&lt;/span&gt;: &lt;span class="m"&gt;1&lt;/span&gt;.0, 
        &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;456&amp;quot;&lt;/span&gt;, 
        &lt;span class="s2"&gt;&amp;quot;description&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;Oranges&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;, 
    &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;price&amp;quot;&lt;/span&gt;: &lt;span class="m"&gt;2&lt;/span&gt;.0, 
        &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;789&amp;quot;&lt;/span&gt;, 
        &lt;span class="s2"&gt;&amp;quot;description&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;Pears&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;]&lt;/span&gt;
gerard@desktop:~$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y consultando un producto concreto, también funciona como debe.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gerard@desktop:~$ wget -qO- http://192.168.1.232/products/456
&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;price&amp;quot;&lt;/span&gt;: &lt;span class="m"&gt;1&lt;/span&gt;.0, 
    &lt;span class="s2"&gt;&amp;quot;_id&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;456&amp;quot;&lt;/span&gt;, 
    &lt;span class="s2"&gt;&amp;quot;description&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;Oranges&amp;quot;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
gerard@desktop:~$ 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto comprobamos que todo queda en su sitio. Solo hará falta limpiar cualquier desecho que hayamos dejado en &lt;em&gt;/root/&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Y con este artículo cerramos la serie.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="jessie"></category><category term="proxy http"></category><category term="balanceador"></category><category term="ssl"></category><category term="nginx"></category><category term="virtual hosts"></category><category term="port forwarding"></category></entry><entry><title>Creando un entorno escalable (III)</title><link href="https://www.linuxsysadmin.ml/2016/03/creando-un-entorno-escalable-3.html" rel="alternate"></link><published>2016-03-14T08:00:00+01:00</published><updated>2016-03-14T08:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-03-14:/2016/03/creando-un-entorno-escalable-3.html</id><summary type="html">&lt;p&gt;En el artículo anterior de esta serie montamos el cluster de la base de datos que íbamos a necesitar para las aplicaciones que conformaban este entorno de ejemplo. Ahora que tenemos la base de datos, falta poner los servidores de aplicaciones que sirven nuestras aplicaciones y que usan el cluster …&lt;/p&gt;</summary><content type="html">&lt;p&gt;En el artículo anterior de esta serie montamos el cluster de la base de datos que íbamos a necesitar para las aplicaciones que conformaban este entorno de ejemplo. Ahora que tenemos la base de datos, falta poner los servidores de aplicaciones que sirven nuestras aplicaciones y que usan el cluster.&lt;/p&gt;
&lt;p&gt;Las aplicaciones que pretendemos servir son aplicaciones hechas en &lt;strong&gt;python&lt;/strong&gt;, siguiendo el protocolo &lt;strong&gt;WSGI&lt;/strong&gt;. Para ir rápidos, ambas utilizan el &lt;em&gt;framework&lt;/em&gt; &lt;strong&gt;bottle&lt;/strong&gt;. En realidad, nos sirve cualquier &lt;em&gt;framework&lt;/em&gt; que construya aplicaciones &lt;strong&gt;WSGI&lt;/strong&gt; estándares, de acuerdo al protocolo. Estas aplicaciones se conectan a la base de datos antes creadas para resolver las peticiones, mediante el &lt;em&gt;driver&lt;/em&gt; de &lt;strong&gt;mongodb&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Desde el punto de vista de entrada al servidor, ambas aplicaciones se van a servir mediante el protocolo &lt;strong&gt;HTTP&lt;/strong&gt; en puerto TCP 8080. Hay muchos servidores que sirven aplicaciones &lt;strong&gt;WSGI&lt;/strong&gt;, por ejemplo, &lt;strong&gt;Apache mod_wsgi&lt;/strong&gt;, &lt;strong&gt;gunicorn&lt;/strong&gt; o &lt;strong&gt;uWSGI&lt;/strong&gt;. De hecho hay docenas de ellos, casi todos capaces de servir aplicaciones &lt;strong&gt;WSGI&lt;/strong&gt; en un puerto cualquiera TCP.&lt;/p&gt;
&lt;p&gt;En este caso, usaremos un servidor de aplicaciones &lt;strong&gt;uWSGI&lt;/strong&gt; que, aunque es un poco mas complicado que &lt;strong&gt;gunicorn&lt;/strong&gt; (y menos que &lt;strong&gt;mod_wsgi&lt;/strong&gt;), me tiene enamorado. Destaco especialmente el modo de funcionamiento &lt;em&gt;emperador&lt;/em&gt; y la capacidad de usar un &lt;em&gt;virtualenv&lt;/em&gt; distinto para cada aplicación servida. De hecho, puede servir diferentes lenguajes y/o versiones, una por cada aplicación.&lt;/p&gt;
&lt;h2&gt;Instalar el servidor de aplicaciones&lt;/h2&gt;
&lt;p&gt;Este paso se repite en las máquinas &lt;em&gt;backend1&lt;/em&gt;, &lt;em&gt;backend2&lt;/em&gt; y  &lt;em&gt;backoffice&lt;/em&gt;; aunque cada una va a servir una aplicación distinta, el servidor de aplicaciones es el mismo. En puntos posteriores pondremos y activaremos las aplicaciones.&lt;/p&gt;
&lt;p&gt;El servidor &lt;strong&gt;uWSGI&lt;/strong&gt; está disponible en los repositorios oficiales de &lt;em&gt;Debian Jessie&lt;/em&gt;. Vamos a instalarlo con un &lt;em&gt;init script&lt;/em&gt; que levante un emperador y le vamos a añadir el &lt;em&gt;plugin&lt;/em&gt; para servir &lt;strong&gt;python&lt;/strong&gt; (en la versión 2.7, según podemos ver).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:~# apt-get install uwsgi-emperor uwsgi-plugin-python
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
Se instalarán los siguientes paquetes extras:
  file libexpat1 libffi6 libjansson4 libmagic1 libmatheval1 libpgm-5.1-0 libpython2.7 libpython2.7-minimal libpython2.7-stdlib
  libsodium13 libsqlite3-0 libxml2 libyaml-0-2 libzmq3 mime-support sgml-base uwsgi-core xml-core
Paquetes sugeridos:
  sgml-base-doc nginx-full cherokee libapache2-mod-proxy-uwsgi libapache2-mod-uwsgi libapache2-mod-ruwsgi uwsgi-plugins-all
  uwsgi-extra python-uwsgidecorators debhelper
Se instalarán los siguientes paquetes NUEVOS:
  file libexpat1 libffi6 libjansson4 libmagic1 libmatheval1 libpgm-5.1-0 libpython2.7 libpython2.7-minimal libpython2.7-stdlib
  libsodium13 libsqlite3-0 libxml2 libyaml-0-2 libzmq3 mime-support sgml-base uwsgi-core uwsgi-emperor uwsgi-plugin-python
  xml-core
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;21&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;6&lt;/span&gt;.608 kB de archivos.
Se utilizarán &lt;span class="m"&gt;25&lt;/span&gt;,9 MB de espacio de disco adicional después de esta operación.
¿Desea continuar? &lt;span class="o"&gt;[&lt;/span&gt;S/n&lt;span class="o"&gt;]&lt;/span&gt; s
...
root@backend1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto ya tenemos el servidor de aplicaciones en funcionamiento. Las instancias se declaran con un fichero de configuración en &lt;em&gt;/etc/uwsgi-emperor/vassals/&lt;/em&gt;, que haremos mas adelante.&lt;/p&gt;
&lt;h2&gt;Consideraciones de seguridad&lt;/h2&gt;
&lt;p&gt;Estas aplicaciones usarán el &lt;em&gt;driver&lt;/em&gt; &lt;strong&gt;pymongo&lt;/strong&gt; para conectar a las instancias de &lt;strong&gt;mongodb&lt;/strong&gt;. Para eso hay que habilitar el tráfico relativo (de los servidores de aplicaciones a los de mongodb, por el puerto TCP 27017).&lt;/p&gt;
&lt;p&gt;En nuestro caso, como estamos trabajando con &lt;strong&gt;LXC&lt;/strong&gt;, lo haremos desde el &lt;em&gt;host&lt;/em&gt;, mediante la modificación de las reglas de &lt;em&gt;firehol&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# cat /etc/firehol/firehol.conf
&lt;span class="nv"&gt;mongo_servers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;10.0.0.5 10.0.0.6 10.0.0.7&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;app_servers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;10.0.0.3 10.0.0.4 10.0.0.5&amp;quot;&lt;/span&gt;
...  
router internal inface lxc0 outface lxc0
...  
      route custom mongodb tcp/27017 default accept src &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$app_servers&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; dst &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$mongo_servers&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
...  
root@lxc:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;No os olvidéis de reiniciar el servicio &lt;em&gt;firehol&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Instalando las aplicaciones&lt;/h2&gt;
&lt;p&gt;Este punto se hace en los tres servidores que sirven aplicaciones (&lt;em&gt;backend1&lt;/em&gt;, &lt;em&gt;backend2&lt;/em&gt; y &lt;em&gt;backoffice&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Las aplicaciones de ejemplo que vamos a usar las podéis encontrar en &lt;a href="https://www.linuxsysadmin.ml/downloads/shop.tar.gz"&gt;este enlace&lt;/a&gt;. Debo admitir que no son bonitas, pero para esta demostración, nos valen.&lt;/p&gt;
&lt;p&gt;Descomprimimos el fichero comprimido con las dos aplicaciones.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:~# tar xzf shop.tar.gz
root@backend1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Esta es la estructura que queda tras descomprimir:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:~# tree
.
├── shop
│   ├── requirements.txt
│   ├── shop_admin
│   │   ├── app.py
│   │   └── views
│   │       ├── index.tpl
│   │       ├── product_form.tpl
│   │       └── product_list.tpl
│   └── shop_api
│       └── app.py
└── shop.tar.gz

&lt;span class="m"&gt;4&lt;/span&gt; directories, &lt;span class="m"&gt;7&lt;/span&gt; files
root@backend1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Esta estructura tiene las dos aplicaciones. Cada tipo de servidor usará solo una por simplicidad, así que borraremos la que no se utilice, de acuerdo al tipo de servidor.&lt;/p&gt;
&lt;p&gt;En resumen, vamos a poner la carpeta &lt;em&gt;shop&lt;/em&gt; en &lt;em&gt;/opt/&lt;/em&gt;, y vamos a poner dentro el &lt;em&gt;virtualenv&lt;/em&gt; con las librerías necesarias.&lt;/p&gt;
&lt;p&gt;Como buena &lt;em&gt;praxis&lt;/em&gt;, vamos a instalar las librerías en un &lt;em&gt;virtualenv&lt;/em&gt; dedicado por aplicación. Para ello necesitamos la herramienta, que puede salir del repositorio oficial o lo podemos descargar, para usarlo y desecharlo posteriormente. Podemos encontrar el paquete en &lt;a href="https://pypi.python.org/packages/source/v/virtualenv/virtualenv-14.0.6.tar.gz#md5=a035037925c82990a7659ecf8764bcdb"&gt;este enlace&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lo descomprimimos y lo dejamos ahí, para que los puntos específicos para cada servidor lo usen a su antojo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:~# tar xzf virtualenv-14.0.6.tar.gz
root@backend1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El &lt;em&gt;script&lt;/em&gt; de creación del &lt;em&gt;virtualenv&lt;/em&gt; se ejecuta con &lt;strong&gt;python&lt;/strong&gt;; así que también lo necesitamos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:~# apt-get install python
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
Se instalarán los siguientes paquetes extras:
  libpython-stdlib python-minimal python2.7 python2.7-minimal
Paquetes sugeridos:
  python-doc python-tk python2.7-doc binutils binfmt-support
Se instalarán los siguientes paquetes NUEVOS:
  libpython-stdlib python python-minimal python2.7 python2.7-minimal
0 actualizados, 5 nuevos se instalarán, 0 para eliminar y 0 no actualizados.
Se necesita descargar 1.854 kB de archivos.
Se utilizarán 5.131 kB de espacio de disco adicional después de esta operación.
¿Desea continuar? [S/n] s
..
root@backend1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Veamos ahora los puntos específicos por tipo de aplicación.&lt;/p&gt;
&lt;h3&gt;Aplicación de backend: la API pública&lt;/h3&gt;
&lt;p&gt;Este punto se ejecuta solamente en los &lt;em&gt;backends&lt;/em&gt; (&lt;em&gt;backend1&lt;/em&gt; y &lt;em&gt;backend2&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Eliminamos la aplicación de administración, que no se usa en los &lt;em&gt;backends&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:~# rm -R shop/shop_admin/
root@backend1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Así nos queda la carpeta:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:~# tree shop
shop
├── requirements.txt
└── shop_api
    └── app.py

&lt;span class="m"&gt;1&lt;/span&gt; directory, &lt;span class="m"&gt;2&lt;/span&gt; files
root@backend1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Copiamos la carpeta a &lt;em&gt;/opt/&lt;/em&gt; que va a ser su emplazamiento habitual.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:~# cp -R shop/ /opt/
root@backend1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a trabajar ya desde la carpeta contenedora del proyecto.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:~# &lt;span class="nb"&gt;cd&lt;/span&gt; /opt/shop/
root@backend1:/opt/shop#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El siguiente paso es crear el conjunto de librerías necesarias, construyendo un &lt;em&gt;virtualenv&lt;/em&gt; con las librerías. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:/opt/shop# /root/virtualenv-14.0.6/virtualenv.py env
New python executable in /opt/shop/env/bin/python
Installing setuptools, pip, wheel...done.
root@backend1:/opt/shop#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Activamos el entorno virtual para instalar las librerías declaradas en el fichero &lt;em&gt;requirements.txt&lt;/em&gt;. Luego salimos del entorno.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:/opt/shop# . env/bin/activate
&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@backend1:/opt/shop# pip install -r requirements.txt
Collecting &lt;span class="nv"&gt;bottle&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.12.9 &lt;span class="o"&gt;(&lt;/span&gt;from -r requirements.txt &lt;span class="o"&gt;(&lt;/span&gt;line &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
...
Installing collected packages: bottle, pymongo
Successfully installed bottle-0.12.9 pymongo-3.2
&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@backend1:/opt/shop# deactivate
root@backend1:/opt/shop#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y para evitarnos problemas de permisos, uniformizamos el propietario de la carpeta:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:~# chown -R www-data:www-data /opt/shop/
root@backend1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Con todo lo necesario para levantar la aplicación, la declaramos como &lt;em&gt;vasallo&lt;/em&gt; del &lt;em&gt;emperador&lt;/em&gt;; el mismo &lt;strong&gt;emperador&lt;/strong&gt; va a levantar un proceso para servir esa configuración.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:/opt/shop# cat /etc/uwsgi-emperor/vassals/shop_api.ini
&lt;span class="o"&gt;[&lt;/span&gt;uwsgi&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;plugin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; python
http-socket &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:8080
&lt;span class="nv"&gt;master&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="nv"&gt;workers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="nv"&gt;virtualenv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /opt/shop/env
&lt;span class="nv"&gt;chdir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /opt/shop/shop_api
&lt;span class="nv"&gt;module&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; app:app
root@backend1:/opt/shop#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y podemos comprobar que todo funciona como debe haciendo una petición a la &lt;strong&gt;API&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backend1:~# curl -i http://localhost:8080/products/
HTTP/1.1 &lt;span class="m"&gt;200&lt;/span&gt; OK
Content-Length: &lt;span class="m"&gt;3&lt;/span&gt;
Content-Type: application/json
Backend: backend1

&lt;span class="o"&gt;[]&lt;/span&gt;
root@backend1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Aplicación de backoffice: la interfaz de administración&lt;/h3&gt;
&lt;p&gt;Este punto aplica solamente a la máquina &lt;em&gt;backoffice&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;El proceso es análogo al de los &lt;em&gt;backends&lt;/em&gt;; quitamos la aplicación que no vamos a utilizar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backoffice:~# rm -R shop/shop_api/
root@backoffice:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Así nos queda la carpeta:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backoffice:~# tree shop
shop
├── requirements.txt
└── shop_admin
    ├── app.py
    └── views
        ├── index.tpl
        ├── product_form.tpl
        └── product_list.tpl

&lt;span class="m"&gt;2&lt;/span&gt; directories, &lt;span class="m"&gt;5&lt;/span&gt; files
root@backoffice:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La transferimos a la carpeta &lt;em&gt;/opt/&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backoffice:~# cp -R shop/ /opt/
root@backoffice:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Nos situamos en la carpeta contenedora:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backoffice:~# &lt;span class="nb"&gt;cd&lt;/span&gt; /opt/shop/
root@backoffice:/opt/shop#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Creamos el &lt;em&gt;virtualenv&lt;/em&gt; en la carpeta contenedora.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backoffice:/opt/shop# /root/virtualenv-14.0.6/virtualenv.py env
New python executable in /opt/shop/env/bin/python
Installing setuptools, pip, wheel...done.
root@backoffice:/opt/shop#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y le instalamos las librerías necesarias, declaradas en el fichero &lt;em&gt;requirements.txt&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backoffice:/opt/shop# . env/bin/activate
&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@backoffice:/opt/shop# pip install -r requirements.txt
Collecting &lt;span class="nv"&gt;bottle&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.12.9 &lt;span class="o"&gt;(&lt;/span&gt;from -r requirements.txt &lt;span class="o"&gt;(&lt;/span&gt;line &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
...
Installing collected packages: bottle, pymongo
Successfully installed bottle-0.12.9 pymongo-3.2
&lt;span class="o"&gt;(&lt;/span&gt;env&lt;span class="o"&gt;)&lt;/span&gt; root@backoffice:/opt/shop# deactivate
root@backoffice:/opt/shop#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Actualizamos el propietario de la aplicación &lt;strong&gt;WSGI&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backoffice:/opt/shop# chown -R www-data:www-data /opt/shop/
root@backoffice:/opt/shop#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y creamos el fichero de configuración del &lt;em&gt;vasallo&lt;/em&gt;, para que lo levante el &lt;em&gt;emperador&lt;/em&gt;, quedando así:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backoffice:/opt/shop# cat /etc/uwsgi-emperor/vassals/shop_admin.ini
&lt;span class="o"&gt;[&lt;/span&gt;uwsgi&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;plugin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; python
http-socket &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0:8080
&lt;span class="nv"&gt;master&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="nv"&gt;workers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="nv"&gt;virtualenv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /opt/shop/env
&lt;span class="nv"&gt;chdir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /opt/shop/shop_admin
&lt;span class="nv"&gt;module&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; app:app
root@backoffice:/opt/shop#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y comprobamos que obtenemos la página web que se espera:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@backoffice:~# curl -i http://localhost:8080/
HTTP/1.1 &lt;span class="m"&gt;200&lt;/span&gt; OK
Content-Length: &lt;span class="m"&gt;33&lt;/span&gt;
Content-Type: text/html&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nv"&gt;charset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;UTF-8

&amp;lt;a &lt;span class="nv"&gt;href&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/products&amp;quot;&lt;/span&gt;&amp;gt;Products&amp;lt;/a&amp;gt;
root@backoffice:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto hemos acabado con las aplicaciones. Nuevamente, todo lo que queda en la carpeta &lt;em&gt;/root/&lt;/em&gt; es desechable.&lt;/p&gt;
&lt;p&gt;En el siguiente artículo vamos a montar el &lt;em&gt;proxy&lt;/em&gt;/balanceador que va a actuar como fachada de todo el sistema.&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="jessie"></category><category term="WSGI"></category><category term="uWSGI"></category><category term="python"></category><category term="virtualenv"></category><category term="firehol"></category></entry><entry><title>Creando un entorno escalable (II)</title><link href="https://www.linuxsysadmin.ml/2016/03/creando-un-entorno-escalable-2.html" rel="alternate"></link><published>2016-03-07T08:00:00+01:00</published><updated>2016-03-07T08:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-03-07:/2016/03/creando-un-entorno-escalable-2.html</id><summary type="html">&lt;p&gt;Seguimos con la serie de montar un entorno escalable. Tras explicar en el primer artículo lo que vamos a montar, seguimos con ello. En este artículo vamos a montar un &lt;em&gt;cluster&lt;/em&gt; de bases de datos; será &lt;strong&gt;mongodb&lt;/strong&gt; porque la aplicación lo requiere y usará la topología de un &lt;strong&gt;replica set …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Seguimos con la serie de montar un entorno escalable. Tras explicar en el primer artículo lo que vamos a montar, seguimos con ello. En este artículo vamos a montar un &lt;em&gt;cluster&lt;/em&gt; de bases de datos; será &lt;strong&gt;mongodb&lt;/strong&gt; porque la aplicación lo requiere y usará la topología de un &lt;strong&gt;replica set&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Este artículo se basa enormemente en &lt;a href="https://www.linuxsysadmin.ml/2015/12/construyendo-una-replica-set-en-mongodb.html"&gt;otro artículo&lt;/a&gt; que ya publicamos, al que vamos a añadir algunas mejoras reflejadas en otros.&lt;/p&gt;
&lt;p&gt;Como ya vimos en el artículo referido, solo necesitamos levantar un proceso &lt;em&gt;mongod&lt;/em&gt; en cada una de las máquinas, para posteriormente casarlos entre sí.&lt;/p&gt;
&lt;h2&gt;Levantando los procesos de mongodb&lt;/h2&gt;
&lt;p&gt;Este punto se repite en las máquinas que van a formar la &lt;strong&gt;replica set&lt;/strong&gt;, que son &lt;em&gt;mongo1&lt;/em&gt;, &lt;em&gt;mongo2&lt;/em&gt; y &lt;em&gt;backoffice&lt;/em&gt;. Vamos a seguir solamente una de ellas; el resto son análogas.&lt;/p&gt;
&lt;p&gt;Crearemos una estructura en &lt;em&gt;/opt/&lt;/em&gt; para alojar los binarios, las configuraciones, los datos y los logs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# mkdir -p /opt/mongodb/&lt;span class="o"&gt;{&lt;/span&gt;bin,conf,data,logs&lt;span class="o"&gt;}&lt;/span&gt;
root@mongo1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En la carpeta de binarios vamos a poner el único que se necesita: el &lt;em&gt;mongod&lt;/em&gt;. Lo podemos sacar descomprimiendo el 
fichero comprimido &lt;em&gt;.tar.gz&lt;/em&gt; de la página de descargas de &lt;strong&gt;mongodb&lt;/strong&gt;. En nuestro caso concreto, lo he sacado de &lt;a href="https://fastdl.mongodb.org/linux/mongodb-linux-i686-3.2.3.tgz"&gt;https://fastdl.mongodb.org/linux/mongodb-linux-i686-3.2.3.tgz&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# cp mongodb-linux-i686-3.2.3/bin/mongod /opt/mongodb/bin/
root@mongo1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ponemos un fichero de configuración para la instancia que queremos correr. Esta configuración puede variar mucho, pero un ejemplo básico para salir del paso con una máquina de 32 bits (que no soportan &lt;em&gt;WiredTiger&lt;/em&gt;) podría ser:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# cat /opt/mongodb/conf/mongo.conf
systemLog:
    path: /opt/mongodb/logs/mongo.log
    logAppend: &lt;span class="nb"&gt;true&lt;/span&gt;
    destination: file

net:
    port: &lt;span class="m"&gt;27017&lt;/span&gt;
    bindIp: &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0

storage:
    dbPath: /opt/mongodb/data/
    engine: mmapv1
    mmapv1:
        smallFiles: &lt;span class="nb"&gt;true&lt;/span&gt;

replication:
    replSetName: rs
root@mongo1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Truco&lt;/strong&gt;: Es un buen momento para montar un sistema de ficheros alternativo para almacenar los datos, sea poner &lt;a href="https://www.linuxsysadmin.ml/2016/01/lvm-logical-volume-manager.html"&gt;LVM&lt;/a&gt; (para tener crecimiento dinámico o &lt;a href="https://www.linuxsysadmin.ml/2016/02/haciendo-snapshots-con-lvm.html"&gt;snapshots&lt;/a&gt;, sea un &lt;a href="https://www.linuxsysadmin.ml/2015/12/construyendo-un-raid-10-en-linux.html"&gt;RAID&lt;/a&gt; (por ejemplo para tener alto rendimiento y/o replicación de datos), o incluso ambos.&lt;/p&gt;
&lt;p&gt;Cumpliendo con una política de seguridad básica, vamos a crear un usuario de sistema para correr el proceso.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# useradd -s /usr/sbin/nologin -r -M mongo -d /opt/mongodb/
root@mongo1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Le damos la propiedad de toda la estructura de &lt;strong&gt;mongodb&lt;/strong&gt;, para ahorrarnos problemas de permisos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# chown -R mongo:mongo /opt/mongodb/
root@mongo1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y ya como resumen, ponemos una salida para ver como nos queda la estructura:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# tree /opt/mongodb/
/opt/mongodb/
├── bin
│   └── mongod
├── conf
│   └── mongo.conf
├── data
└── logs

&lt;span class="m"&gt;4&lt;/span&gt; directories, &lt;span class="m"&gt;2&lt;/span&gt; files
root@mongo1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El último paso consiste en crear una &lt;em&gt;unit&lt;/em&gt; en &lt;strong&gt;systemd&lt;/strong&gt; (o un &lt;em&gt;init script&lt;/em&gt;, dependiendo de la distribución usada; de hecho, cada máquina puede ir con una distribución distinta).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# cat /etc/systemd/system/mongo.service
&lt;span class="o"&gt;[&lt;/span&gt;Unit&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;Description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;MongoDB

&lt;span class="o"&gt;[&lt;/span&gt;Service&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;User&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;mongo
&lt;span class="nv"&gt;LimitFSIZE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;infinity
&lt;span class="nv"&gt;LimitCPU&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;infinity
&lt;span class="nv"&gt;LimitAS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;infinity
&lt;span class="nv"&gt;LimitNOFILE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;64000&lt;/span&gt;
&lt;span class="nv"&gt;LimitNPROC&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;64000&lt;/span&gt;
&lt;span class="nv"&gt;ExecStartPre&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/bin/rm -f /opt/mongodb/data/mongod.lock
&lt;span class="nv"&gt;ExecStart&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/opt/mongodb/bin/mongod -f /opt/mongodb/conf/mongo.conf

&lt;span class="o"&gt;[&lt;/span&gt;Install&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;WantedBy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;multi-user.target
root@mongo1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Lo activamos para que se levante solo en los siguientes arranques, y lo levantamos para la sesión actual.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# systemctl &lt;span class="nb"&gt;enable&lt;/span&gt; mongo
Created symlink from /etc/systemd/system/multi-user.target.wants/mongo.service to /etc/systemd/system/mongo.service.
root@mongo1:~# systemctl start mongo
root@mongo1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Repetid este paso en las otras máquinas de &lt;strong&gt;mongodb&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Consideraciones de seguridad&lt;/h2&gt;
&lt;p&gt;Para que una &lt;strong&gt;replica set&lt;/strong&gt; funcione como debe, todos los procesos deben comunicarse entre sí. Como los hemos puesto en el mismo puerto, podemos agruparlo todo en una sola regla.&lt;/p&gt;
&lt;p&gt;Como en nuestro caso estamos virtualizando con &lt;strong&gt;LXC&lt;/strong&gt;, vamos a controlar el tráfico con el &lt;strong&gt;firehol&lt;/strong&gt; de la máquina anfitriona.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;...
root@lxc:~# cat /etc/firehol/firehol.conf
&lt;span class="nv"&gt;mongo_servers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;10.0.0.5 10.0.0.6 10.0.0.7&amp;quot;&lt;/span&gt;
...  
router internal inface lxc0 outface lxc0
    route custom mongodb tcp/27017 default accept src &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$mongo_servers&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; dst &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$mongo_servers&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
...
root@lxc:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Acordaos de reiniciar &lt;strong&gt;firehol&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Atando la replica set&lt;/h2&gt;
&lt;p&gt;Este paso se ejecuta en una sola máquina, que va a reproducir los cambios a las demás, por efecto de la &lt;strong&gt;replica set&lt;/strong&gt;. Por ejemplo, lo hago en &lt;em&gt;mongo1&lt;/em&gt;, por hacer alguna.&lt;/p&gt;
&lt;p&gt;Entramos en el &lt;em&gt;mongo shell&lt;/em&gt;, desde donde lanzaremos el resto de comandos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# ./mongodb-linux-i686-3.2.3/bin/mongo
MongoDB shell version: &lt;span class="m"&gt;3&lt;/span&gt;.2.3
connecting to: &lt;span class="nb"&gt;test&lt;/span&gt;
Welcome to the MongoDB shell.
For interactive help, &lt;span class="nb"&gt;type&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;help&amp;quot;&lt;/span&gt;.
...
&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Siguiendo los pasos estándares, creamos una configuración vacía en la máquina elegida, y añadimos las otras dos. Tened en cuenta que la máquina &lt;em&gt;backoffice&lt;/em&gt; se declara como un árbitro, por decisión de diseño.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; rs.initiate&lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;info2&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;no configuration specified. Using a default configuration for the set&amp;quot;&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;me&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;mongo1:27017&amp;quot;&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
rs:SECONDARY&amp;gt; rs.add&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mongo2:27017&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
rs:PRIMARY&amp;gt; rs.addArb&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;backoffice:27017&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
rs:PRIMARY&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos verificar que todo está bien mediante el comando &lt;em&gt;rs.status()&lt;/em&gt;, como sigue:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rs:PRIMARY&amp;gt; rs.status&lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;set&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;rs&amp;quot;&lt;/span&gt;,
...  
        &lt;span class="s2"&gt;&amp;quot;members&amp;quot;&lt;/span&gt; : &lt;span class="o"&gt;[&lt;/span&gt;
                &lt;span class="o"&gt;{&lt;/span&gt;
...  
                        &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;mongo1:27017&amp;quot;&lt;/span&gt;,
                        &lt;span class="s2"&gt;&amp;quot;stateStr&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;PRIMARY&amp;quot;&lt;/span&gt;,
...  
                &lt;span class="o"&gt;}&lt;/span&gt;,
                &lt;span class="o"&gt;{&lt;/span&gt;
...  
                        &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;mongo2:27017&amp;quot;&lt;/span&gt;,
                        &lt;span class="s2"&gt;&amp;quot;stateStr&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;SECONDARY&amp;quot;&lt;/span&gt;,
...  
                &lt;span class="o"&gt;}&lt;/span&gt;,
                &lt;span class="o"&gt;{&lt;/span&gt;
...  
                        &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;backoffice:27017&amp;quot;&lt;/span&gt;,
                        &lt;span class="s2"&gt;&amp;quot;stateStr&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;ARBITER&amp;quot;&lt;/span&gt;,
...  
                &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;]&lt;/span&gt;,
        &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
rs:PRIMARY&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y salimos del &lt;em&gt;mongo shell&lt;/em&gt;, que ya no necesitamos; las aplicaciones de &lt;em&gt;backend&lt;/em&gt; y de &lt;em&gt;backoffice&lt;/em&gt; ya incluyen una librería para conectarse por sí mismos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rs:PRIMARY&amp;gt; &lt;span class="nb"&gt;exit&lt;/span&gt;
bye
root@mongo1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Todo lo que queda en &lt;em&gt;/root/&lt;/em&gt; es innecesario y se puede borrar. De todas formas podemos dejar el resto de binarios en &lt;em&gt;/opt/mongodb/&lt;/em&gt; en alguna de las máquinas por si acaso.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;El siguiente paso va a ser montar los servidores de aplicaciones en los backends y en el backoffice&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="jessie"></category><category term="mongodb"></category><category term="replica set"></category><category term="systemd"></category><category term="firehol"></category></entry><entry><title>Creando un entorno escalable (I)</title><link href="https://www.linuxsysadmin.ml/2016/02/creando-un-entorno-escalable.html" rel="alternate"></link><published>2016-02-29T08:00:00+01:00</published><updated>2016-02-29T08:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-02-29:/2016/02/creando-un-entorno-escalable.html</id><summary type="html">&lt;p&gt;Mucha gente tiene un servidor único para alojar páginas web dinámicas, por ejemplo con &lt;strong&gt;PHP&lt;/strong&gt; y con &lt;strong&gt;MySQL&lt;/strong&gt;. Sin embargo, a veces esto puede resultar insuficiente; nos puede interesar tener un entorno de bajas especificaciones y de bajo coste, pero preparado crecer al mismo ritmo que lo hacen los usuarios …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Mucha gente tiene un servidor único para alojar páginas web dinámicas, por ejemplo con &lt;strong&gt;PHP&lt;/strong&gt; y con &lt;strong&gt;MySQL&lt;/strong&gt;. Sin embargo, a veces esto puede resultar insuficiente; nos puede interesar tener un entorno de bajas especificaciones y de bajo coste, pero preparado crecer al mismo ritmo que lo hacen los usuarios.&lt;/p&gt;
&lt;p&gt;En este caso, el truco consiste en hacer trabajar a varias máquinas como si fueran una sola, escondidas en una o varias subredes privadas y poniendo un representante único de todo el sistema (que es el que va a recibir &lt;strong&gt;todas&lt;/strong&gt; las peticiones).&lt;/p&gt;
&lt;p&gt;Este representante suele ser lo que llamamos un &lt;strong&gt;balanceador de carga&lt;/strong&gt;, cuya función es repartir el trabajo entre varios servidores de &lt;strong&gt;backend&lt;/strong&gt;. Al tratarse solo de un "policía de tráfico" su rendimiento es elevado con unas especificaciones modestas, mientras que los servidores de &lt;strong&gt;backend&lt;/strong&gt; consiguen resolver las mismas peticiones por unidad de tiempo; la mejora reside en que pueden haber varios servidores de &lt;strong&gt;backend&lt;/strong&gt; resolviendo peticiones en paralelo.&lt;/p&gt;
&lt;p&gt;Normalmente, estos servidores de &lt;strong&gt;backend&lt;/strong&gt; suelen conectarse a otros servicios (idealmente en otros servidores) para cumplir con sus funciones, por ejemplo con un grupo de servidores de &lt;strong&gt;bases de datos&lt;/strong&gt; dispuestos como un &lt;em&gt;cluster&lt;/em&gt;, que suelen tener una topología propia.&lt;/p&gt;
&lt;p&gt;En este tutorial se va a montar un entorno pequeño de estas características, sirviendo una &lt;em&gt;API&lt;/em&gt; pública en servidores de &lt;strong&gt;backend&lt;/strong&gt;, una aplicación web de administración de los datos de la &lt;em&gt;API&lt;/em&gt; en un servidor de &lt;strong&gt;backoffice&lt;/strong&gt;, y un &lt;em&gt;cluster&lt;/em&gt; de &lt;strong&gt;bases de datos&lt;/strong&gt; representado por una &lt;em&gt;replica set&lt;/em&gt; de MongoDB; todo ello oculto en una red privada y un balanceador usando &lt;em&gt;virtualhosts&lt;/em&gt; para ir a una aplicación u otra según el protocolo usado.&lt;/p&gt;
&lt;p&gt;Esto es lo que propongo montar:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Entorno propuesto" src="https://www.linuxsysadmin.ml/images/entorno_propuesto.png"&gt;&lt;/p&gt;
&lt;p&gt;Para ello, vamos a crear las máquinas virtuales necesarias. En este caso, voy a usar mi servidor de &lt;strong&gt;virtualización con LXC&lt;/strong&gt;, tal como lo monté en &lt;a href="https://www.linuxsysadmin.ml/2015/11/virtualizando-contenedores-lxc-tras-bridge-interno.html"&gt;este artículo&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# lxc-ls -f
NAME        STATE    IPV4        IPV6  AUTOSTART
------------------------------------------------
backend1    RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.3    -     YES
backend2    RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.4    -     YES
backoffice  RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.5    -     YES
frontend    RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2    -     YES
mongo1      RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.6    -     YES
mongo2      RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.7    -     YES
root@lxc:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para hacer mas fácil las referencias a las diferentes máquinas, vamos a utilizar sus nombres; como no me apetece montar un servidor DNS, vamos a ponerlas en el fichero &lt;em&gt;/etc/hosts&lt;/em&gt; en todas las máquinas virtuales.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# cat /etc/hosts
...
10.0.0.2        frontend
10.0.0.3        backend1
10.0.0.4        backend2
10.0.0.5        backoffice
10.0.0.6        mongo1
10.0.0.7        mongo2
...
root@mongo1:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a ir montando todas las máquinas una por una; es laborioso pero no es nada complicado. Las reglas del &lt;em&gt;firewall&lt;/em&gt; también las iremos explicando según el rol de cada máquina.&lt;/p&gt;
&lt;p&gt;El orden de montaje no es importante, pero como queremos ir comprobando en cada caso que va funcionando, se montarán de acuerdo al orden de requisitos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;El &lt;em&gt;cluster&lt;/em&gt; de &lt;strong&gt;bases de datos&lt;/strong&gt;, que no tiene dependencias.&lt;/li&gt;
&lt;li&gt;Los servidores de &lt;strong&gt;backend&lt;/strong&gt; y &lt;strong&gt;backoffice&lt;/strong&gt; que dependen del &lt;em&gt;cluster&lt;/em&gt; de &lt;strong&gt;bases de datos&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Finalmente, pondremos el servidor de &lt;strong&gt;frontend&lt;/strong&gt;, con los &lt;em&gt;virtualhosts&lt;/em&gt; y el balanceador, lanzando las peticiones contra los &lt;strong&gt;backends&lt;/strong&gt; y el &lt;strong&gt;backoffice&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Sabiendo lo que vamos a montar, solo queda decir: ¡Manos a la obra!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><category term="linux"></category><category term="entorno"></category><category term="escalable"></category></entry><entry><title>Restringiendo accesos mediante certificados de cliente</title><link href="https://www.linuxsysadmin.ml/2016/02/restringiendo-accesos-mediante-certificados-de-cliente.html" rel="alternate"></link><published>2016-02-22T08:00:00+01:00</published><updated>2016-02-22T08:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-02-22:/2016/02/restringiendo-accesos-mediante-certificados-de-cliente.html</id><summary type="html">&lt;p&gt;De vez en cuando, tenemos algún contenido web o una API que necesita un control de acceso superior. El método mas eficaz del que disponemos hoy en día es la autenticación con certificados SSL cliente, en donde es el cliente el que debe ofrecer un certificado que el servidor validará …&lt;/p&gt;</summary><content type="html">&lt;p&gt;De vez en cuando, tenemos algún contenido web o una API que necesita un control de acceso superior. El método mas eficaz del que disponemos hoy en día es la autenticación con certificados SSL cliente, en donde es el cliente el que debe ofrecer un certificado que el servidor validará.&lt;/p&gt;
&lt;p&gt;Como se trata de proteger contenido web, vamos a necesitar un servidor web, por ejemplo, &lt;strong&gt;nginx&lt;/strong&gt;. De paso, vamos a instalar el paquete &lt;strong&gt;openssl&lt;/strong&gt;, que nos permitirá generar los certificados usados.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# apt-get install nginx-light openssl
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
Se instalarán los siguientes paquetes extras:
  nginx-common
Paquetes sugeridos:
  fcgiwrap nginx-doc ssl-cert ca-certificates
Se instalarán los siguientes paquetes NUEVOS:
  nginx-common nginx-light openssl
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;3&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;1&lt;/span&gt;.126 kB de archivos.
Se utilizarán &lt;span class="m"&gt;2&lt;/span&gt;.148 kB de espacio de disco adicional después de esta operación.
¿Desea continuar? &lt;span class="o"&gt;[&lt;/span&gt;S/n&lt;span class="o"&gt;]&lt;/span&gt; s
...
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Generar el certificado y la clave de la CA&lt;/h2&gt;
&lt;p&gt;Empezaremos por generar la clave de la CA, que va a servir para firmar el certificado que pondremos en el servidor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl genrsa -des3 -out ca.key &lt;span class="m"&gt;4096&lt;/span&gt;
Generating RSA private key, &lt;span class="m"&gt;4096&lt;/span&gt; bit long modulus
...................................................................++
.++
e is &lt;span class="m"&gt;65537&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;0x10001&lt;span class="o"&gt;)&lt;/span&gt;
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; ca.key:
Verifying - Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; ca.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora generamos el certificado de la CA. Lo generamos directamente firmado en un solo paso.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl req -new -x509 -days &lt;span class="m"&gt;365&lt;/span&gt; -key ca.key -out ca.crt -subj &lt;span class="s2"&gt;&amp;quot;/C=ES/ST=Spain/L=Barcelona/O=LinuxSysadmin&amp;quot;&lt;/span&gt;
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; ca.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Generar el certificado para el servidor web&lt;/h2&gt;
&lt;p&gt;Generamos la clave para el certificado del servidor web.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl genrsa -des3 -out server.key &lt;span class="m"&gt;4096&lt;/span&gt;
Generating RSA private key, &lt;span class="m"&gt;4096&lt;/span&gt; bit long modulus
.............................................++
.....................................++
e is &lt;span class="m"&gt;65537&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;0x10001&lt;span class="o"&gt;)&lt;/span&gt;
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; server.key:
Verifying - Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; server.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora creamos un certificado para el servidor web. Es importante que el campo &lt;strong&gt;CN&lt;/strong&gt; sea el mismo que el nombre del &lt;em&gt;virtualhost&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl req -new -key server.key -out server.csr -subj &lt;span class="s2"&gt;&amp;quot;/C=ES/ST=Spain/L=Barcelona/O=LinuxSysadmin/CN=private.linuxsysadmin.tk&amp;quot;&lt;/span&gt;
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; server.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y lo firmamos con la clave y el certificado de la CA.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl x509 -req -days &lt;span class="m"&gt;365&lt;/span&gt; -in server.csr -CA ca.crt -CAkey ca.key -set_serial &lt;span class="m"&gt;01&lt;/span&gt; -out server.crt
Signature ok
&lt;span class="nv"&gt;subject&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/C&lt;span class="o"&gt;=&lt;/span&gt;ES/ST&lt;span class="o"&gt;=&lt;/span&gt;Spain/L&lt;span class="o"&gt;=&lt;/span&gt;Barcelona/O&lt;span class="o"&gt;=&lt;/span&gt;LinuxSysadmin/CN&lt;span class="o"&gt;=&lt;/span&gt;private.linuxsysadmin.tk
Getting CA Private Key
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; ca.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Si la clave está protegida por una passphrase, se va a necesitar introducirla cada vez que se quiera levantar el servidor web. Nos lo podemos ahorrar con unos simples comandos, que dejará la clave como insegura.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mv server.key server.key.secure
root@server:~# openssl rsa -in server.key.secure -out server.key
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; server.key.secure:
writing RSA key
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Generar el certificado cliente&lt;/h2&gt;
&lt;p&gt;Generamos la clave para el certificado del cliente.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl genrsa -des3 -out client.key &lt;span class="m"&gt;1024&lt;/span&gt;
Generating RSA private key, &lt;span class="m"&gt;1024&lt;/span&gt; bit long modulus
...............................++++++
.....................++++++
e is &lt;span class="m"&gt;65537&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;0x10001&lt;span class="o"&gt;)&lt;/span&gt;
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; client.key:
Verifying - Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; client.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;El siguiente paso consiste en generar una petición de certificado, que posteriormente haremos firmar. El campo &lt;strong&gt;CN&lt;/strong&gt; puede ser recogido por el servidor web y trasladado mediante cabeceras a un hipotético &lt;em&gt;backend&lt;/em&gt;, en caso de hacer un &lt;em&gt;proxy_pass&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl req -new -key client.key -out client.csr -subj &lt;span class="s2"&gt;&amp;quot;/C=ES/ST=Spain/L=Barcelona/O=LinuxSysadmin/CN=Gerard Monells&amp;quot;&lt;/span&gt;
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; client.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Firmamos nuestra petición de certificado con la clave de la CA, obteniendo el certificado final.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl x509 -req -days &lt;span class="m"&gt;365&lt;/span&gt; -in client.csr -CA ca.crt -CAkey ca.key -set_serial &lt;span class="m"&gt;91&lt;/span&gt; -out client.crt
Signature ok
&lt;span class="nv"&gt;subject&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/C&lt;span class="o"&gt;=&lt;/span&gt;ES/ST&lt;span class="o"&gt;=&lt;/span&gt;Spain/L&lt;span class="o"&gt;=&lt;/span&gt;Barcelona/O&lt;span class="o"&gt;=&lt;/span&gt;LinuxSysadmin/CN&lt;span class="o"&gt;=&lt;/span&gt;Gerard Monells
Getting CA Private Key
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; ca.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora queda empaquetar la clave y el certificado en un fichero &lt;em&gt;client.p12&lt;/em&gt; que pueda ser importado en un navegador web.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl pkcs12 -export -in client.crt -inkey client.key -out client.p12 -name &lt;span class="s2"&gt;&amp;quot;LinuxSysadmin&amp;quot;&lt;/span&gt;
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; client.key:
Enter Export Password:
Verifying - Enter Export Password:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Montando el dominio web&lt;/h2&gt;
&lt;p&gt;Además de necesitar el certificado y la clave servidor, es necesario que el servidor web conozca el certificado de la CA para que pueda verificar el servidor cliente que nos ofrezca el navegador.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cp server.key /etc/ssl/private/
root@server:~# cp server.crt /etc/ssl/certs/
root@server:~# cp ca.crt /etc/ssl/certs/
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Así quedarían los certificados una vez en su sitio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# tree /etc/ssl/
/etc/ssl/
├── certs
│   ├── ca.crt
│   └── server.crt
├── openssl.cnf
└── private
    └── server.key

&lt;span class="m"&gt;2&lt;/span&gt; directories, &lt;span class="m"&gt;4&lt;/span&gt; files
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a poner un fichero de configuración en &lt;strong&gt;nginx&lt;/strong&gt;, que va a escuchar por el puerto 443 y con &lt;strong&gt;SSL&lt;/strong&gt; habilitado. Indicamos también donde están los ficheros que servirá el &lt;strong&gt;nginx&lt;/strong&gt;, la localización de los certificados y la necesidad de verificar al cliente mediante certificado contra el certificado de la CA.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cat /etc/nginx/sites-enabled/private.linuxsysadmin.tk
server &lt;span class="o"&gt;{&lt;/span&gt;
    listen                      &lt;span class="m"&gt;443&lt;/span&gt; ssl&lt;span class="p"&gt;;&lt;/span&gt;
    server_name                 private.linuxsysadmin.tk&lt;span class="p"&gt;;&lt;/span&gt;
    root                        /www&lt;span class="p"&gt;;&lt;/span&gt;

    ssl_certificate             /etc/ssl/certs/server.crt&lt;span class="p"&gt;;&lt;/span&gt;
    ssl_certificate_key         /etc/ssl/private/server.key&lt;span class="p"&gt;;&lt;/span&gt;
    ssl_client_certificate      /etc/ssl/certs/ca.crt&lt;span class="p"&gt;;&lt;/span&gt;
    ssl_verify_client           on&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Podemos verificar que la sintaxis de la configuración es correcta usando el binario de &lt;strong&gt;nginx&lt;/strong&gt; con el parámetro adecuado.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# nginx -t
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf &lt;span class="nb"&gt;test&lt;/span&gt; is successful
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y sabiendo que es correcto, reiniciamos el servidor web.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# service nginx restart
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Comprobación de funcionamiento&lt;/h2&gt;
&lt;p&gt;Como hemos indicado en la configuración en el &lt;em&gt;document root&lt;/em&gt;, vamos a servir el contenido que se encuentra en &lt;em&gt;/www&lt;/em&gt;. Empezaremos poniendo algún contenido en él.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mkdir /www
root@server:~# &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Private area&amp;quot;&lt;/span&gt; &amp;gt; /www/index.html
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora apuntemos el navegador a la &lt;strong&gt;URL&lt;/strong&gt; del servidor web. Debemos aceptar el certificado autofirmado, puesto que no viene firmado por ninguna autoridad certificadora conocida, por ejemplo, &lt;strong&gt;VeriSign&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Aún así, como no hemos presentado el certificado cliente, el servidor web nos impide el acceso, con una respuesta &lt;strong&gt;HTTP 400&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="2 way SSL access denied" src="https://www.linuxsysadmin.ml/images/2-way-ssl-access-denied.png"&gt;&lt;/p&gt;
&lt;p&gt;Ahora debemos importar el certificado &lt;em&gt;client.p12&lt;/em&gt; en el navegador web. En el caso concreto de &lt;strong&gt;Google Chrome&lt;/strong&gt;, se hace desde el siguiente menú:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Menu &amp;rarr; Settings &amp;rarr; Show advanced settings &amp;rarr; HTTPS/SSL &amp;rarr; Manage certificates &amp;rarr; Your certificates &amp;rarr; Import &amp;rarr; client.p12&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Y ya podemos acceder a nuestro contenido protegido, previa selección del certificado a usar.&lt;/p&gt;
&lt;p&gt;&lt;img alt="2 way SSL certificate" src="https://www.linuxsysadmin.ml/images/2-way-ssl-certificate.png"&gt;&lt;/p&gt;
&lt;p&gt;Y con esto ya tenemos montada la autenticación cliente mediante certificados.&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="jessie"></category><category term="nginx"></category><category term="2 way ssl"></category><category term="ssl"></category><category term="https"></category><category term="certificado"></category></entry><entry><title>Restringiendo accesos web mediante autenticación básica</title><link href="https://www.linuxsysadmin.ml/2016/02/restringiendo-accesos-web-mediante-autenticacion-basica.html" rel="alternate"></link><published>2016-02-08T08:30:00+01:00</published><updated>2016-02-08T08:30:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-02-08:/2016/02/restringiendo-accesos-web-mediante-autenticacion-basica.html</id><summary type="html">&lt;p&gt;Algunas veces nos encontramos con la necesidad de restringir el acceso a algunos recursos web. Normalmente se suele implementar algún sistema de &lt;em&gt;login&lt;/em&gt;, &lt;em&gt;cookies&lt;/em&gt; o &lt;em&gt;sesiones&lt;/em&gt;; no obstante, esta opción no siempre nos es posible, y tenemos que proteger esos recursos usando los mecanismos que nos ofrezca el servidor web …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Algunas veces nos encontramos con la necesidad de restringir el acceso a algunos recursos web. Normalmente se suele implementar algún sistema de &lt;em&gt;login&lt;/em&gt;, &lt;em&gt;cookies&lt;/em&gt; o &lt;em&gt;sesiones&lt;/em&gt;; no obstante, esta opción no siempre nos es posible, y tenemos que proteger esos recursos usando los mecanismos que nos ofrezca el servidor web.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ATENCIÓN&lt;/strong&gt;: Este método es bastante simple, y se puede descodificar lo que manda el cliente; por eso se recomienda encarecidamente usar &lt;strong&gt;SSL&lt;/strong&gt;, mediante el uso de &lt;strong&gt;HTTPS&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Empezaremos instalando el servidor web y la herramienta de generación de certificados para usar con &lt;strong&gt;SSL&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# apt-get install nginx-light openssl
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias
Leyendo la información de estado... Hecho
Se instalarán los siguientes paquetes extras:
  nginx-common
Paquetes sugeridos:
  fcgiwrap nginx-doc ssl-cert ca-certificates
Se instalarán los siguientes paquetes NUEVOS:
  nginx-common nginx-light openssl
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;3&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;1&lt;/span&gt;.126 kB de archivos.
Se utilizarán &lt;span class="m"&gt;2&lt;/span&gt;.148 kB de espacio de disco adicional después de esta operación.
¿Desea continuar? &lt;span class="o"&gt;[&lt;/span&gt;S/n&lt;span class="o"&gt;]&lt;/span&gt; s
...
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Generar el certificado y la clave de la CA&lt;/h2&gt;
&lt;p&gt;Empezaremos por generar la clave de la CA, que va a servir para firmar el certificado que pondremos en el servidor.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl genrsa -des3 -out ca.key &lt;span class="m"&gt;4096&lt;/span&gt;
Generating RSA private key, &lt;span class="m"&gt;4096&lt;/span&gt; bit long modulus
..................................++
.....................................................................++
e is &lt;span class="m"&gt;65537&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;0x10001&lt;span class="o"&gt;)&lt;/span&gt;
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; ca.key:
Verifying - Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; ca.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora generamos el certificado de la CA. Lo generamos directamente firmado en un solo paso.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl req -new -x509 -days &lt;span class="m"&gt;365&lt;/span&gt; -key ca.key -out ca.crt -subj &lt;span class="s2"&gt;&amp;quot;/C=ES/ST=Spain/L=Barcelona/O=LinuxSysadmin&amp;quot;&lt;/span&gt;
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; ca.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Generar el certificado para el servidor web&lt;/h2&gt;
&lt;p&gt;Generamos la clave para el certificado del servidor web.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl genrsa -des3 -out server.key &lt;span class="m"&gt;4096&lt;/span&gt;
Generating RSA private key, &lt;span class="m"&gt;4096&lt;/span&gt; bit long modulus
.....................................................................++
......................................................................................................................................................................++
e is &lt;span class="m"&gt;65537&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;0x10001&lt;span class="o"&gt;)&lt;/span&gt;
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; server.key:
Verifying - Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; server.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora creamos un certificado para el servidor web.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl req -new -key server.key -out server.csr -subj &lt;span class="s2"&gt;&amp;quot;/C=ES/ST=Spain/L=Barcelona/O=LinuxSysadmin/CN=private.linuxsysadmin.tk&amp;quot;&lt;/span&gt;
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; server.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y lo firmamos con la clave y el certificado de la CA.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# openssl x509 -req -days &lt;span class="m"&gt;365&lt;/span&gt; -in server.csr -CA ca.crt -CAkey ca.key -set_serial &lt;span class="m"&gt;01&lt;/span&gt; -out server.crt
Signature ok
&lt;span class="nv"&gt;subject&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/C&lt;span class="o"&gt;=&lt;/span&gt;ES/ST&lt;span class="o"&gt;=&lt;/span&gt;Spain/L&lt;span class="o"&gt;=&lt;/span&gt;Barcelona/O&lt;span class="o"&gt;=&lt;/span&gt;LinuxSysadmin/CN&lt;span class="o"&gt;=&lt;/span&gt;private.linuxsysadmin.tk
Getting CA Private Key
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; ca.key:
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: Si la clave está protegida por una &lt;em&gt;passphrase&lt;/em&gt;, se va a necesitar introducirla cada vez que se quiera levantar el servidor web. Nos lo podemos ahorrar con unos simples comandos, que dejará la clave como insegura.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mv server.key server.key.secure
root@server:~# openssl rsa -in server.key.secure -out server.key
Enter pass phrase &lt;span class="k"&gt;for&lt;/span&gt; server.key.secure:
writing RSA key
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Montando el dominio web&lt;/h2&gt;
&lt;p&gt;Para habilitar &lt;strong&gt;SSL&lt;/strong&gt; en un dominio, necesitamos la clave y el certificado del servidor, así que vamos a ponerlos en una carpeta pensado para tal efecto.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cp server.key /etc/ssl/private/
root@server:~# cp server.crt /etc/ssl/certs/
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Así quedarían los certificados una vez en su sitio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# tree /etc/ssl/
/etc/ssl/
├── certs
│   └── server.crt
├── openssl.cnf
└── private
    └── server.key

&lt;span class="m"&gt;2&lt;/span&gt; directories, &lt;span class="m"&gt;3&lt;/span&gt; files
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a poner un fichero de configuración en &lt;strong&gt;nginx&lt;/strong&gt;, que va a escuchar por el puerto 443 y con &lt;strong&gt;SSL&lt;/strong&gt; habilitado. Indicamos también donde están los ficheros que servirá el &lt;strong&gt;nginx&lt;/strong&gt;, la localización de los certificados y activamos la autenticación básica.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# cat /etc/nginx/sites-enabled/private.linuxsysadmin.tk
server &lt;span class="o"&gt;{&lt;/span&gt;
        listen &lt;span class="m"&gt;443&lt;/span&gt; ssl&lt;span class="p"&gt;;&lt;/span&gt;
        server_name private.linuxsysadmin.tk&lt;span class="p"&gt;;&lt;/span&gt;
        root /www&lt;span class="p"&gt;;&lt;/span&gt;

        ssl_certificate /etc/ssl/certs/server.crt&lt;span class="p"&gt;;&lt;/span&gt;
        ssl_certificate_key /etc/ssl/private/server.key&lt;span class="p"&gt;;&lt;/span&gt;

        auth_basic &lt;span class="s2"&gt;&amp;quot;Admin Area&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        auth_basic_user_file /etc/nginx/auth/private&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora tenemos que crear un fichero tipo &lt;em&gt;.htpasswd&lt;/em&gt; como los de &lt;strong&gt;apache&lt;/strong&gt;. Crearemos primero la carpeta en donde lo vamos a dejar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mkdir /etc/nginx/auth
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En la carpeta creada pondremos un fichero llamado &lt;em&gt;private&lt;/em&gt; con un formato idéntico a los &lt;em&gt;.htpasswd&lt;/em&gt; de &lt;strong&gt;apache&lt;/strong&gt;. Aquí podríamos usar las herramientas de &lt;strong&gt;apache-utils&lt;/strong&gt;, pero de momento nos conformaremos con crearlo con &lt;strong&gt;openssl&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;admin:&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;openssl passwd -crypt s3cr3t&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; /etc/nginx/auth/private
root@server:~# cat /etc/nginx/auth/private
admin:y6xasR0LI8mbg
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finalmente reiniciamos el servidor web para que aplique los cambios en la configuración.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# service nginx restart
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Comprobación de funcionamiento&lt;/h2&gt;
&lt;p&gt;Es importante que nos acordemos de crear nuestro &lt;em&gt;document root&lt;/em&gt; con algún contenido.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mkdir /www
root@server:~# &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Private area&amp;quot;&lt;/span&gt; &amp;gt; /www/index.html
root@server:~#
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Si apuntamos un navegador al dominio configurado, y tras aceptar nuestro certificado autofirmado como excepción, deberíamos ver que se nos piden las credenciales en una ventana emergente.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Autenticación básica: credenciales" src="https://www.linuxsysadmin.ml/images/autenticacion-basica-credenciales.png"&gt;&lt;/p&gt;
&lt;p&gt;Y con eso tenemos nuestro contenido protegido de los curiosos.&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="jessie"></category><category term="nginx"></category><category term="autenticacion basica"></category><category term="htpasswd"></category><category term="ssl"></category><category term="https"></category><category term="certificado"></category></entry><entry><title>Un proxy DNS con dnsmasq</title><link href="https://www.linuxsysadmin.ml/2016/02/un-proxy-dns-con-dnsmasq.html" rel="alternate"></link><published>2016-02-01T08:30:00+01:00</published><updated>2016-02-01T08:30:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-02-01:/2016/02/un-proxy-dns-con-dnsmasq.html</id><summary type="html">&lt;p&gt;A veces nos puede interesar disponer de una servidor &lt;strong&gt;DNS&lt;/strong&gt; para nombrar las máquinas de nuestra red privada, sin la complejidad de &lt;strong&gt;BIND&lt;/strong&gt;. Otras, queremos acelerar el acceso a internet desde nuestra red; es interesante ver el tiempo que se pierde en la resolución &lt;strong&gt;DNS&lt;/strong&gt;. Para eso disponemos de &lt;strong&gt;dnsmasq …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;A veces nos puede interesar disponer de una servidor &lt;strong&gt;DNS&lt;/strong&gt; para nombrar las máquinas de nuestra red privada, sin la complejidad de &lt;strong&gt;BIND&lt;/strong&gt;. Otras, queremos acelerar el acceso a internet desde nuestra red; es interesante ver el tiempo que se pierde en la resolución &lt;strong&gt;DNS&lt;/strong&gt;. Para eso disponemos de &lt;strong&gt;dnsmasq&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;El servicio &lt;strong&gt;dnsmasq&lt;/strong&gt; proporciona servicios como caché &lt;strong&gt;DNS&lt;/strong&gt; y como servidor &lt;strong&gt;DHCP&lt;/strong&gt;. Se trata de un &lt;em&gt;proxy&lt;/em&gt; &lt;strong&gt;DNS&lt;/strong&gt; que va a dirigir las consultas &lt;strong&gt;DNS&lt;/strong&gt; contra el servidor configurado en el &lt;em&gt;proxy&lt;/em&gt;, guardando una copia en &lt;em&gt;caché&lt;/em&gt; para agilizar futuras consultas.&lt;/p&gt;
&lt;p&gt;Es muy fácil de configurar y es bastante ligero. Se considera ideal para redes pequeñas con menos de 50 ordenadores.&lt;/p&gt;
&lt;p&gt;En mi caso, resultó muy útil para solucionar el problema de &lt;strong&gt;DNS&lt;/strong&gt; que me planteaba una red &lt;strong&gt;Virtualbox&lt;/strong&gt; &lt;em&gt;solo anfitrión&lt;/em&gt;, en donde se escondían mis máquinas virtuales con dirección IP estática. Resulta que me muevo entre varias zonas de trabajo, y que no hay ningún servidor &lt;strong&gt;DNS&lt;/strong&gt; accesible desde todas; ir cambiando los &lt;strong&gt;DNS&lt;/strong&gt; de todas las máquinas era trabajoso.&lt;/p&gt;
&lt;p&gt;Con este problema, puse &lt;strong&gt;dnsmasq&lt;/strong&gt; en mi anfitrión (que usaba &lt;strong&gt;DHCP&lt;/strong&gt; y recibía el &lt;strong&gt;DNS&lt;/strong&gt; automáticamente), y configuré todas las máquinas para que usaran el anfitrión como servidor &lt;strong&gt;DNS&lt;/strong&gt;; nunca mas tuve que configurarlos.&lt;/p&gt;
&lt;h2&gt;Instalación&lt;/h2&gt;
&lt;p&gt;La instalación en una máquina derivada de &lt;em&gt;Debian&lt;/em&gt; es muy simple; está en los repositorios oficiales.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@proxy:~# apt-get install dnsmasq
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias       
Leyendo la información de estado... Hecho
Se instalarán los siguientes paquetes extras:
  dns-root-data dnsmasq-base libnetfilter-conntrack3
Paquetes sugeridos:
  resolvconf
Se instalarán los siguientes paquetes NUEVOS:
  dns-root-data dnsmasq dnsmasq-base libnetfilter-conntrack3
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;4&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;488&lt;/span&gt; kB de archivos.
Se utilizarán &lt;span class="m"&gt;1&lt;/span&gt;.170 kB de espacio de disco adicional después de esta operación.
¿Desea continuar? &lt;span class="o"&gt;[&lt;/span&gt;S/n&lt;span class="o"&gt;]&lt;/span&gt; s
...
root@proxy:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En caso de querer modificar la configuración, se debe editar el fichero &lt;em&gt;/etc/dnsmasq.conf&lt;/em&gt;, y luego reiniciar el servicio &lt;em&gt;dnsmasq&lt;/em&gt;. En este mismo fichero se puede configurar el servicio &lt;strong&gt;DHCP&lt;/strong&gt; (directiva &lt;em&gt;dhcp-range&lt;/em&gt;), el servidor de nombres a dar al resto (la misma máquina de &lt;em&gt;dnsmasq&lt;/em&gt;, por defecto), el servidor &lt;strong&gt;NTP&lt;/strong&gt; o el &lt;em&gt;gateway&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;En mi caso, no vi necesario activar estos servicios, así que el fichero de configuración no se vio modificada. Así pues, con esto basta.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRUCO&lt;/strong&gt;: En caso de querer resolver localmente, &lt;em&gt;dnsmasq&lt;/em&gt; sirve los nombres alojados en &lt;em&gt;/etc/hosts&lt;/em&gt;, a menos que se indique lo contrario en la configuración. Basta con modificar ese fichero.&lt;/p&gt;
&lt;h2&gt;Comprobación y uso&lt;/h2&gt;
&lt;p&gt;Para comprobar que funciona, vamos a poner otra máquina, configurada para usar el nuevo servidor &lt;strong&gt;DNS&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# cat /etc/resolv.conf 
nameserver &lt;span class="m"&gt;192&lt;/span&gt;.168.56.1
root@client:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;En principio basta con comprobar que resuelve el nombre de una petición cualquiera, por ejemplo, con un &lt;strong&gt;ping&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Sin embargo, podemos apreciar la mejora de la &lt;em&gt;caché&lt;/em&gt; mediante una herramienta mas avanzada de resolución &lt;strong&gt;DNS&lt;/strong&gt;, por ejemplo, con &lt;strong&gt;dig&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# dig www.linuxsysadmin.tk &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="s2"&gt;&amp;quot;Query time&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;;;&lt;/span&gt; Query time: &lt;span class="m"&gt;118&lt;/span&gt; msec
root@client:~# dig www.linuxsysadmin.tk &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="s2"&gt;&amp;quot;Query time&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;;;&lt;/span&gt; Query time: &lt;span class="m"&gt;5&lt;/span&gt; msec
root@client:~# dig www.linuxsysadmin.tk &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="s2"&gt;&amp;quot;Query time&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;;;&lt;/span&gt; Query time: &lt;span class="m"&gt;4&lt;/span&gt; msec
root@client:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto hemos cumplido; tenemos un &lt;em&gt;proxy caché&lt;/em&gt; &lt;strong&gt;DNS&lt;/strong&gt;, que nos agiliza las peticiones, nos resuelve localmente y nos evita ir cambiando el &lt;strong&gt;DNS&lt;/strong&gt; cada vez que nos movemos de zona de trabajo.&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="jessie"></category><category term="dnsmasq"></category><category term="cache"></category><category term="dns"></category><category term="dhcp"></category></entry><entry><title>Un repositorio de Debian con reprepro</title><link href="https://www.linuxsysadmin.ml/2016/01/un-repositorio-de-debian-con-reprepro.html" rel="alternate"></link><published>2016-01-11T08:00:00+01:00</published><updated>2016-01-11T08:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2016-01-11:/2016/01/un-repositorio-de-debian-con-reprepro.html</id><summary type="html">&lt;p&gt;Una de las grandes facilidades que nos ofrece una distribución de Linux es su sistema de gestor de paquetes. Los paquetes oficiales nos simplifican la instalación y mantenimiento de paquetes; sin embargo, podemos sacar provecho del sistema de paquetes para uso personal, para automatizar instalaciones y actualizaciones que queramos hacer …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Una de las grandes facilidades que nos ofrece una distribución de Linux es su sistema de gestor de paquetes. Los paquetes oficiales nos simplifican la instalación y mantenimiento de paquetes; sin embargo, podemos sacar provecho del sistema de paquetes para uso personal, para automatizar instalaciones y actualizaciones que queramos hacer.&lt;/p&gt;
&lt;p&gt;En este artículo vamos a crear un repositorio en el que podemos poner paquetes, sean sacados del repositorio oficial (para hacer de caché), o sean paquetes creados por nosotros con aplicativos propios o empaquetados a partir de paquetes no libres.&lt;/p&gt;
&lt;p&gt;Para hacerlo, necesitamos una máquina en donde pondremos el repositorio, y a efectos de demostración, una máquina en donde instalaremos paquetes de dicho repositorio. En este caso, usaremos como &lt;em&gt;LXC&lt;/em&gt; tecnología para crear las máquina virtuales.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# lxc-ls -f
NAME        STATE    IPV4      IPV6  AUTOSTART  
----------------------------------------------
client      RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.3  -     YES        
repository  RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2  -     YES        
root@lxc:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Montando el repositorio&lt;/h2&gt;
&lt;p&gt;Un repositorio &lt;em&gt;Debian&lt;/em&gt; no es mas que un servidor web sirviendo una estructura de ficheros con una forma concreta, que vamos a crear con &lt;strong&gt;reprepro&lt;/strong&gt; y vamos a servir con &lt;strong&gt;nginx&lt;/strong&gt;. Así pues, los instalamos.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@repository:~# apt-get install reprepro nginx-light
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias       
Leyendo la información de estado... Hecho
...
Se instalarán los siguientes paquetes NUEVOS:
  ca-certificates gnupg-agent gnupg2 libarchive13 libassuan0 libcurl3-gnutls libffi6 libgmp10 libgnutls-deb0-28 libgpgme11 libhogweed2 libidn11 libksba8 libldap-2.4-2
  liblzo2-2 libnettle4 libp11-kit0 libpth20 librtmp1 libsasl2-2 libsasl2-modules libsasl2-modules-db libssh2-1 libtasn1-6 libxml2 nginx-common nginx-light openssl
  pinentry-curses reprepro sgml-base xml-core
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;32&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;7&lt;/span&gt;.645 kB de archivos.
Se utilizarán &lt;span class="m"&gt;21&lt;/span&gt;,2 MB de espacio de disco adicional después de esta operación.
¿Desea continuar? &lt;span class="o"&gt;[&lt;/span&gt;S/n&lt;span class="o"&gt;]&lt;/span&gt; s
...
root@repository:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Un repositorio necesita una clave &lt;strong&gt;gpg&lt;/strong&gt; para firmar los paquetes que sirve; aunque de eso se encarga &lt;strong&gt;reprepro&lt;/strong&gt;, tenemos que generarla:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@repository:~# gpg --gen-key
gpg &lt;span class="o"&gt;(&lt;/span&gt;GnuPG&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;.4.18&lt;span class="p"&gt;;&lt;/span&gt; Copyright &lt;span class="o"&gt;(&lt;/span&gt;C&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="m"&gt;2014&lt;/span&gt; Free Software Foundation, Inc.
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
...
gpg: /root/.gnupg/trustdb.gpg: se ha creado base de datos de confianza
gpg: clave C1B88DF7 marcada como de confianza absoluta
claves pública y secreta creadas y firmadas.
...
root@repository:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora podemos ver que las claves se han creado y podemos anotar su identificador para continuar con el procedimiento.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@repository:~# gpg --list-keys
/root/.gnupg/pubring.gpg
------------------------
pub   2048R/C1B88DF7 &lt;span class="m"&gt;2016&lt;/span&gt;-01-07
uid                  Gerard Monells &amp;lt;gerard.monells@gmail.com&amp;gt;
sub   2048R/5C5B84E3 &lt;span class="m"&gt;2016&lt;/span&gt;-01-07

root@repository:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a crear el repositorio en &lt;em&gt;/opt/repo/&lt;/em&gt;, con una carpeta &lt;em&gt;public&lt;/em&gt; que es lo que vamos a servir con &lt;strong&gt;nginx&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@repository:~# mkdir -p /opt/repo/&lt;span class="o"&gt;{&lt;/span&gt;conf,public&lt;span class="o"&gt;}&lt;/span&gt;
root@repository:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Por comodidad, vamos a trabajar en la carpeta base del repositorio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@repository:~# &lt;span class="nb"&gt;cd&lt;/span&gt; /opt/repo
root@repository:/opt/repo# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Un repositorio hecho con &lt;strong&gt;reprepro&lt;/strong&gt; se declara mediante un fichero de configuración, que vamos a crear en la carpeta &lt;em&gt;conf&lt;/em&gt;, declarando el nombre del repositorio, las arquitecturas y la clave con la que se firman los paquetes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@repository:/opt/repo# cat conf/distributions 
Codename: linuxsysadmin
Components: main
Architectures: i386
SignWith: C1B88DF7
root@repository:/opt/repo# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a poner la parte pública de nuestra clave &lt;strong&gt;gpg&lt;/strong&gt; en la raíz del servidor web, para que los clientes puedan agregarla a su almacén de claves, para usar sin problemas los paquetes de nuestro repositorio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@repository:/opt/repo# gpg -a --export C1B88DF7 &amp;gt; /opt/repo/public/key.gpg
root@repository:/opt/repo# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora vamos a poner una configuración a &lt;strong&gt;nginx&lt;/strong&gt; que nos permita servir la carpeta pública en el puerto web.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@repository:/opt/repo# cat /etc/nginx/sites-enabled/repository
server &lt;span class="o"&gt;{&lt;/span&gt;
    server_name localhost&lt;span class="p"&gt;;&lt;/span&gt;
    root /opt/repo/public&lt;span class="p"&gt;;&lt;/span&gt;
    autoindex on&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
root@repository:/opt/repo# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Recargamos o reiniciamos el servicio &lt;strong&gt;nginx&lt;/strong&gt; para que la configuración surta efecto:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@repository:/opt/repo# service nginx restart
root@repository:/opt/repo# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Añadiendo paquetes al repositorio&lt;/h2&gt;
&lt;p&gt;Añadir un paquete a nuestro repositorio es tan fácil como invocar el comando &lt;em&gt;reprepro&lt;/em&gt;, con la opción &lt;em&gt;includedeb&lt;/em&gt; del paquete, en alguna carpeta de nuestra máquina. El resto son opciones que indican donde están las carpetas del repositorio.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: Si se pone un paquete empaquetado por nosotros, es importante que su fichero &lt;em&gt;control&lt;/em&gt; incluya las directivas &lt;em&gt;Section&lt;/em&gt; y &lt;em&gt;Priority&lt;/em&gt;, normalmente solo recomendadas, pero necesarias para &lt;strong&gt;reprepro&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Por ejemplo, podemos usar el paquete de un &lt;a href="https://www.linuxsysadmin.ml/2015/12/empaquetando-ficheros-punto-deb.html"&gt;artículo anterior&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@repository:/opt/repo# reprepro --distdir ./public/dists --outdir ./public includedeb linuxsysadmin /root/welcome_1.0-1_all.deb 
Exporting indices...
root@repository:/opt/repo# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: Puede que el comando falle si no se ha montado el sistema de ficheros &lt;em&gt;/dev/pts&lt;/em&gt;, especialmente en un entorno tipo &lt;strong&gt;chroot&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Usando el repositorio&lt;/h2&gt;
&lt;p&gt;Cambiamos de máquina; ahora vamos a la máquina que vaya a usar el repositorio y vamos a configurar el repositorio nuevo.&lt;/p&gt;
&lt;p&gt;Lo primero es declarar la &lt;strong&gt;source&lt;/strong&gt; de nuestro repositorio, declarando la dirección web del repositorio, el nombre del repositorio y el componente.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# cat /etc/apt/sources.list.d/linuxsysadmin.list 
deb http://10.0.0.2/ linuxsysadmin main
root@client:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora nos descargamos la clave pública del repositorio y la añadimos al almacén de claves de &lt;strong&gt;apt&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# wget -qO- http://10.0.0.2/key.gpg &lt;span class="p"&gt;|&lt;/span&gt; apt-key add -
OK
root@client:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto ya tenemos el repositorio habilitado. A partir de aquí su uso es el mismo que el de cualquier otro repositorio. Hacemos un &lt;em&gt;apt-get update&lt;/em&gt; para descargar la lista de paquetes del repositorio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# apt-get update
Des:1 http://10.0.0.2 linuxsysadmin InRelease &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.340 B&lt;span class="o"&gt;]&lt;/span&gt;
...
Des:2 http://10.0.0.2 linuxsysadmin/main i386 Packages &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;333&lt;/span&gt; B&lt;span class="o"&gt;]&lt;/span&gt;
...
Descargados &lt;span class="m"&gt;2&lt;/span&gt;.040 B en 6s &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;330&lt;/span&gt; B/s&lt;span class="o"&gt;)&lt;/span&gt;                                                                                                                                    
Leyendo lista de paquetes... Hecho
root@client:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A partir de aquí, y sabiendo nuestro sistema los paquetes de los que dispone el nuevo repositorio, podemos buscar los paquetes que hay en él.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# apt-cache search welcome &lt;span class="p"&gt;|&lt;/span&gt; grep ^welcome
welcome2l - Linux ANSI boot logo
welcome - A fancy shell script
root@client:~# apt-cache show welcome
Package: welcome
Version: &lt;span class="m"&gt;1&lt;/span&gt;.0-1
Architecture: all
Maintainer: Linux Sysadmin
Priority: optional
Section: main
Filename: pool/main/w/welcome/welcome_1.0-1_all.deb
Size: &lt;span class="m"&gt;786&lt;/span&gt;
SHA256: 2e701f7fbc090230fb7abc06597fbe5b4e9e70dcc553e749e69793a745b032f2
SHA1: 41351d1d2135bcee09e1fa3bade984ece9f23caf
MD5sum: 574fab58b3c871184047c40d0e732b35
Description: A fancy shell script
 To demonstrate how to package a .deb file
Description-md5: ed73975a1e7c5f0422fef1f624586821
Depends: bash, coreutils

root@client:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Visto que el paquete está disponible, podemos instalarlo, usando &lt;em&gt;apt-get&lt;/em&gt; o cualquier otro frontal, gráfico o no.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# apt-get install welcome
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias       
Leyendo la información de estado... Hecho
Se instalarán los siguientes paquetes NUEVOS:
  welcome
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;1&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;786&lt;/span&gt; B de archivos.
Se utilizarán &lt;span class="m"&gt;0&lt;/span&gt; B de espacio de disco adicional después de esta operación.
Des:1 http://10.0.0.2/ linuxsysadmin/main welcome all &lt;span class="m"&gt;1&lt;/span&gt;.0-1 &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;786&lt;/span&gt; B&lt;span class="o"&gt;]&lt;/span&gt;
Descargados &lt;span class="m"&gt;786&lt;/span&gt; B en 0s &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;37&lt;/span&gt;,4 kB/s&lt;span class="o"&gt;)&lt;/span&gt;
debconf: se retrasa la configuración de los paquetes, ya que «apt-utils» no está instalado
Seleccionando el paquete welcome previamente no seleccionado.
&lt;span class="o"&gt;(&lt;/span&gt;Leyendo la base de datos ... &lt;span class="m"&gt;10434&lt;/span&gt; ficheros o directorios instalados actualmente.&lt;span class="o"&gt;)&lt;/span&gt;
Preparando para desempaquetar .../archives/welcome_1.0-1_all.deb ...
Desempaquetando welcome &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.0-1&lt;span class="o"&gt;)&lt;/span&gt; ...
Configurando welcome &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.0-1&lt;span class="o"&gt;)&lt;/span&gt; ...
root@client:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Tal como lo esperábamos, el comando &lt;em&gt;welcome&lt;/em&gt; está instalado y funciona como esperábamos:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# which welcome
/usr/bin/welcome
root@client:~# welcome
Hello world!
root@client:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto tenemos nuestro repositorio funcional.&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="repositorio"></category><category term="reprepro"></category><category term="nginx"></category><category term="gpg"></category><category term="apt"></category></entry><entry><title>Utilizando apt-cacher-ng para agilizar la instalación de paquetes</title><link href="https://www.linuxsysadmin.ml/2015/12/utilizando-apt-cacher-ng-para-agilizar-la-instalacion-de-paquetes.html" rel="alternate"></link><published>2015-12-21T10:00:00+01:00</published><updated>2015-12-21T10:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2015-12-21:/2015/12/utilizando-apt-cacher-ng-para-agilizar-la-instalacion-de-paquetes.html</id><summary type="html">&lt;p&gt;Hace tiempo veo que tras usar muchas maquinas virtuales &lt;em&gt;Debian&lt;/em&gt; para el uso diario y para las demostraciones de este blog, el ancho de banda usado para bajar los paquetes se dispara. La mayoría de veces se trata de los mismos paquetes, para instalar las mismas aplicaciones, servicios o actualizaciones …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Hace tiempo veo que tras usar muchas maquinas virtuales &lt;em&gt;Debian&lt;/em&gt; para el uso diario y para las demostraciones de este blog, el ancho de banda usado para bajar los paquetes se dispara. La mayoría de veces se trata de los mismos paquetes, para instalar las mismas aplicaciones, servicios o actualizaciones.&lt;/p&gt;
&lt;p&gt;En el artículo de hoy, voy a enseñar como usar un &lt;em&gt;proxy&lt;/em&gt; con una &lt;em&gt;caché&lt;/em&gt; para &lt;em&gt;apt-get&lt;/em&gt;, llamado &lt;strong&gt;apt-cacher-ng&lt;/strong&gt;, de forma que los paquetes son descargados por la primera máquina que los pida, guardados en un servidor local y aprovechados por el resto de máquinas.&lt;/p&gt;
&lt;h2&gt;Preparación de las máquinas&lt;/h2&gt;
&lt;p&gt;Partimos de la máquina habitual, llamada &lt;strong&gt;aptcacher&lt;/strong&gt;, siendo esta un contenedor LXC con una &lt;em&gt;Debian Jessie&lt;/em&gt; básica, aunque esto se podría haber puesto en una &lt;em&gt;Ubuntu&lt;/em&gt; o cualquier otra distribución que funcione con paquetes &lt;em&gt;.deb&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Otras máquinas que vamos a usar son unas máquinas cliente en donde vamos a instalar paquetes cualesquiera para demostrar el funcionamiento, llamadas &lt;strong&gt;client1&lt;/strong&gt; y &lt;strong&gt;client2&lt;/strong&gt;; estos clientes están en la misma red que la máquina &lt;strong&gt;aptcacher&lt;/strong&gt; y tienen conectividad con ella por el puerto &lt;em&gt;TCP&lt;/em&gt; 3142.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@lxc:~# lxc-ls -f
NAME       STATE    IPV4      IPV6  AUTOSTART  
---------------------------------------------
aptcacher  RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2  -     YES        
client1    RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.3  -     YES        
client2    RUNNING  &lt;span class="m"&gt;10&lt;/span&gt;.0.0.4  -     YES        
root@lxc:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Empezamos instalando el servicio &lt;strong&gt;apt-cacher-ng&lt;/strong&gt; en la máquina servidor &lt;strong&gt;aptcacher&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@aptcacher:~# apt-get install apt-cacher-ng
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias... Hecho
...  
Se instalarán los siguientes paquetes NUEVOS:
  apt-cacher-ng ed
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;2&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;500&lt;/span&gt; kB de archivos.
Se utilizarán &lt;span class="m"&gt;1&lt;/span&gt;.168 kB de espacio de disco adicional después de esta operación.
¿Desea continuar? &lt;span class="o"&gt;[&lt;/span&gt;S/n&lt;span class="o"&gt;]&lt;/span&gt; s
...
root@aptcacher:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Las configuraciones que vienen por defecto son bastante adecuadas y no tuve que efectuar ningún cambio.&lt;/p&gt;
&lt;p&gt;Por otra parte, hay que configurar las máquinas que se quieran beneficiar de este servidor, añadiendo una línea de configuración en su &lt;strong&gt;apt-get&lt;/strong&gt;, por ejemplo, poniendo un fichero adicional en &lt;em&gt;/etc/apt/apt.conf.d/&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@aptcacher:~# cat /etc/apt/apt.conf.d/02proxy 
Acquire::http &lt;span class="o"&gt;{&lt;/span&gt; Proxy &lt;span class="s2"&gt;&amp;quot;http://10.0.0.2:3142&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
root@aptcacher:~# 

root@client1:~# cat /etc/apt/apt.conf.d/02proxy 
Acquire::http &lt;span class="o"&gt;{&lt;/span&gt; Proxy &lt;span class="s2"&gt;&amp;quot;http://10.0.0.2:3142&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
root@client1:~# 

root@client2:~# cat /etc/apt/apt.conf.d/02proxy 
Acquire::http &lt;span class="o"&gt;{&lt;/span&gt; Proxy &lt;span class="s2"&gt;&amp;quot;http://10.0.0.2:3142&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
root@client2:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esto queda montado todo el sistema.&lt;/p&gt;
&lt;h2&gt;Funcionamiento de la caché&lt;/h2&gt;
&lt;p&gt;El funcionamiento es muy simple: basta con instalar en un cliente un paquete, por ejemplo, &lt;em&gt;python&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client1:~# apt-get install python
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias... Hecho
...  
Se instalarán los siguientes paquetes NUEVOS:
  file libexpat1 libffi6 libmagic1 libpython-stdlib libpython2.7-minimal
  libpython2.7-stdlib libsqlite3-0 mime-support python python-minimal
  python2.7 python2.7-minimal
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;13&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;5&lt;/span&gt;.010 kB de archivos.
Se utilizarán &lt;span class="m"&gt;21&lt;/span&gt;,3 MB de espacio de disco adicional después de esta operación.
¿Desea continuar? &lt;span class="o"&gt;[&lt;/span&gt;S/n&lt;span class="o"&gt;]&lt;/span&gt; s
...
Descargados &lt;span class="m"&gt;5&lt;/span&gt;.010 kB en 15s &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;327&lt;/span&gt; kB/s&lt;span class="o"&gt;)&lt;/span&gt;                                        
...
root@client1:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Como estos paquetes no están en la &lt;em&gt;caché&lt;/em&gt; del servidor, se han descargado de internet en 15 segundos, de acuerdo a la velocidad de mi conexión de internet y de la velocidad de respuesta de los repositorios elegidos.&lt;/p&gt;
&lt;p&gt;Si revisamos la página de estadísticas de &lt;strong&gt;apt-cacher-ng&lt;/strong&gt;, disponible en &lt;em&gt;http://aptcacher:3142/acng-report.html&lt;/em&gt; podemos ver que se han descargado 4,78mb en 13 paquetes; todos son &lt;strong&gt;miss&lt;/strong&gt; de la cache, es decir, se han ido a buscar al repositorio oficial.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Estadísticas web de apt-cacher" src="https://www.linuxsysadmin.ml/images/apt-cacher-ng-1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Ahora vamos a instalar &lt;em&gt;python&lt;/em&gt; en otro de los clientes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client2:~# apt-get install python
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias... Hecho
...
Se instalarán los siguientes paquetes NUEVOS:
  file libexpat1 libffi6 libmagic1 libpython-stdlib libpython2.7-minimal
  libpython2.7-stdlib libsqlite3-0 mime-support python python-minimal
  python2.7 python2.7-minimal
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;13&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
Se necesita descargar &lt;span class="m"&gt;5&lt;/span&gt;.010 kB de archivos.
Se utilizarán &lt;span class="m"&gt;21&lt;/span&gt;,3 MB de espacio de disco adicional después de esta operación.
¿Desea continuar? &lt;span class="o"&gt;[&lt;/span&gt;S/n&lt;span class="o"&gt;]&lt;/span&gt; s
...
Descargados &lt;span class="m"&gt;5&lt;/span&gt;.010 kB en 1s &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;.902 kB/s&lt;span class="o"&gt;)&lt;/span&gt;
root@client2:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Hemos elegido el paquete &lt;em&gt;python&lt;/em&gt; para asegurar que ambas máquinas instalan lo mismo; como se puede ver, se ha descargado la misma cantidad de datos, pero en vez de los 15 segundos anteriores, ahora se ha tardado 1 segundo. Eso es porque los paquetes solicitados estaban en el &lt;em&gt;proxy&lt;/em&gt;, es decir, en el servidor &lt;strong&gt;aptcacher&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Podemos ver en la misma página de administración el resultado: ahora hay 13 &lt;strong&gt;hits&lt;/strong&gt; adicionales, ya que los paquetes solicitados estaban en local.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Estadísticas web de apt-cacher" src="https://www.linuxsysadmin.ml/images/apt-cacher-ng-2.jpg"&gt;&lt;/p&gt;
&lt;p&gt;De esta forma, si tenemos un elevado número de máquinas del mismo tipo, solo consumiremos el ancho de banda necesario para traerlos de internet &lt;strong&gt;una sola vez&lt;/strong&gt;.&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="jessie"></category><category term="apt-cacher-ng"></category><category term="cache"></category></entry><entry><title>Construyendo un RAID 10 en linux</title><link href="https://www.linuxsysadmin.ml/2015/12/construyendo-un-raid-10-en-linux.html" rel="alternate"></link><published>2015-12-17T23:00:00+01:00</published><updated>2015-12-17T23:00:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2015-12-17:/2015/12/construyendo-un-raid-10-en-linux.html</id><summary type="html">&lt;p&gt;El otro día estaba habilitando un servidor de &lt;em&gt;mongodb&lt;/em&gt; para un entorno de producción. Como me interesaba mejorar el rendimiento de los accesos a disco y no disponía de discos SSD con una durabilidad aceptable, me propuse montar un &lt;em&gt;array de discos&lt;/em&gt; en configuración de &lt;strong&gt;RAID 10&lt;/strong&gt;, como se recomienda …&lt;/p&gt;</summary><content type="html">&lt;p&gt;El otro día estaba habilitando un servidor de &lt;em&gt;mongodb&lt;/em&gt; para un entorno de producción. Como me interesaba mejorar el rendimiento de los accesos a disco y no disponía de discos SSD con una durabilidad aceptable, me propuse montar un &lt;em&gt;array de discos&lt;/em&gt; en configuración de &lt;strong&gt;RAID 10&lt;/strong&gt;, como se recomienda.&lt;/p&gt;
&lt;p&gt;Para este tutorial vamos a tener una máquina virtual (es una &lt;em&gt;Debian&lt;/em&gt;, pero vale cualquier otra distribución) con 5 discos, 1 de sistema y otros 4 para usar en la configuración &lt;strong&gt;RAID 10&lt;/strong&gt;, cada uno con 8gb, a efecto de demostración.&lt;/p&gt;
&lt;p&gt;En este caso, el sistema operativo estaba en &lt;em&gt;/dev/sda&lt;/em&gt; y sus particiones, mientras que los discos para los datos de &lt;em&gt;mongodb&lt;/em&gt; fueron &lt;em&gt;/dev/sdb&lt;/em&gt;, &lt;em&gt;/dev/sdc&lt;/em&gt;, &lt;em&gt;/dev/sdd&lt;/em&gt;, &lt;em&gt;/dev/sde&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# ls /dev/sd* -1
/dev/sda
/dev/sda1
/dev/sdb
/dev/sdc
/dev/sdd
/dev/sde
root@server:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Creación del dispositivo RAID 10&lt;/h2&gt;
&lt;p&gt;Empezamos instalando el controlador de &lt;strong&gt;RAID&lt;/strong&gt; por software:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# apt-get install mdadm
Leyendo lista de paquetes... Hecho
Creando árbol de dependencias       
Leyendo la información de estado... Hecho
..
Se instalarán los siguientes paquetes NUEVOS:
  bsd-mailx exim4-base exim4-config exim4-daemon-light liblockfile-bin
  liblockfile1 mdadm psmisc
&lt;span class="m"&gt;0&lt;/span&gt; actualizados, &lt;span class="m"&gt;8&lt;/span&gt; nuevos se instalarán, &lt;span class="m"&gt;0&lt;/span&gt; para eliminar y &lt;span class="m"&gt;0&lt;/span&gt; no actualizados.
...
update-initramfs: Generating /boot/initrd.img-3.16.0-4-586
W: mdadm: /etc/mdadm/mdadm.conf defines no arrays.
W: mdadm: no arrays defined in configuration file.
root@server:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Con las herramientas instaladas, procedemos a crear un &lt;em&gt;/dev/md0&lt;/em&gt; que será nuestro &lt;strong&gt;disco RAID&lt;/strong&gt;, indicando el nivel &lt;strong&gt;RAID 10&lt;/strong&gt; y los 4 discos reales que van a formarlo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mdadm -v --create /dev/md0 --level&lt;span class="o"&gt;=&lt;/span&gt;raid10 --raid-devices&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt; /dev/sdb /dev/sdc /dev/sdd /dev/sde
mdadm: layout defaults to n2
mdadm: layout defaults to n2
mdadm: chunk size defaults to 512K
mdadm: size &lt;span class="nb"&gt;set&lt;/span&gt; to 8380416K
mdadm: Defaulting to version &lt;span class="m"&gt;1&lt;/span&gt;.2 metadata
mdadm: array /dev/md0 started.
root@server:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para que ese array de discos sea reconocido en cada inicio del sistema, hay que añadir en &lt;em&gt;/etc/mdadm/mdadm.conf&lt;/em&gt; la información relacionada al array, de la misma forma que la tengamos en este momento.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mdadm --detail --scan --verbose &amp;gt;&amp;gt; /etc/mdadm/mdadm.conf
root@server:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y ya tenemos nuestro dispositivo &lt;strong&gt;RAID 10&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Preparación del dispositivo&lt;/h2&gt;
&lt;p&gt;Ahora disponemos de un &lt;strong&gt;RAID 10&lt;/strong&gt; de 4 discos de 8gb, que corresponden a una capacidad total de 16gb utilizables, como el dispositivo &lt;em&gt;/dev/md0&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Este dispositivo es transparente para nosotros y no es diferente de cualquier otro dispositivo de bloques, con lo que se puede particionar, formatear e incluso actuar como un &lt;em&gt;physical volume&lt;/em&gt; en caso de usar &lt;strong&gt;LVM&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Para esta demostración, se creará una única partición que ocupe todo el disco y que será montada en &lt;em&gt;/data&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Así pues, sin mas preámbulo la particionamos; en mi caso lo hice con &lt;em&gt;cfdisk&lt;/em&gt;. Este es el resultado:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# fdisk -l /dev/md0

Disco /dev/md0: &lt;span class="m"&gt;16&lt;/span&gt; GiB, &lt;span class="m"&gt;17163091968&lt;/span&gt; bytes, &lt;span class="m"&gt;33521664&lt;/span&gt; sectores
Unidades: sectores de &lt;span class="m"&gt;1&lt;/span&gt; * &lt;span class="nv"&gt;512&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;512&lt;/span&gt; bytes
Tamaño de sector &lt;span class="o"&gt;(&lt;/span&gt;lógico/físico&lt;span class="o"&gt;)&lt;/span&gt;: &lt;span class="m"&gt;512&lt;/span&gt; bytes / &lt;span class="m"&gt;512&lt;/span&gt; bytes
Tamaño de E/S &lt;span class="o"&gt;(&lt;/span&gt;mínimo/óptimo&lt;span class="o"&gt;)&lt;/span&gt;: &lt;span class="m"&gt;524288&lt;/span&gt; bytes / &lt;span class="m"&gt;1048576&lt;/span&gt; bytes
Tipo de etiqueta de disco: gpt
Identificador del disco: E3FE7B0A-0F5D-4151-84E8-49670C33B65E

Device     Start      End  Sectors Size Type
/dev/md0p1  &lt;span class="m"&gt;2048&lt;/span&gt; &lt;span class="m"&gt;33521630&lt;/span&gt; &lt;span class="m"&gt;33519583&lt;/span&gt;  16G Linux filesystem

root@server:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;La primera (y única partición) se llama &lt;em&gt;/dev/md0p1&lt;/em&gt; y es el dispositivo que vamos a formatear, para posteriormente montarlo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mkfs.ext4 /dev/md0p1 
mke2fs &lt;span class="m"&gt;1&lt;/span&gt;.42.12 &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;29&lt;/span&gt;-Aug-2014&lt;span class="o"&gt;)&lt;/span&gt;
Se está creando El sistema de ficheros con &lt;span class="m"&gt;4189947&lt;/span&gt; 4k bloques y &lt;span class="m"&gt;1048576&lt;/span&gt; nodos-i

UUID del sistema de ficheros: 11e454ce-72c4-41f8-a7bc-4d4a78b873c0
Respaldo del superbloque guardado en los bloques: 
    &lt;span class="m"&gt;32768&lt;/span&gt;, &lt;span class="m"&gt;98304&lt;/span&gt;, &lt;span class="m"&gt;163840&lt;/span&gt;, &lt;span class="m"&gt;229376&lt;/span&gt;, &lt;span class="m"&gt;294912&lt;/span&gt;, &lt;span class="m"&gt;819200&lt;/span&gt;, &lt;span class="m"&gt;884736&lt;/span&gt;, &lt;span class="m"&gt;1605632&lt;/span&gt;, &lt;span class="m"&gt;2654208&lt;/span&gt;, 
    &lt;span class="m"&gt;4096000&lt;/span&gt;

Reservando las tablas de grupo: hecho                           
Escribiendo las tablas de nodos-i: hecho                           
Creando el fichero de transacciones &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;32768&lt;/span&gt; bloques&lt;span class="o"&gt;)&lt;/span&gt;: hecho
Escribiendo superbloques y la información contable del sistema de ficheros:   hecho  

root@server:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Creamos la carpeta que va a servir de &lt;em&gt;mountpoint&lt;/em&gt; para esta nueva partición:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mkdir /data
root@server:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Añadimos la partición en el fichero &lt;em&gt;/etc/fstab&lt;/em&gt;, para que se monte automáticamente tras cada reinicio:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# grep md0p1 /etc/fstab 
/dev/md0p1 /data ext4 defaults &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;
root@server:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finalmente la montamos. Como esta información ya está en el fichero &lt;em&gt;/etc/fstab&lt;/em&gt; no es necesario especificar los detalles.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mount /data
root@server:~# df -h
S.ficheros     Tamaño Usados  Disp Uso% Montado en
/dev/sda1        &lt;span class="m"&gt;2&lt;/span&gt;,0G   651M  &lt;span class="m"&gt;1&lt;/span&gt;,2G  &lt;span class="m"&gt;35&lt;/span&gt;% /
udev              10M      &lt;span class="m"&gt;0&lt;/span&gt;   10M   &lt;span class="m"&gt;0&lt;/span&gt;% /dev
tmpfs             50M   &lt;span class="m"&gt;4&lt;/span&gt;,4M   46M   &lt;span class="m"&gt;9&lt;/span&gt;% /run
tmpfs            124M      &lt;span class="m"&gt;0&lt;/span&gt;  124M   &lt;span class="m"&gt;0&lt;/span&gt;% /dev/shm
tmpfs            &lt;span class="m"&gt;5&lt;/span&gt;,0M      &lt;span class="m"&gt;0&lt;/span&gt;  &lt;span class="m"&gt;5&lt;/span&gt;,0M   &lt;span class="m"&gt;0&lt;/span&gt;% /run/lock
tmpfs            124M      &lt;span class="m"&gt;0&lt;/span&gt;  124M   &lt;span class="m"&gt;0&lt;/span&gt;% /sys/fs/cgroup
/dev/md0p1        16G    44M   15G   &lt;span class="m"&gt;1&lt;/span&gt;% /data
root@server:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Como detalle, al no tratarse de una partición raíz de sistema operativo, no hace falta reservar bloques de emergencia; se trata de un 5% de la capacidad que podemos liberar (5% de 16gb son 800mb que podemos usar).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# tune2fs -m &lt;span class="m"&gt;0&lt;/span&gt; /dev/md0p1 
tune2fs &lt;span class="m"&gt;1&lt;/span&gt;.42.12 &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;29&lt;/span&gt;-Aug-2014&lt;span class="o"&gt;)&lt;/span&gt;
Se pone el porcentaje de bloques reservados a &lt;span class="m"&gt;0&lt;/span&gt;% &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt; bloques&lt;span class="o"&gt;)&lt;/span&gt;
root@server:~# df -h
S.ficheros     Tamaño Usados  Disp Uso% Montado en
/dev/sda1        &lt;span class="m"&gt;2&lt;/span&gt;,0G   651M  &lt;span class="m"&gt;1&lt;/span&gt;,2G  &lt;span class="m"&gt;35&lt;/span&gt;% /
udev              10M      &lt;span class="m"&gt;0&lt;/span&gt;   10M   &lt;span class="m"&gt;0&lt;/span&gt;% /dev
tmpfs             50M   &lt;span class="m"&gt;4&lt;/span&gt;,4M   46M   &lt;span class="m"&gt;9&lt;/span&gt;% /run
tmpfs            124M      &lt;span class="m"&gt;0&lt;/span&gt;  124M   &lt;span class="m"&gt;0&lt;/span&gt;% /dev/shm
tmpfs            &lt;span class="m"&gt;5&lt;/span&gt;,0M      &lt;span class="m"&gt;0&lt;/span&gt;  &lt;span class="m"&gt;5&lt;/span&gt;,0M   &lt;span class="m"&gt;0&lt;/span&gt;% /run/lock
tmpfs            124M      &lt;span class="m"&gt;0&lt;/span&gt;  124M   &lt;span class="m"&gt;0&lt;/span&gt;% /sys/fs/cgroup
/dev/md0p1        16G    44M   16G   &lt;span class="m"&gt;1&lt;/span&gt;% /data
root@server:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Verificación&lt;/h2&gt;
&lt;p&gt;Podemos ver la información de estado del array de discos con el mismo comando &lt;em&gt;mdadm&lt;/em&gt;, como sigue:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@server:~# mdadm --detail /dev/md0
/dev/md0:
        Version : &lt;span class="m"&gt;1&lt;/span&gt;.2
  Creation Time : Sat Dec &lt;span class="m"&gt;12&lt;/span&gt; &lt;span class="m"&gt;21&lt;/span&gt;:19:42 &lt;span class="m"&gt;2015&lt;/span&gt;
     Raid Level : raid10
     Array Size : &lt;span class="m"&gt;16760832&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;15&lt;/span&gt;.98 GiB &lt;span class="m"&gt;17&lt;/span&gt;.16 GB&lt;span class="o"&gt;)&lt;/span&gt;
  Used Dev Size : &lt;span class="m"&gt;8380416&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;.99 GiB &lt;span class="m"&gt;8&lt;/span&gt;.58 GB&lt;span class="o"&gt;)&lt;/span&gt;
   Raid Devices : &lt;span class="m"&gt;4&lt;/span&gt;
  Total Devices : &lt;span class="m"&gt;4&lt;/span&gt;
    Persistence : Superblock is persistent

    Update Time : Sat Dec &lt;span class="m"&gt;12&lt;/span&gt; &lt;span class="m"&gt;21&lt;/span&gt;:30:11 &lt;span class="m"&gt;2015&lt;/span&gt;
          State : clean 
 Active Devices : &lt;span class="m"&gt;4&lt;/span&gt;
Working Devices : &lt;span class="m"&gt;4&lt;/span&gt;
 Failed Devices : &lt;span class="m"&gt;0&lt;/span&gt;
  Spare Devices : &lt;span class="m"&gt;0&lt;/span&gt;

         Layout : &lt;span class="nv"&gt;near&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;
     Chunk Size : 512K

           Name : server:0  &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;local&lt;/span&gt; to host server&lt;span class="o"&gt;)&lt;/span&gt;
           UUID : 217558a7:bc1cb1d4:9530ecda:ea477a6b
         Events : &lt;span class="m"&gt;19&lt;/span&gt;

    Number   Major   Minor   RaidDevice State
       &lt;span class="m"&gt;0&lt;/span&gt;       &lt;span class="m"&gt;8&lt;/span&gt;       &lt;span class="m"&gt;16&lt;/span&gt;        &lt;span class="m"&gt;0&lt;/span&gt;      active sync set-A   /dev/sdb
       &lt;span class="m"&gt;1&lt;/span&gt;       &lt;span class="m"&gt;8&lt;/span&gt;       &lt;span class="m"&gt;32&lt;/span&gt;        &lt;span class="m"&gt;1&lt;/span&gt;      active sync set-B   /dev/sdc
       &lt;span class="m"&gt;2&lt;/span&gt;       &lt;span class="m"&gt;8&lt;/span&gt;       &lt;span class="m"&gt;48&lt;/span&gt;        &lt;span class="m"&gt;2&lt;/span&gt;      active sync set-A   /dev/sdd
       &lt;span class="m"&gt;3&lt;/span&gt;       &lt;span class="m"&gt;8&lt;/span&gt;       &lt;span class="m"&gt;64&lt;/span&gt;        &lt;span class="m"&gt;3&lt;/span&gt;      active sync set-B   /dev/sde
root@server:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;RESUMEN&lt;/strong&gt;: Ahora tengo un disco doble de rápido, doble de capacidad y con doble copia de datos. Afortunadamente, los discos duros son baratos...&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="jessie"></category><category term="raid"></category></entry><entry><title>Construyendo una replica set en mongodb</title><link href="https://www.linuxsysadmin.ml/2015/12/construyendo-una-replica-set-en-mongodb.html" rel="alternate"></link><published>2015-12-08T12:30:00+01:00</published><updated>2015-12-08T12:30:00+01:00</updated><author><name>Gerard</name></author><id>tag:www.linuxsysadmin.ml,2015-12-08:/2015/12/construyendo-una-replica-set-en-mongodb.html</id><summary type="html">&lt;p&gt;Muchas veces nos interesa obtener alta disponibilidad en los servicios que gestionamos. No hay nada mas desagradable que una llamada a las tantas de la noche porque se ha caído un nodo de una base de datos y no damos servicio. Para eso &lt;em&gt;mongodb&lt;/em&gt; nos ofrece el mecanismo de replicación …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Muchas veces nos interesa obtener alta disponibilidad en los servicios que gestionamos. No hay nada mas desagradable que una llamada a las tantas de la noche porque se ha caído un nodo de una base de datos y no damos servicio. Para eso &lt;em&gt;mongodb&lt;/em&gt; nos ofrece el mecanismo de replicación.&lt;/p&gt;
&lt;p&gt;En este artículo vamos a montar una &lt;em&gt;replica set&lt;/em&gt;, de forma que si se cayera un nodo de la base de datos, otro asumiría su rol, de forma que se seguiría dando servicio.&lt;/p&gt;
&lt;p&gt;Nuestra &lt;em&gt;replica set&lt;/em&gt; va a tener 3 nodos, que vamos a alojar en 3 máquinas distintas, de forma que la caída de una máquina afecte solamente a 1 proceso de &lt;em&gt;mongodb&lt;/em&gt;. La caonfiguración de 3 nodos nos da una tolerancia a fallos de 1 máquina; mientras queden 2, el clúster va a seguir operativo.&lt;/p&gt;
&lt;h2&gt;Descripción del entorno&lt;/h2&gt;
&lt;p&gt;Disponemos de 3 máquinas que vamos a llamar &lt;strong&gt;mongo1&lt;/strong&gt;, &lt;strong&gt;mongo2&lt;/strong&gt; y &lt;strong&gt;mongo3&lt;/strong&gt;. Cada una funciona con un sistema operativo &lt;em&gt;Debian jessie&lt;/em&gt; con &lt;em&gt;systemd&lt;/em&gt; y cuenta 1 gb de disco y con 256 mb de memoria; para esta demostración no se necesita mas.&lt;/p&gt;
&lt;p&gt;Como pequeño detalle, las máquinas se van referir entre ellas por nombre, pero como no me interesa poner una solución completa de &lt;em&gt;DNS&lt;/em&gt;, he puesto en el fichero &lt;em&gt;/etc/hosts&lt;/em&gt; de todas las máquinas las equivalencias.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# grep mongo /etc/hosts
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.2    mongo1
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.3    mongo2
&lt;span class="m"&gt;10&lt;/span&gt;.0.0.4    mongo3
root@mongo1:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Consideraciones de seguridad&lt;/h2&gt;
&lt;p&gt;Estas máquinas se comunican entre sí por el puerto TCP en el que corran sus procesos; para seguir con el puerto "titular" vamos a ponerlos en el puerto 27017. Es importante que las 3 máquinas puedan acceder al puerto de las otras 2. Adicionalmente, la máquina que vaya a usar este clúster también debe pode acceder al puerto 27017 de las 3 máquinas.&lt;/p&gt;
&lt;h2&gt;Preparación de las máquinas individuales&lt;/h2&gt;
&lt;p&gt;Queremos una versión de &lt;em&gt;mongodb&lt;/em&gt; un poco reciente, así que no vamos a usar los paquetes oficiales de la distribución, y la empresa de &lt;em&gt;mongodb&lt;/em&gt; no ofrece paquete para &lt;em&gt;Debian jessie&lt;/em&gt;. Por ello vamos a montar un esqueleto de ficheros como se describe en &lt;a href="https://www.linuxsysadmin.ml/2015/11/escribiendo-units-en-systemd.html"&gt;un artículo anterior&lt;/a&gt;. Vamos a describir el proceso en la máquina &lt;strong&gt;mongo1&lt;/strong&gt;, para replicarlo a posteriori en las otras 2.&lt;/p&gt;
&lt;p&gt;Creamos la estructura de carpetas que van a contener todo lo relativo a &lt;strong&gt;mongodb&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# mkdir -p /opt/mongodb/&lt;span class="o"&gt;{&lt;/span&gt;bin,conf,data/replica,logs&lt;span class="o"&gt;}&lt;/span&gt;
root@mongo1:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Copiamos el binario &lt;strong&gt;mongod&lt;/strong&gt; que encontraremos en el fichero &lt;em&gt;.tar.gz&lt;/em&gt; de la página de descargas de la página web.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# cp mongod /opt/mongodb/bin/
root@mongo1:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Creamos un fichero de configuración con el que vamos a levantar el proceso en esta máquina.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# cat /opt/mongodb/conf/replica.conf
systemLog:
    path: /opt/mongodb/logs/replica.log
    logAppend: &lt;span class="nb"&gt;true&lt;/span&gt;
    destination: file

net:
    port: &lt;span class="m"&gt;27017&lt;/span&gt;
    bindIp: &lt;span class="m"&gt;0&lt;/span&gt;.0.0.0

storage:
    dbPath: /opt/mongodb/data/replica
    smallFiles: &lt;span class="nb"&gt;true&lt;/span&gt;

replication:
    replSetName: replica
root@mongo1:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Por razones de seguridad vamos a lanzar el servicio con un usuario propio de sistema.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# useradd -s /usr/sbin/nologin -r -M mongo -d /opt/mongodb/
root@mongo1:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y para ahorrarnos problemas de permisos, lo hacemos propietario de todo lo referente al servicio:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# chown -R mongo:mongo /opt/mongodb/
root@mongo1:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a crearle una &lt;strong&gt;unit&lt;/strong&gt; para que el sistema se encargue de levantar automáticamente el servicio en caso de reinicio de la máquina:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# cat /etc/systemd/system/mongo.service
&lt;span class="o"&gt;[&lt;/span&gt;Unit&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;Description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;MongoDB

&lt;span class="o"&gt;[&lt;/span&gt;Service&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;User&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;mongo
&lt;span class="nv"&gt;LimitFSIZE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;infinity
&lt;span class="nv"&gt;LimitCPU&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;infinity
&lt;span class="nv"&gt;LimitAS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;infinity
&lt;span class="nv"&gt;LimitNOFILE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;64000&lt;/span&gt;
&lt;span class="nv"&gt;LimitNPROC&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;64000&lt;/span&gt;
&lt;span class="nv"&gt;ExecStartPre&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/bin/rm -f /opt/mongodb/data/replica/mongod.lock
&lt;span class="nv"&gt;ExecStart&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/opt/mongodb/bin/mongod -f /opt/mongodb/conf/replica.conf

&lt;span class="o"&gt;[&lt;/span&gt;Install&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="nv"&gt;WantedBy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;multi-user.target
root@mongo1:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finalmente activamos la &lt;strong&gt;unit&lt;/strong&gt; e iniciamos el servicio.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@mongo1:~# systemctl &lt;span class="nb"&gt;enable&lt;/span&gt; mongo
Created symlink from /etc/systemd/system/multi-user.target.wants/mongo.service to /etc/systemd/system/mongo.service.
root@mongo1:~# systemctl start mongo
root@mongo1:~# 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ahora toca repetir el proceso en las otras 2 máquinas, exactamente igual.&lt;/p&gt;
&lt;h2&gt;Configuración del clúster&lt;/h2&gt;
&lt;p&gt;Accedemos a una de las máquinas del futuro clúster desde cualquier máquina que pueda hacerlo y que disponga del binario &lt;strong&gt;mongo&lt;/strong&gt; (el mongo shell), que también viene en el archivo &lt;em&gt;.tar.gz&lt;/em&gt; descargado de la página oficial; este shell no es necesario para la aplicación que use el clúster ya que el &lt;strong&gt;driver&lt;/strong&gt; de cada lenguaje suple sus funciones, pero es muy útil tenerlo a mano para tareas de administración y consultas varias.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;root@client:~# ./mongo --host &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2
MongoDB shell version: &lt;span class="m"&gt;3&lt;/span&gt;.0.7
connecting to: &lt;span class="m"&gt;10&lt;/span&gt;.0.0.2:27017/test
Welcome to the MongoDB shell.
For interactive help, &lt;span class="nb"&gt;type&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;help&amp;quot;&lt;/span&gt;.
&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Hay dos formas de crear la configuración del clúster: pasando el documento de configuración en el método &lt;em&gt;initiate&lt;/em&gt; o añadir los nodos a posteriori con el método &lt;em&gt;add&lt;/em&gt;. Voy a usar este método por ser mas fácil.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; rs.initiate&lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;info2&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;no configuration explicitly specified -- making one&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;me&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;mongo1:27017&amp;quot;&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
replica:PRIMARY&amp;gt; rs.add&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mongo2:27017&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
replica:PRIMARY&amp;gt; rs.add&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mongo3:27017&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
replica:PRIMARY&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Vamos a lanzar el método &lt;em&gt;status&lt;/em&gt; hasta que todos los nodos sean primarios o secundarios, momento en el que la &lt;em&gt;replica&lt;/em&gt; va a quedar correctamente montada.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;replica:PRIMARY&amp;gt; rs.status&lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;set&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;replica&amp;quot;&lt;/span&gt;,
...
    &lt;span class="s2"&gt;&amp;quot;members&amp;quot;&lt;/span&gt; : &lt;span class="o"&gt;[&lt;/span&gt;
        &lt;span class="o"&gt;{&lt;/span&gt;
...
            &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;mongo1:27017&amp;quot;&lt;/span&gt;,
            &lt;span class="s2"&gt;&amp;quot;stateStr&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;PRIMARY&amp;quot;&lt;/span&gt;,
            &lt;span class="s2"&gt;&amp;quot;self&amp;quot;&lt;/span&gt; : &lt;span class="nb"&gt;true&lt;/span&gt;
...
        &lt;span class="o"&gt;}&lt;/span&gt;,
        &lt;span class="o"&gt;{&lt;/span&gt;
...
            &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;mongo2:27017&amp;quot;&lt;/span&gt;,
            &lt;span class="s2"&gt;&amp;quot;stateStr&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;SECONDARY&amp;quot;&lt;/span&gt;,
            &lt;span class="s2"&gt;&amp;quot;syncingTo&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;mongo1:27017&amp;quot;&lt;/span&gt;,
...
        &lt;span class="o"&gt;}&lt;/span&gt;,
        &lt;span class="o"&gt;{&lt;/span&gt;
...
            &lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;mongo3:27017&amp;quot;&lt;/span&gt;,
            &lt;span class="s2"&gt;&amp;quot;stateStr&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;SECONDARY&amp;quot;&lt;/span&gt;,
            &lt;span class="s2"&gt;&amp;quot;syncingTo&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;mongo1:27017&amp;quot;&lt;/span&gt;,
...
        &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;]&lt;/span&gt;,
    &lt;span class="s2"&gt;&amp;quot;ok&amp;quot;&lt;/span&gt; : &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
replica:PRIMARY&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Y con esta salida del método &lt;em&gt;status&lt;/em&gt; ya lo tenemos todo funcionando correctamente.&lt;/p&gt;</content><category term="linux"></category><category term="debian"></category><category term="jessie"></category><category term="mongodb"></category><category term="replica set"></category><category term="systemd"></category></entry></feed>